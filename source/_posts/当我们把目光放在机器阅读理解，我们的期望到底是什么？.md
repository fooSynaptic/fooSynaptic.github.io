---
title: "机器阅读理解：从传统方法到大语言模型"
date: 2019-10-04T07:41:50+08:00
tags:
  - MRC
  - Deep learning
  - LLM
  - RAG
---

> **核心问题**：当我们期望机器"理解"文本时，我们的期望到底是什么？

## 机器阅读理解的演进

### 传统 MRC (2015-2019)

基于 span extraction 的方法：

```
输入: Context + Question
输出: (start_idx, end_idx)
```

代表模型：BiDAF, R-Net, QANet, BERT

### LLM 时代的 MRC (2020-至今)

从"抽取"到"生成"的范式转变：

```
输入: Context + Question + Instruction
输出: 自由形式的答案
```

## 任务分类与难度

| 类型 | 传统方法 | LLM 方法 | 难度 |
|------|----------|----------|------|
| **抽取式** | ✅ 擅长 | ✅ 擅长 | ⭐ |
| **多跳推理** | ❌ 困难 | ⚠️ 有限 | ⭐⭐⭐ |
| **数值推理** | ❌ 几乎不能 | ⚠️ 需要 CoT | ⭐⭐⭐⭐ |
| **常识推理** | ❌ 不能 | ✅ 较好 | ⭐⭐⭐ |
| **开放生成** | ❌ 不能 | ✅ 擅长 | ⭐⭐ |

## 现代方法：RAG

检索增强生成 (Retrieval-Augmented Generation) 结合了检索和生成的优势：

```python
class RAGSystem:
    def __init__(self, retriever, generator):
        self.retriever = retriever  # e.g., Dense Retriever
        self.generator = generator  # e.g., LLM
    
    def answer(self, question: str) -> str:
        # 1. 检索相关文档
        docs = self.retriever.retrieve(question, top_k=5)
        
        # 2. 构建上下文
        context = "\n\n".join([d.text for d in docs])
        
        # 3. 生成答案
        prompt = f"""基于以下文档回答问题：

{context}

问题：{question}
答案："""
        
        return self.generator.generate(prompt)
```

### 检索器选择

| 检索器 | 特点 | 适用场景 |
|--------|------|----------|
| **BM25** | 关键词匹配，快速 | 短查询，精确匹配 |
| **Dense Retriever** | 语义匹配 | 语义相似查询 |
| **ColBERT** | 延迟交互 | 平衡效率与效果 |
| **Hybrid** | 结合稀疏+稠密 | 生产环境 |

## Chain-of-Thought 推理

对于需要推理的问题，CoT prompting 显著提升效果：

```python
# 标准 Prompting
prompt_standard = "Q: 小明有5个苹果，给了小红2个，还剩几个？\nA:"

# Chain-of-Thought Prompting  
prompt_cot = """Q: 小明有5个苹果，给了小红2个，还剩几个？
A: 让我们一步步思考：
1. 小明最初有 5 个苹果
2. 他给了小红 2 个苹果
3. 剩余苹果数 = 5 - 2 = 3
答案是 3 个苹果。"""
```

## 评估指标

### 传统指标

$$
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

$$
\text{EM (Exact Match)} = \mathbb{1}[\text{pred} = \text{gold}]
$$

### LLM 时代的指标

```python
# 使用 LLM 作为评估器
def llm_evaluate(question, gold_answer, pred_answer):
    prompt = f"""评估预测答案的质量（1-5分）：

问题：{question}
标准答案：{gold_answer}
预测答案：{pred_answer}

评分标准：
5分 - 完全正确且信息完整
4分 - 基本正确，略有遗漏
3分 - 部分正确
2分 - 有相关信息但不正确
1分 - 完全错误

分数："""
    return llm.generate(prompt)
```

## 实践建议

### 何时用传统 MRC

- 答案明确在文档中
- 需要精确的位置标注
- 低延迟要求
- 资源受限

### 何时用 RAG + LLM

- 需要整合多个文档
- 答案需要推理或总结
- 开放域问答
- 用户期望自然语言回答

## 代码示例：现代 RAG 系统

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 初始化组件
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.load_local("my_index", embeddings)
llm = ChatOpenAI(model="gpt-4", temperature=0)

# 创建 RAG 链
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # 或 "map_reduce", "refine"
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True
)

# 使用
result = qa_chain({"query": "什么是机器阅读理解？"})
print(result["result"])
```

## 延伸阅读

- [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/)
- [Natural Questions](https://ai.google.com/research/NaturalQuestions)
- [RAG Paper](https://arxiv.org/abs/2005.11401)
- [LangChain Documentation](https://docs.langchain.com/)

---

> 转载请注明出处
