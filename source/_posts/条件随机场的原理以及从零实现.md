---
title: "条件随机场的原理以及从零实现"
date: 2019-11-19T13:53:52+08:00
tags:
  - CRF
  - build from scratch
---

条件随机场在概率图模型当中相对比较复杂，因此也比较难理解，随着深度学习在工业界渐渐地后劲不足，传统的概率语言模型渐渐地被程序员青睐，因此深入理解这些统计语言模型对于一个合格的算法岗位来说是非常有必要的。很多人觉得crf很难，包括我也是，但是希望通过阅读这篇文章并且了解CRF的从头实现过程能够帮助您理解条件随机场。

条件随机场（CRF）是给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布模型，因此广泛用于序列标注任务，例如基于crf的分词，词性标注，命名实体识别，拼写检查纠错和多音字的问题。

# 条件随机场中涉及到三个问题

  * 概率计算问题：给定的条件随机场P(Y|X)和输入序列，计算对应的输出序列的条件概率和期望值。
  * 条件随机场的预测问题：给定条件随机场和输入序列，计算概率最大的输出序列。
  * 条件随机场的学习问题：给定训练数据集估计条件随机场模型的参数问题。



其中概率计算问题可以通过前向后向算法来完成，预测问题可以通过序列预测的标配维特比算法来完成，而条件随机场的学习问题可以有不同的优化方法，优化的目标就是期望最大化，在这里我们使用的是随机初始化每个feature到标签的权重和随机梯度下降来来优化参数来使得期望最大。

我们选择reference tagging作为我们的目标来完成这次从零开始的CRF学习，本质上是一个命名实体识别的任务，先看一眼数据：
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    

| 
    
    
    <NEWREFERENCE>  
    <author> A. Cau, R. Kuiper, and W.-P. de Roever. </author> <title> Formalising Dijkstra's development strategy within Stark's formalism. </title> <editor> In C. B. Jones, R. C. Shaw, and T. Denvir, editors, </editor> <booktitle> Proc. 5th. BCS-FACS Refinement Workshop, </booktitle> <date> 1992. </date>  
    <NEWREFERENCE>  
    <author> M. Kitsuregawa, H. Tanaka, and T. Moto-oka. </author> <title> Application of hash to data base machine and its architecture. </title> <journal> New Generation Computing, </journal> <volume> 1(1), </volume> <date> 1983. </date>  
    <NEWREFERENCE>  
    <author> Alexander Vrchoticky. </author> <title> Modula/R language definition. </title> <tech> Technical Report TU Wien rr-02-92, version 2.0, </tech> <institution> Dept. for Real-Time Systems, Technical University of Vienna, </institution> <date> May 1993. </date>  
      
  
---|---  
  
如你所见在每个reference中，有标注好的author，title和journal，完整的数据中标注有8到9个，我是通过beautifulSoup来parse token和标签的：
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    

| 
    
    
    def parser(line):  
        """parse a tagged reference."""  
        lines = bs(line, features="lxml")  
        sublines = [_line for _line in lines.body.descendants if isinstance(_line, bs4.element.Tag)]  
        res = []  
        for p in sublines:  
            tag = p.name  
            for token in p.string.split():  
                res.append((tag, token))  
        return res  
      
  
---|---  
  
在完整的条件随机场形式化当中，有两个特征函数，一个是状态特征函数，一个是转移特征函数，通过观察数据我采用了构词学的角度以及前后词的特征作为一个位置的全部特征，可以理解为一个特征的集合，我分别定义了词的特征函数和转移特征函数。
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    12  
    13  
    14  
    15  
    16  
    17  
    18  
    19  
    20  
    21  
    22  
    23  
    24  
    25  
    26  
    27  
    28  
    29  
    30  
    31  
    32  
    33  
    34  
    35  
    36  
    37  
    38  
    39  
    

| 
    
    
    def tokenfeatures(word):  
        """ Obsevation state features """  
        features = []  
      
        ### 1  
        features.append("token==" + word)  
      
        ### 2  
        mid = len(word)//2  
        if mid > 0:  
            features.append('Swith+'+word[:mid])  
            features.append('Ewith+'+word[mid:])  
          
        ### 3  
        features.append('abbre==' + re.sub('[0-9]', '0', re.sub('[^a-zA-Z0-9()\.\,]', '', word.lower())))  
      
        ### 4  
        fsymolic = re.findall('[^a-zA-Z0-9]', word)  
        if len(fsymolic) > 0:  
            features.extend(["contains" + c for c in fsymolic])  
        return features  
      
      
    def Transferfeature(seq):  
        """ Transfer features """  
        seq.insert(0, ['seq_start'])  
        seq.append(['seq_end'])  
        for i in range(len(seq)):  
            seq[i].extend(tokenfeatures(seq[i][0]))  
        if True:  
            # previous token feature  
            for i in range(1, len(seq)):  
                seq[i].extend('@-1:'+f for f in seq[i-1][1:] \  
                    if not (f.startswith('@+1:') or f.startswith('@-1:')))  
            # next token feature  
            for i in range(len(seq)-1):  
                seq[i].extend('@+1:'+f for f in seq[i+1][1:] \  
                    if not (f.startswith('@+1:') or f.startswith('@-1:')))  
        return seq  
      
  
---|---  
  
因为CRF是一个无向图模型，所以可以定义所有观测词到状态（隐藏状态，标注特征）的一个路径。

观测特征的路径参考![《统计学习方法》](https://pic1.zhimg.com/80/v2-2a8951656433bbafaa116f1fc031d624_hd.jpg)
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    

| 
    
    
    class linearChainCRF():  
        def __init__(self, xnode, ynode):  
            """  
            xnode: the mapping from observation features to int  
            ynode: the mapping from target label to int  
            """  
            self.K = len(ynode)              # size of target label  
            self.M = len(xnode)	        # total feature dimension length  
            self.featureNodes, self.labelNodes = xnode, ynode  
            self.W = np.zeros((self.M, self.K))  
            print("feature dimension: ", self.W.shape)  
      
  
---|---  
  
参考上面的代码，xnode代表着特征集合，ynode代表着状态的集合，他们都会map到一个int方便我们利用数据进行访问从特征到状态的权重（其实这里的思想就是简化的条件随机场的简化形式，将两个特征函数简化成一个）

![简化形式](https://pic1.zhimg.com/80/v2-f6496848d92f5721585ca56ba858b420_hd.png)

这里面需要披露更多的CRF的细节来帮助我们理解接下来的东西

1 对于每个输入观测数据的token list，将其完整映射到词属性的list集合，并且在这里我们定义了featureTable这个类的getitem方法来使其支持[]中括号operator（？？）
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    12  
    13  
    14  
    15  
    16  
    

| 
    
    
    class FeatureTable():  
        def __init__(self, tokenlist, xnode, ynode):  
            self.seq = []  
            for tokenfeatures in tokenlist:  
                # tokenfeatures[0] is the raw token, others are token attributes.  
                self.seq.append(np.array([xnode[f] for f in tokenfeatures], dtype=np.int32))  
      
            self.xdim, self.ydim = len(xnode), len(ynode)  
      
        def __getitem__(self, item):  
            t, yp, y = item  
            ftoken = self.seq[t]  
            if yp is not None:   
                ftoken = np.append(ftoken, yp)  
      
            return [(f, y) for f in ftoken]  
      
  
---|---  
  
下面的几个内容基本上这篇文章的全部重心，那就是我们考虑CRF之所以不同于多分类问题和多元的逻辑斯特回归问题那就是观测状态和隐藏状态之间的约束（无向图之间的边），那么一个很重要的问题就是给定输入序列和标注序列，我们要得到在完整的图中，输入状态能够企及（可以更新）那些参数，**在约束之内，条件随机场和多元的logistic回归没有差异** 。
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    12  
    13  
    14  
    15  
    16  
    17  
    18  
    19  
    20  
    21  
    22  
    23  
    24  
    25  
    26  
    27  
    28  
    29  
    30  
    31  
    32  
    33  
    34  
    35  
    

| 
    
    
    def edgefeatures(self, x, y):  
        """  
        This is important  
        for our raw input t1, t2, t3, t4 ...  
        transfer the token sequence information into {f1, ... ft}1, {f1, ... ft}2  
        """  
        assert len(x.seq) == len(y), "seq to label not alignmentted..."  
        features = list(x[0, None, y[0]])   #tuple list  
          
        for t in range(1, x.seq.__len__()):  
            for idx in x[t, y[t-1], y[t]]:  
                features.append(idx)  
      
        return features  
      
    def currentField(self, x):  
        """calculate potential parameter given current obeservation data. """  
        """x is a taggedFeatureTable object"""  
        N, K, W = x.seq.__len__(), self.K, self.W  
      
        # estimate  
        g0 = empty(K)  
        g = empty((N-1, K, K))    #at timestep n, the logit of i-th label transfer to j-th label  
      
        for y in range(K):  
            # at step 0, the logit of label k (from time -1)  
            g0[y] = sum(W[idx[0], idx[1]] for idx in x[0, None, y])  
            #g0[y] = W[x[0, None, y]].sum()  
      
        for t in range(1, N):  
            for y in range(K):  
                for yp in range(K):  
                    g[t-1,yp,y] = sum(W[idx[0], idx[1]] for idx in x[t, yp, y])  
      
        return g0, g  
      
  
---|---  
  
在currentField这个方法中，我们需要返回的是参数，其中go是初始状态的权重，而g是在时间步n，从状态i转移到状态j的似然概率（未归一化），得到的每个特征下的归一化概率用以计算完整的期望，参考文末代码中的Exp方法，期望的计算需要前向和后向概率。  
![求期望](https://pic4.zhimg.com/80/v2-3b116e13f0af2c9b155a2157baf5b557_hd.jpg)

下面来逐一解决crf的三个问题：

## 概率计算问题

给定的条件随机场P(Y|X)和输入序列，计算对应的输出序列的条件概率和期望值。对于给定label的似然估计（未归一化概率），包括了前向和后向算法。
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    12  
    13  
    14  
    15  
    16  
    17  
    18  
    19  
    20  
    21  
    22  
    23  
    24  
    25  
    26  
    27  
    28  
    29  
    30  
    31  
    32  
    33  
    34  
    

| 
    
    
    def logitEstimator(self, x, targetLabel):  
        """estimate label likely-hood given W"""  
        N = x.seq.__len__()  
        K = self.K  
        W = self.W  
        g0, g = self.currentField(x)  
        a = self.forward(g0, g, N, K)  
        logZ = logsumexp(a[N-1, :])  
        return sum(W[k] for k in targetLabel) - logZ  
      
      
    def forward(self, g0, g, N, K):  
        """forward stepwise"""  
        a = zeros((N, K))  
        a[0, :] = g0  
      
        for t in range(1, N):  
            yp = a[t-1, :]  
            for y in range(K):  
                a[t, y] = logsumexp(yp + g[t-1, :, y])  
          
        return a  
      
      
    def backward(self, g, N, K):  
        """backword stepwise"""  
        b = zeros((N, K))  
      
        for t in range(N-2, -1, -1):  
            yp = b[t+1, :]  
            for y in range(K):  
                b[t, y] = logsumexp(yp + g[t, y, :])  
      
        return b  
      
  
---|---  
  
## 序列预测问题

维特比算法（由代码中可以看出维特比的复杂度是kn方，所以有人告诉你NLG用维特比可以放心大胆的喷他）
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    12  
    13  
    14  
    15  
    16  
    17  
    18  
    19  
    20  
    21  
    22  
    23  
    24  
    25  
    

| 
    
    
    def vertebi_inference(self, x):  
        """sequence labeling, inference the most likely label given x"""  
        """x is a taggedFeatureTable object"""  
        N, K = x.seq.__len__(), self.K  
        g0, g = self.currentField(x)  
        B = ones((N, K), dtype=int32) * -1  
      
        # compute max-marginals and backtrace matrix  
        V = g0  
        for t in range(1, N):  
            U = empty(K)            # at time step t, only record the maxlogit pre label  
            for y in range(K):  
                w = V + g[t-1,:,y]  # at time t, the logit of all labels in t-1 transfer to label y  
                B[t,y] = b = w.argmax()     # recore which label in time t-1 have the max logit  
                U[y] = w[b]  
            V = U  
      
        # extract the best path by brack-tracking  
        y = V.argmax()  
        trace = []  
        for t in range(N-1, -1, -1):  
            trace.append(y)  
            y = B[t, y]  
      
        return trace[::-1]  
      
  
---|---  
  
## 参数学习问题
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    12  
    13  
    14  
    15  
    16  
    17  
    

| 
    
    
    def sgd(self, data, iters = 20, Nwarm_up = 10, saveParam = False, modelpath = 'linearCRF'):  
            W = self.W  ## reference  
            for i in range(iters):  
                print("Epochs: ", i)  
                lr_rate = Nwarm_up/(i+1)**0.501  
                for x, labels in tqdm(data):  
                    ftoken = FeatureTable(x, self.featureNodes, self.labelNodes)  
                    flabel = self.edgefeatures(ftoken, [self.labelNodes[label] for label in labels])  
                    for k, explogit in self.Exp(ftoken).items():  
                        W[k] -= lr_rate * explogit  
                    for k in flabel:  
                        W[k] += lr_rate  
      
            if saveParam:  
                if not os.path.exists(modelpath):  
                    os.mkdir(modelpath)  
                joblib.dump(self.W, os.path.join(modelpath, 'crf_W.pkl'))  
      
  
---|---  
  
可以参考我的开源代码，在20个epoch之后的f1（多标签分类）达到了91%以上。测试数据占完整数据的百分之十。
    
    
    1  
    2  
    

| 
    
    
    Average estimator logit likelyhood is -80.44978854578937.  
    Average f1 is 0.9166892806881518.  
      
  
---|---  
  
参考我的开源代码：  
[crf from scratch](https://link.zhihu.com/?target=https%3A//github.com/fooSynaptic/NLP_utils/blob/master/ML/PGM/CRF/crf_from_scratch/models.py)
