---
title: "条件随机场的原理以及从零实现"
date: 2019-11-19T13:53:52+08:00
tags:
  - CRF
  - build from scratch
---

条件随机场在概率图模型中相对复杂，因此比较难理解。随着深度学习在工业界逐渐遇到瓶颈，传统的概率语言模型重新受到青睐。深入理解这些统计语言模型对于算法工程师来说非常必要。

很多人觉得 CRF 很难（包括我），但希望通过阅读这篇文章并了解 CRF 的从头实现过程，能够帮助你理解条件随机场。

## 什么是条件随机场

条件随机场（CRF）是给定一组输入随机变量的条件下，另一组输出随机变量的条件概率分布模型。因此广泛用于序列标注任务，例如：

- 分词
- 词性标注
- 命名实体识别
- 拼写检查纠错
- 多音字消歧

## CRF 涉及的三个问题

1. **概率计算问题**：给定 P(Y|X) 和输入序列，计算对应输出序列的条件概率和期望值
2. **预测问题**：给定 CRF 和输入序列，计算概率最大的输出序列
3. **学习问题**：给定训练数据集，估计 CRF 模型的参数

**解决方法：**
- 概率计算：前向-后向算法
- 预测问题：维特比算法
- 学习问题：随机梯度下降优化期望最大化

## 实践：Reference Tagging

我们选择 reference tagging 作为目标任务，本质上是一个命名实体识别任务。

**数据示例：**

```xml
<NEWREFERENCE>
<author> A. Cau, R. Kuiper, and W.-P. de Roever. </author>
<title> Formalising Dijkstra's development strategy within Stark's formalism. </title>
<editor> In C. B. Jones, R. C. Shaw, and T. Denvir, editors, </editor>
<booktitle> Proc. 5th. BCS-FACS Refinement Workshop, </booktitle>
<date> 1992. </date>
```

**数据解析代码：**

```python
def parser(line):
    """parse a tagged reference."""
    lines = bs(line, features="lxml")
    sublines = [_line for _line in lines.body.descendants 
                if isinstance(_line, bs4.element.Tag)]
    res = []
    for p in sublines:
        tag = p.name
        for token in p.string.split():
            res.append((tag, token))
    return res
```

## 特征函数

CRF 形式化中有两个特征函数：状态特征函数和转移特征函数。

```python
def tokenfeatures(word):
    """Observation state features"""
    features = []
    
    # 词本身
    features.append("token==" + word)
    
    # 前后缀
    mid = len(word) // 2
    if mid > 0:
        features.append('Swith+' + word[:mid])
        features.append('Ewith+' + word[mid:])
    
    # 缩写形式
    features.append('abbre==' + re.sub('[0-9]', '0', 
                    re.sub('[^a-zA-Z0-9().,]', '', word.lower())))
    
    # 包含的符号
    fsymbolic = re.findall('[^a-zA-Z0-9]', word)
    if len(fsymbolic) > 0:
        features.extend(["contains" + c for c in fsymbolic])
    
    return features


def Transferfeature(seq):
    """Transfer features"""
    seq.insert(0, ['seq_start'])
    seq.append(['seq_end'])
    
    for i in range(len(seq)):
        seq[i].extend(tokenfeatures(seq[i][0]))
    
    # 添加前后词特征
    for i in range(1, len(seq)):
        seq[i].extend('@-1:' + f for f in seq[i-1][1:]
            if not (f.startswith('@+1:') or f.startswith('@-1:')))
    
    for i in range(len(seq) - 1):
        seq[i].extend('@+1:' + f for f in seq[i+1][1:]
            if not (f.startswith('@+1:') or f.startswith('@-1:')))
    
    return seq
```

## 线性链 CRF 实现

```python
class linearChainCRF():
    def __init__(self, xnode, ynode):
        """
        xnode: 观测特征到 int 的映射
        ynode: 标签到 int 的映射
        """
        self.K = len(ynode)  # 标签数量
        self.M = len(xnode)  # 特征维度
        self.featureNodes, self.labelNodes = xnode, ynode
        self.W = np.zeros((self.M, self.K))
        print("feature dimension:", self.W.shape)
```

> CRF 之所以不同于多分类问题和多元逻辑回归，是因为观测状态和隐藏状态之间存在约束（无向图的边）。**在约束之内，条件随机场和多元 logistic 回归没有差异**。

## 概率计算：前向-后向算法

```python
def forward(self, g0, g, N, K):
    """前向算法"""
    a = zeros((N, K))
    a[0, :] = g0
    
    for t in range(1, N):
        yp = a[t-1, :]
        for y in range(K):
            a[t, y] = logsumexp(yp + g[t-1, :, y])
    
    return a


def backward(self, g, N, K):
    """后向算法"""
    b = zeros((N, K))
    
    for t in range(N-2, -1, -1):
        yp = b[t+1, :]
        for y in range(K):
            b[t, y] = logsumexp(yp + g[t, y, :])
    
    return b
```

## 序列预测：维特比算法

```python
def viterbi_inference(self, x):
    """序列标注：推断给定 x 的最可能标签"""
    N, K = x.seq.__len__(), self.K
    g0, g = self.currentField(x)
    B = ones((N, K), dtype=int32) * -1
    
    # 计算最大边际和回溯矩阵
    V = g0
    for t in range(1, N):
        U = empty(K)
        for y in range(K):
            w = V + g[t-1, :, y]
            B[t, y] = b = w.argmax()
            U[y] = w[b]
        V = U
    
    # 回溯提取最佳路径
    y = V.argmax()
    trace = []
    for t in range(N-1, -1, -1):
        trace.append(y)
        y = B[t, y]
    
    return trace[::-1]
```

> 维特比算法的复杂度是 O(K²N)，所以有人说 NLG 用维特比可以放心大胆地喷他。

## 参数学习：SGD

```python
def sgd(self, data, iters=20, Nwarm_up=10, saveParam=False, modelpath='linearCRF'):
    W = self.W
    for i in range(iters):
        print("Epochs:", i)
        lr_rate = Nwarm_up / (i + 1) ** 0.501
        
        for x, labels in tqdm(data):
            ftoken = FeatureTable(x, self.featureNodes, self.labelNodes)
            flabel = self.edgefeatures(ftoken, 
                     [self.labelNodes[label] for label in labels])
            
            for k, explogit in self.Exp(ftoken).items():
                W[k] -= lr_rate * explogit
            for k in flabel:
                W[k] += lr_rate
    
    if saveParam:
        if not os.path.exists(modelpath):
            os.mkdir(modelpath)
        joblib.dump(self.W, os.path.join(modelpath, 'crf_W.pkl'))
```

## 实验结果

在 20 个 epoch 之后（测试数据占 10%）：

```
Average estimator logit likelihood: -80.45
Average F1: 0.9167 (91.67%)
```

## 参考代码

[CRF from scratch](https://github.com/fooSynaptic/NLP_utils/blob/master/ML/PGM/CRF/crf_from_scratch/models.py)
