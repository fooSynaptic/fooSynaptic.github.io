---
title: "矩阵分解之一：Truncate SVD 和random SVD"
date: 2019-10-04T07:32:18+08:00
---

最近看了一点矩阵分解的论文发现了一些好玩的东西，所以会准备总结几篇矩阵分解的文章

## 奇异值分解相对于矩阵分解的问题

每个学过线性代数的人都不会对奇异值分解感到陌生，因为SVD广泛地应用于统计学，信号处理以及机器学习当中。形式上来看，一个维度为 m × n的实数矩阵的奇异值分列可以表示为A = U Σ Vᵀ；其中U代表的是一个维度为m × m的正交奇异向量矩阵，∑代表着奇异值的对角矩阵，最后一项是一个n × n的正交奇异向量矩阵。  
在矩阵分解这个问题上，奇异值分解提供的策略是计算一个相对于A更低秩的近似矩阵Aᵣ，意味着`r<m, n`， 并且要使得`||Aᵣ – A||`最小。那么对于A = U Σ Vᵀtruncate SVD的策略是：  
对于对角矩阵上面的奇异值进行降序排序；  
在对角矩阵∑上面取前r个奇异值，相对应的在左右两边的奇异向量矩阵上面也取相对于的r列，最后分别得到了Σᵣ，Uᵣ， Vᵣ。  
将Aᵣ = Uᵣ Σᵣ Vᵣᵀ作为最终矩阵分解的产物。  
上述过程可以用如下的示意图来表示：

这样，通过SVD就可以成功的完成矩阵降维的过程。我们从矩阵分解来到了很相近的另外一个主题——降维，作为降维届的代表技术——PCA，而矩阵的奇异值分解能够直接得到矩阵通过PCA的投影空间。对于一个协变量矩阵X为 m × p（m为观测）的观测特征矩阵，计算一个p × l 的矩阵W和一个 l × l 的对角矩阵Λ，能够使得 Xᵀ X ≈ W Λ Wᵀ。这样地近似可以将原先的观测矩阵投影到一个维度为l的空间从而达到降维。有一种精简的PCA算法计算的形式是通过直接近似一个低秩的协变量矩阵X ≈ Uᵣ Σᵣ Vᵣᵀ，然后乘上转置，最终因为左奇异向量矩阵正交而直接得到投影的向量空间。  
Xᵀ X ≈ (Uᵣ Σᵣ Vᵣᵀ)ᵀ (Uᵣ Σᵣ Vᵣᵀ) = Vᵣ Σᵣ² Vᵣᵀ  
如上最终的对角矩阵就是新的投影空间。  
但是truncate SVD有一些缺点（通过上述和PCA的论述其实可以得到这个缺点是传统降维技术共有的）：  
实际的工业环境中，矩阵的维度是巨大的，并且数据往往是缺失，不准确的；当不准确的输入限制了输出的精确性时，仅仅依靠这些实际的数据只会白白浪费计算资源。  
传统的降维技术是不支持并行计算的，如果你熟悉工业数据你就应该知道这一点有多么可怕。

到这里我们就要进入这篇文章的转折点引出我们的主角了——

## Randomized SVD

它相对于truncate SVD的优势有这么几点：  
很稳定。  
它的性能并不依赖于局部的特征。  
大量的矩阵乘法过程，可以利用GPU并行计算，所以它比truncate SVD更快。  
相比于直接从理论上阐述randomized SVD，我想更直接一点，直接参考它的实现过程能够让我们更快地理解。  
首先我们需要定义一个方法来找到一个正交矩阵，这个矩阵的范围近似于观测矩阵的范围（这个和我们上面的思路很像，一个更小的矩阵），这里我们会用到一些传统的LU和QR分解。  
** _在这里的LU和QR分解起的是规范子的作用，QR相对LU更慢但是更准确，所以QR规范放在最后一层。_**
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    

| 
    
    
    def randomized_range_finder(A, size, n_iter=5):  
        Q = np.random.normal(size=(A.shape[1], size))  
          
        for i in range(n_iter):  
            Q, _ = linalg.lu(A @ Q, permute_l=True)  
            Q, _ = linalg.lu(A.T @ Q, permute_l=True)  
              
        Q, _ = linalg.qr(A @ Q, mode='economic')  
        return Q  
      
  
---|---  
  
现在我们能够得到了观测矩阵范围的近似Q，我们利用Q来得到最终的近似结果：
    
    
    1  
    2  
    3  
    4  
    5  
    6  
    7  
    8  
    9  
    10  
    11  
    12  
    13  
    14  
    15  
    

| 
    
    
    def randomized_svd(M, n_components, n_oversamples=10, n_iter=4):  
        #这里n_random就是truncate SVD中的r  
        n_random = n_components + n_oversamples  
          
        Q = randomized_range_finder(M, n_random, n_iter)  
          
        #把原始观测投影到(k + p)维度空间  
        B = Q.T @ M  
          
        # 对B进行奇异值分解  
        Uhat, s, V = linalg.svd(B, full_matrices=False)  
        del B  
        U = Q @ Uhat  
          
        return U[:, :n_components], s[:n_components], V[:n_components, :]  
      
  
---|---  
  
没有看懂？好吧让我们来讲讲这个原理，randomize SVD对于矩阵分解能够作为一种通用的算法，简单讲分成两步，  
第一步是求原始矩阵范围的近似Q，这个过程中通过对一个随机初始化的小维度矩阵Q不断地进行和原矩阵相乘然后分解，最终得到一个稳定的向量矩阵，为什么不取对角矩阵，因为对角矩阵是特征基矩阵，而左边的向量矩阵可以作为量纲矩阵。最终我们的目标是得到A ≈ Q Qᵀ A。  
第二步很简单，有了A ≈ Q Qᵀ A，我们构造一个矩阵B = Qᵀ A，因为Q是低秩的，所以矩阵B很小。我们可以用传统SVD的方法来对矩阵B进行分解B = S Σ Vᵀ，得到左奇异，奇异值矩阵，右奇异向量矩阵。到这里A ≈ Q Qᵀ A = Q (S Σ Vᵀ)，最右边所有量都已知，成功地对A求到了一个低秩的近似U Σ Vᵀ。  
（通过上面的代码，可以很直接得理解这两步原理。）

## Tricks and intuition

randomized SVD的trick就是能够非常高效得求得范围近似矩阵Q，从直觉上来思考，为了估计原始矩阵的范围，我们可以用一些随机的向量，通过原始矩阵A和这些随机向量的相乘所作出的变动的近似来得到A的波动范围。  
Specific：假设我们使用一个高斯向量矩阵M来和原矩阵相乘，计算Y = A M，然后对Y进行QR分解Q R = Y，这样得到的矩阵Q，它的没一列就是Y的范围的正交基，所以可以作为A的范围近似。

=========  
转载请注明出处  
=========
