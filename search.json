[{"title":"DDIA Part 1ï¼šæ•°æ®ç³»ç»ŸåŸºç¡€","url":"/2025/12/28/DDIA-Part1-%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/","content":"æœ¬æ–‡æ˜¯ DDIA ç¬¬ä¸€éƒ¨åˆ†çš„è¯»ä¹¦ç¬”è®°ï¼Œæ¶µç›–ç¬¬ 1-4 ç« ï¼šå¯é æ€§ä¸å¯æ‰©å±•æ€§ã€æ•°æ®æ¨¡å‹ã€å­˜å‚¨å¼•æ“ã€æ•°æ®ç¼–ç ã€‚\n\nç¬¬1ç« ï¼šå¯é æ€§ã€å¯æ‰©å±•æ€§ä¸å¯ç»´æŠ¤æ€§æ•°æ®å¯†é›†å‹åº”ç”¨çš„ç»„æˆç°ä»£æ•°æ®å¯†é›†å‹åº”ç”¨é€šå¸¸ç”±å¤šä¸ªç»„ä»¶ç»„åˆï¼š\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚         æ•°æ®å¯†é›†å‹åº”ç”¨æ¶æ„               â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚  æ•°æ®åº“ â†’ ç¼“å­˜ â†’ æœç´¢ç´¢å¼•               â”‚â”‚  æµå¤„ç† â†’ æ‰¹å¤„ç† â†’ æ¶ˆæ¯é˜Ÿåˆ—             â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nå¯é æ€§ (Reliability)\nç³»ç»Ÿåœ¨é¢å¯¹æ•…éšœæ—¶ä»èƒ½æ­£ç¡®è¿è¡Œ\n\næ•…éšœç±»å‹ä¸åº”å¯¹ï¼š\n\n\n\næ•…éšœç±»å‹\nåº”å¯¹ç­–ç•¥\n\n\n\nç¡¬ä»¶æ•…éšœ\nRAIDã€åŒç”µæºã€å¤šå‰¯æœ¬\n\n\nè½¯ä»¶é”™è¯¯\næµ‹è¯•ã€éš”ç¦»ã€ç›‘æ§ã€å¿«é€Ÿé‡å¯\n\n\näººä¸ºé”™è¯¯\næ²™ç®±ç¯å¢ƒã€ç°åº¦å‘å¸ƒã€å¿«é€Ÿå›æ»š\n\n\nå¯æ‰©å±•æ€§ (Scalability)\nç³»ç»Ÿåº”å¯¹è´Ÿè½½å¢é•¿çš„èƒ½åŠ›\n\næ€§èƒ½æŒ‡æ ‡ï¼šä½¿ç”¨ç™¾åˆ†ä½æ•°è€Œéå¹³å‡å€¼\n\n\n\nç™¾åˆ†ä½\nå«ä¹‰\n\n\n\np50\nä¸­ä½æ•°ï¼Œå…¸å‹å“åº”æ—¶é—´\n\n\np95\n95%è¯·æ±‚å¿«äºæ­¤å€¼\n\n\np99\nå¸¸ç”¨äº SLA æ ‡å‡†\n\n\næ‰©å±•ç­–ç•¥ï¼š\n\nçºµå‘æ‰©å±•ï¼šä½¿ç”¨æ›´å¼ºå¤§çš„æœºå™¨\næ¨ªå‘æ‰©å±•ï¼šä½¿ç”¨å¤šå°æ™®é€šæœºå™¨\nå¼¹æ€§æ‰©å±•ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨å¢å‡èµ„æº\n\nå¯ç»´æŠ¤æ€§ (Maintainability)\n\n\næ–¹é¢\nç›®æ ‡\n\n\n\nå¯æ“ä½œæ€§\nè¿ç»´å›¢é˜Ÿèƒ½è½»æ¾ä¿æŒç³»ç»Ÿè¿è¡Œ\n\n\nç®€å•æ€§\næ–°å·¥ç¨‹å¸ˆèƒ½å¿«é€Ÿç†è§£ç³»ç»Ÿ\n\n\nå¯æ¼”åŒ–æ€§\nèƒ½è½»æ¾ä¿®æ”¹å’Œæ‰©å±•ç³»ç»Ÿ\n\n\n\nç¬¬2ç« ï¼šæ•°æ®æ¨¡å‹ä¸æŸ¥è¯¢è¯­è¨€ä¸‰ç§æ•°æ®æ¨¡å‹å¯¹æ¯”\n\n\næ¨¡å‹\nç‰¹ç‚¹\né€‚ç”¨åœºæ™¯\n\n\n\nå…³ç³»æ¨¡å‹\nç»“æ„åŒ–ã€è§„èŒƒåŒ–ã€SQL\näº‹åŠ¡å¤„ç†ã€å¤æ‚æŸ¥è¯¢\n\n\næ–‡æ¡£æ¨¡å‹\nçµæ´»æ¨¡å¼ã€åµŒå¥—ç»“æ„\næ ‘çŠ¶æ•°æ®ã€å¿«é€Ÿè¿­ä»£\n\n\nå›¾æ¨¡å‹\nå¤šå¯¹å¤šå…³ç³»\nç¤¾äº¤ç½‘ç»œã€çŸ¥è¯†å›¾è°±\n\n\nå…³ç³»æ¨¡å‹-- è§„èŒƒåŒ–è®¾è®¡ï¼šä½¿ç”¨å¤–é”®CREATE TABLE users (    user_id INT PRIMARY KEY,    name VARCHAR(100),    position_id INT REFERENCES positions(position_id));\n\nä¼˜åŠ¿ï¼šæ•°æ®ä¸€è‡´æ€§ã€çµæ´»æŸ¥è¯¢ã€äº‹åŠ¡æ”¯æŒå±€é™ï¼šå¯¹è±¡-å…³ç³»é˜»æŠ—ä¸åŒ¹é…ã€æ¨¡å¼åƒµåŒ–\næ–‡æ¡£æ¨¡å‹{  \"user_id\": 1,  \"name\": \"å¼ ä¸‰\",  \"positions\": [    {\"title\": \"å·¥ç¨‹å¸ˆ\", \"company\": \"ABCå…¬å¸\"},    {\"title\": \"æŠ€æœ¯æ€»ç›‘\", \"company\": \"XYZå…¬å¸\"}  ]}\n\nSchema-on-read vs Schema-on-writeï¼š\n\nå…³ç³»æ•°æ®åº“ï¼šå†™å…¥æ—¶éªŒè¯æ¨¡å¼ï¼ˆé™æ€ç±»å‹ï¼‰\næ–‡æ¡£æ•°æ®åº“ï¼šè¯»å–æ—¶è§£é‡Šç»“æ„ï¼ˆåŠ¨æ€ç±»å‹ï¼‰\n\nå›¾æ¨¡å‹// Neo4j Cypher æŸ¥è¯¢MATCH (alice:Person {name: 'Alice'})-[:FOLLOWS]-&gt;()-[:FOLLOWS]-&gt;(fof)RETURN fof.name\n\nå£°æ˜å¼ vs å‘½ä»¤å¼\n\n\nç±»å‹\nç‰¹ç‚¹\nç¤ºä¾‹\n\n\n\nå£°æ˜å¼\næè¿°æƒ³è¦ä»€ä¹ˆç»“æœ\nSQL, Cypher\n\n\nå‘½ä»¤å¼\næè¿°å¦‚ä½•å¾—åˆ°ç»“æœ\nç¼–ç¨‹è¯­è¨€å¾ªç¯\n\n\n\nç¬¬3ç« ï¼šå­˜å‚¨ä¸æ£€ç´¢ä¸¤å¤§å­˜å‚¨å¼•æ“å®¶æ—\n\n\nç±»å‹\nä¼˜åŒ–ç›®æ ‡\nä»£è¡¨äº§å“\n\n\n\næ—¥å¿—ç»“æ„ (LSM)\nå†™å…¥ä¼˜åŒ–\nRocksDB, Cassandra\n\n\nåŸåœ°æ›´æ–° (B-Tree)\nè¯»å–ä¼˜åŒ–\nMySQL, PostgreSQL\n\n\nLSM-Tree ç»“æ„Level 0 (å†…å­˜):â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  Memtable  â”‚ â† å½“å‰å†™å…¥ï¼ˆå¹³è¡¡æ ‘ï¼‰â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â†“ è¾¾åˆ°é˜ˆå€¼ï¼Œå†™å…¥ç£ç›˜Level 1-N (ç£ç›˜):â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”â”‚SS1 â”‚ â”‚SS2 â”‚ â”‚SS3 â”‚ â† SSTableï¼ˆæ’åºé”®ï¼‰â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜\n\nè¯»å–æµç¨‹ï¼šMemtable â†’ æœ€æ–° SSTable â†’ â€¦ â†’ æœ€è€ SSTable\nä¼˜åŒ–æŠ€æœ¯ï¼š\n\nå¸ƒéš†è¿‡æ»¤å™¨ï¼šå¿«é€Ÿåˆ¤æ–­é”®æ˜¯å¦å­˜åœ¨\nå‹ç¼©ç­–ç•¥ï¼šSize-Tieredï¼ˆå†™å¯†é›†ï¼‰ã€Leveledï¼ˆè¯»å¯†é›†ï¼‰\n\nB-Tree ç»“æ„          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    [30, 70]   â”‚ â† æ ¹èŠ‚ç‚¹          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         /       â”‚        \\â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚[10,20]â”‚  â”‚[40,50,60] â”‚  â”‚[80,90]â”‚ â† å¶å­èŠ‚ç‚¹â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜\n\nWAL (é¢„å†™æ—¥å¿—)ï¼šå…ˆå†™æ—¥å¿—ï¼Œå†æ›´æ–°æ•°æ®ï¼Œä¿è¯å´©æºƒæ¢å¤\nB-Tree vs LSM-Tree\n\n\nç‰¹æ€§\nB-Tree\nLSM-Tree\n\n\n\nå†™å…¥\nåŸåœ°æ›´æ–°\nè¿½åŠ å†™å…¥\n\n\nè¯»å–\nå¿«ï¼ˆä¸€æ¬¡å®šä½ï¼‰\nå¯èƒ½æ£€æŸ¥å¤šä¸ªæ–‡ä»¶\n\n\nå†™æ”¾å¤§\nè¾ƒä½\nè¾ƒé«˜ï¼ˆå‹ç¼©å¼€é”€ï¼‰\n\n\nç©ºé—´åˆ©ç”¨\nå¯èƒ½ç¢ç‰‡åŒ–\næ›´ç´§å‡‘\n\n\nOLTP vs OLAP\n\n\nç‰¹æ€§\nOLTP\nOLAP\n\n\n\næ“ä½œ\nå¢åˆ æ”¹æŸ¥\nå¤æ‚æŸ¥è¯¢ã€èšåˆ\n\n\næ•°æ®é‡\nGB~TB\nTB~PB\n\n\nç”¨æˆ·\nåº”ç”¨ç¨‹åº\nåˆ†æå¸ˆ\n\n\nå­˜å‚¨\nè¡Œå­˜å‚¨\nåˆ—å­˜å‚¨\n\n\nåˆ—å­˜å‚¨ä¼˜åŠ¿ï¼šåªè¯»å–éœ€è¦çš„åˆ—ã€å‹ç¼©å‹å¥½ã€å‘é‡åŒ–å¤„ç†\n\nç¬¬4ç« ï¼šæ•°æ®ç¼–ç ä¸æ¼”åŒ–å…¼å®¹æ€§æ¦‚å¿µæ—¶é—´çº¿: v1 â”€â”€&gt; v2 â”€â”€&gt; v3 â”€â”€&gt; v4åå‘å…¼å®¹ï¼šæ–°ä»£ç èƒ½è¯»å–æ—§æ•°æ® (v3 è¯» v1 æ•°æ® âœ“)å‰å‘å…¼å®¹ï¼šæ—§ä»£ç èƒ½è¯»å–æ–°æ•°æ® (v1 è¯» v3 æ•°æ® âœ“)\n\næ»šåŠ¨å‡çº§ï¼šæ–°æ—§ç‰ˆæœ¬ä»£ç åŒæ—¶è¿è¡Œï¼Œéœ€è¦åŒå‘å…¼å®¹\nç¼–ç æ ¼å¼å¯¹æ¯”\n\n\næ ¼å¼\nå¯è¯»æ€§\nç©ºé—´æ•ˆç‡\næ¨¡å¼æ¼”åŒ–\n\n\n\nJSON\né«˜\nä½\næ‰‹åŠ¨\n\n\nProtobuf\næ— \né«˜\næ”¯æŒ\n\n\nThrift\næ— \né«˜\næ”¯æŒ\n\n\nAvro\næ— \næœ€é«˜\næ”¯æŒ\n\n\nProtocol Buffersmessage Person {  required string user_name = 1;  optional int64 favorite_number = 2;  repeated string interests = 3;}\n\nå…³é”®è§„åˆ™ï¼šå­—æ®µåå¯æ”¹ï¼Œå­—æ®µæ ‡ç­¾ï¼ˆæ•°å­—ï¼‰ä¸èƒ½æ”¹\nå…¼å®¹æ€§è§„åˆ™ï¼š\n\n\n\næ“ä½œ\nåå‘å…¼å®¹\nå‰å‘å…¼å®¹\n\n\n\næ·»åŠ å¯é€‰å­—æ®µ\nâœ“\nâœ“\n\n\nåˆ é™¤å¯é€‰å­—æ®µ\nâœ“\nâœ“\n\n\næ·»åŠ å¿…å¡«å­—æ®µ\nâœ—\nâœ—\n\n\nAvroç‰¹ç‚¹ï¼šè¯»å†™æ¨¡å¼åˆ†ç¦»ï¼Œä¸å­˜å‚¨å­—æ®µæ ‡ç­¾ï¼Œæ›´ç´§å‡‘\n{  \"type\": \"record\",  \"name\": \"Person\",  \"fields\": [    {\"name\": \"userName\", \"type\": \"string\"},    {\"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null}  ]}\n\næ•°æ®æµæ¨¡å¼\n\n\næ¨¡å¼\nåœºæ™¯\nå…¼å®¹æ€§è€ƒè™‘\n\n\n\næ•°æ®åº“\næŒä¹…å­˜å‚¨\næ•°æ®å¯èƒ½æ¯”ä»£ç æ›´æŒä¹…\n\n\næœåŠ¡è°ƒç”¨\nREST/RPC\nAPI ç‰ˆæœ¬æ§åˆ¶\n\n\næ¶ˆæ¯ä¼ é€’\né˜Ÿåˆ—/Actor\nç”Ÿäº§è€…æ¶ˆè´¹è€…è§£è€¦\n\n\n\næœ¬éƒ¨åˆ†è¦ç‚¹æ€»ç»“\nå¯é æ€§ã€å¯æ‰©å±•æ€§ã€å¯ç»´æŠ¤æ€§æ˜¯ä¼˜ç§€ç³»ç»Ÿçš„ä¸‰å¤§æ”¯æŸ±\næ•°æ®æ¨¡å‹é€‰æ‹©å–å†³äºæ•°æ®ç»“æ„å’ŒæŸ¥è¯¢éœ€æ±‚\nLSM-Tree ä¼˜åŒ–å†™å…¥ï¼ŒB-Tree ä¼˜åŒ–è¯»å–\nåˆ—å­˜å‚¨é€‚åˆ OLAPï¼Œè¡Œå­˜å‚¨é€‚åˆ OLTP\näºŒè¿›åˆ¶ç¼–ç æ¯” JSON æ›´ç´§å‡‘é«˜æ•ˆ\næ¨¡å¼æ¼”åŒ–éœ€è¦ä¿è¯å‰å‘å’Œåå‘å…¼å®¹\n\n\nè¿”å›æ€»è§ˆ | ä¸‹ä¸€éƒ¨åˆ†ï¼šåˆ†å¸ƒå¼æ•°æ®\n","categories":["è¯»ä¹¦ç¬”è®°"],"tags":["DDIA","æ•°æ®åº“","å­˜å‚¨å¼•æ“","æ•°æ®æ¨¡å‹"]},{"title":"DDIA Part 2ï¼šåˆ†å¸ƒå¼æ•°æ®","url":"/2025/12/28/DDIA-Part2-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE/","content":"æœ¬æ–‡æ˜¯ DDIA ç¬¬äºŒéƒ¨åˆ†çš„è¯»ä¹¦ç¬”è®°ï¼Œæ¶µç›–ç¬¬ 5-9 ç« ï¼šæ•°æ®å¤åˆ¶ã€åˆ†åŒºã€äº‹åŠ¡ã€åˆ†å¸ƒå¼æŒ‘æˆ˜ã€ä¸€è‡´æ€§ä¸å…±è¯†ã€‚\n\nç¬¬5ç« ï¼šæ•°æ®å¤åˆ¶å¤åˆ¶çš„ç›®çš„\n\n\nç›®çš„\nè¯´æ˜\n\n\n\né«˜å¯ç”¨æ€§\néƒ¨åˆ†èŠ‚ç‚¹æ•…éšœæ—¶ç³»ç»Ÿä»å¯ç”¨\n\n\né™ä½å»¶è¿Ÿ\næ•°æ®æ”¾åœ¨ç¦»ç”¨æˆ·æ›´è¿‘çš„åœ°æ–¹\n\n\næé«˜è¯»åå\nå¤šä¸ªå‰¯æœ¬å¹¶è¡Œå¤„ç†è¯»è¯·æ±‚\n\n\nä¸»ä»å¤åˆ¶å®¢æˆ·ç«¯ â”€â”€å†™å…¥â”€â”€&gt; ä¸»èŠ‚ç‚¹ â”€â”€å¤åˆ¶æ—¥å¿—â”€â”€&gt; ä»èŠ‚ç‚¹1                              â””â”€â”€&gt; ä»èŠ‚ç‚¹2\n\nåŒæ­¥ vs å¼‚æ­¥å¤åˆ¶ï¼š\n\n\n\næ–¹å¼\nä¼˜ç‚¹\nç¼ºç‚¹\n\n\n\nåŒæ­¥\næ•°æ®ä¸€è‡´\nå»¶è¿Ÿé«˜ï¼Œå¯ç”¨æ€§å·®\n\n\nå¼‚æ­¥\nå»¶è¿Ÿä½\nå¯èƒ½æ•°æ®ä¸¢å¤±\n\n\nåŠåŒæ­¥\nå¹³è¡¡\nå®ç°å¤æ‚\n\n\nå¤åˆ¶å»¶è¿Ÿé—®é¢˜\n\n\né—®é¢˜\nè¯´æ˜\nè§£å†³æ–¹æ¡ˆ\n\n\n\nè¯»å·±ä¹‹å†™\nå†™åè¯»å¯èƒ½è¯»åˆ°æ—§æ•°æ®\nä¿®æ”¹çš„æ•°æ®ä»ä¸»èŠ‚ç‚¹è¯»\n\n\nå•è°ƒè¯»\nåˆ·æ–°åå¯èƒ½çœ‹åˆ°æ›´æ—§çš„æ•°æ®\næ¯ä¸ªç”¨æˆ·å›ºå®šå‰¯æœ¬\n\n\nä¸€è‡´å‰ç¼€è¯»\nå› æœå…³ç³»è¢«æ‰“ä¹±\nç›¸å…³å†™å…¥åŒä¸€åˆ†åŒº\n\n\nå¤šä¸»å¤åˆ¶é€‚ç”¨åœºæ™¯ï¼šå¤šæ•°æ®ä¸­å¿ƒã€ç¦»çº¿å®¢æˆ·ç«¯ã€åä½œç¼–è¾‘\nå†™å†²çªè§£å†³ï¼š\n\n\n\nç­–ç•¥\nè¯´æ˜\n\n\n\næœ€åå†™å…¥èƒœå‡º (LWW)\næ—¶é—´æˆ³æœ€æ–°çš„è¦†ç›–\n\n\nåˆå¹¶å€¼\nå¦‚æ‹¼æ¥ â€œA/Bâ€\n\n\nCRDT\nç‰¹æ®Šæ•°æ®ç»“æ„è‡ªåŠ¨åˆå¹¶\n\n\næç¤ºç”¨æˆ·\nç±»ä¼¼ Git å†²çª\n\n\næ— ä¸»å¤åˆ¶ (Dynamo é£æ ¼)Quorum å…¬å¼ï¼š\nN=3, W=2, R=2:å†™å…¥éœ€è¦ 2 ä¸ªå‰¯æœ¬ç¡®è®¤è¯»å–éœ€è¦æŸ¥è¯¢ 2 ä¸ªå‰¯æœ¬è‡³å°‘ 1 ä¸ªå‰¯æœ¬æ—¢å†™å…¥åˆè¯»å–ï¼Œä¿è¯è¯»åˆ°æœ€æ–°å€¼\n\n\nç¬¬6ç« ï¼šæ•°æ®åˆ†åŒºåˆ†åŒºç­–ç•¥\n\n\nç­–ç•¥\nä¼˜ç‚¹\nç¼ºç‚¹\n\n\n\næŒ‰é”®èŒƒå›´\næ”¯æŒèŒƒå›´æŸ¥è¯¢\nå¯èƒ½çƒ­ç‚¹\n\n\næŒ‰é”®å“ˆå¸Œ\nåˆ†å¸ƒå‡åŒ€\nå¤±å»èŒƒå›´æŸ¥è¯¢\n\n\nä¸€è‡´æ€§å“ˆå¸Œï¼šæ·»åŠ /åˆ é™¤èŠ‚ç‚¹åªå½±å“ç›¸é‚»åŒºé—´\nçƒ­ç‚¹é—®é¢˜æ˜æ˜Ÿå‘å¾®åš â†’ æ‰€æœ‰è¯·æ±‚å‘å¾€åŒä¸€åˆ†åŒº\n\nè§£å†³æ–¹æ¡ˆï¼šæ‹†åˆ†çƒ­é”®ï¼ˆæ·»åŠ éšæœºåç¼€ï¼‰ã€æœ¬åœ°ç¼“å­˜ã€é™æµ\näºŒçº§ç´¢å¼•åˆ†åŒº\n\n\nç±»å‹\nç‰¹ç‚¹\n\n\n\næœ¬åœ°ç´¢å¼•\næ¯åˆ†åŒºç»´æŠ¤è‡ªå·±çš„ç´¢å¼•ï¼ŒæŸ¥è¯¢éœ€ scatter/gather\n\n\nå…¨å±€ç´¢å¼•\nç´¢å¼•æœ¬èº«åˆ†åŒºï¼Œè¯»å¿«å†™æ…¢ï¼ˆå¼‚æ­¥æ›´æ–°ï¼‰\n\n\nå†å¹³è¡¡ç­–ç•¥\n\n\nç­–ç•¥\nè¯´æ˜\näº§å“ç¤ºä¾‹\n\n\n\nå›ºå®šåˆ†åŒºæ•°\né¢„åˆ›å»ºå¤§é‡åˆ†åŒº\nElasticsearch\n\n\nåŠ¨æ€åˆ†åŒº\nè‡ªåŠ¨æ‹†åˆ†åˆå¹¶\nHBase\n\n\næŒ‰èŠ‚ç‚¹æ¯”ä¾‹\næ¯èŠ‚ç‚¹å›ºå®šåˆ†åŒºæ•°\nCassandra\n\n\nè¯·æ±‚è·¯ç”±æ–¹æ¡ˆ1: å®¢æˆ·ç«¯ç›´è¿ä»»æ„èŠ‚ç‚¹ â†’ è½¬å‘æ–¹æ¡ˆ2: è·¯ç”±å±‚ï¼ˆçŸ¥é“åˆ†åŒºæ˜ å°„ï¼‰æ–¹æ¡ˆ3: å®¢æˆ·ç«¯æ„ŸçŸ¥åˆ†åŒºï¼ˆå¦‚ä½¿ç”¨ ZooKeeperï¼‰\n\n\nç¬¬7ç« ï¼šäº‹åŠ¡ACID ç‰¹æ€§\n\n\nç‰¹æ€§\nå«ä¹‰\n\n\n\nåŸå­æ€§\nå…¨éƒ¨æˆåŠŸæˆ–å…¨éƒ¨å¤±è´¥\n\n\nä¸€è‡´æ€§\nä»æœ‰æ•ˆçŠ¶æ€åˆ°æœ‰æ•ˆçŠ¶æ€\n\n\néš”ç¦»æ€§\nå¹¶å‘äº‹åŠ¡äº’ä¸å¹²æ‰°\n\n\næŒä¹…æ€§\næäº¤åæ•°æ®ä¸ä¸¢å¤±\n\n\néš”ç¦»çº§åˆ«\n\n\nçº§åˆ«\né˜²æ­¢é—®é¢˜\n\n\n\nè¯»å·²æäº¤\nè„è¯»ã€è„å†™\n\n\nå¿«ç…§éš”ç¦»\nä¸å¯é‡å¤è¯»\n\n\nä¸²è¡ŒåŒ–\næ‰€æœ‰å¹¶å‘å¼‚å¸¸\n\n\nä¸¢å¤±æ›´æ–°é—®é¢˜äº‹åŠ¡1: è¯»å– counter=10     å†™å…¥ counter=11äº‹åŠ¡2:      è¯»å– counter=10        å†™å…¥ counter=11æœŸæœ›: 12ï¼Œå®é™…: 11ï¼ˆäº‹åŠ¡1çš„æ›´æ–°ä¸¢å¤±ï¼‰\n\nè§£å†³ï¼šåŸå­æ“ä½œã€æ˜¾å¼é”å®šã€Compare-and-Set\nå†™åæ–œåŒ»é™¢è§„åˆ™ï¼šè‡³å°‘1ååŒ»ç”Ÿå€¼ç­Alice æŸ¥è¯¢ï¼š2äººå€¼ç­ï¼Œå–æ¶ˆè‡ªå·±Bob åŒæ—¶ï¼š2äººå€¼ç­ï¼Œå–æ¶ˆè‡ªå·±ç»“æœï¼šæ— äººå€¼ç­ï¼\n\nè§£å†³ï¼šä¸²è¡ŒåŒ–éš”ç¦»ã€ç‰©åŒ–å†²çªã€æ˜¾å¼é”å®š\nä¸²è¡ŒåŒ–å®ç°\n\n\næ–¹å¼\nç‰¹ç‚¹\näº§å“\n\n\n\nå®é™…ä¸²è¡Œ\nå•çº¿ç¨‹æ‰§è¡Œ\nRedis, VoltDB\n\n\nä¸¤é˜¶æ®µé” (2PL)\nè¯»å†™äº’æ–¥\nä¼ ç»Ÿæ•°æ®åº“\n\n\nä¸²è¡ŒåŒ–å¿«ç…§éš”ç¦» (SSI)\nä¹è§‚å¹¶å‘\nPostgreSQL 9.1+\n\n\nåˆ†å¸ƒå¼äº‹åŠ¡ (2PC)é˜¶æ®µ1: åè°ƒè€… â”€â”€å‡†å¤‡?â”€â”€&gt; æ‰€æœ‰å‚ä¸è€…é˜¶æ®µ2: åè°ƒè€… â”€â”€æäº¤/å›æ»šâ”€â”€&gt; æ‰€æœ‰å‚ä¸è€…é—®é¢˜ï¼šåè°ƒè€…æ•…éšœæ—¶å‚ä¸è€…é˜»å¡\n\n\nç¬¬8ç« ï¼šåˆ†å¸ƒå¼ç³»ç»Ÿçš„æŒ‘æˆ˜éƒ¨åˆ†å¤±æ•ˆ\n\n\nå•æœºç³»ç»Ÿ\nåˆ†å¸ƒå¼ç³»ç»Ÿ\n\n\n\nè¦ä¹ˆå·¥ä½œè¦ä¹ˆä¸å·¥ä½œ\néƒ¨åˆ†å¯èƒ½å·¥ä½œ\n\n\næ•…éšœé€šå¸¸å®Œå…¨\næ•…éšœé€šå¸¸éƒ¨åˆ†\n\n\nä¸å¯é çš„ç½‘ç»œè¯·æ±‚å¯èƒ½ï¼šä¸¢å¤±ã€å»¶è¿Ÿã€é‡å¤å“åº”å¯èƒ½ï¼šä¸¢å¤±ã€å»¶è¿Ÿæ— æ³•åŒºåˆ†ï¼šç½‘ç»œæ•…éšœ vs èŠ‚ç‚¹æ•…éšœ\n\nè¶…æ—¶å›°å¢ƒï¼šå¤ªçŸ­è¯¯åˆ¤æ­£å¸¸èŠ‚ç‚¹ï¼Œå¤ªé•¿æ¢å¤æ…¢\nä¸å¯é çš„æ—¶é’Ÿ\n\n\næ—¶é’Ÿç±»å‹\nç”¨é€”\n\n\n\næ—¥å†æ—¶é’Ÿ\nå½“å‰æ—¶é—´ï¼ˆå¯èƒ½è·³è·ƒï¼‰\n\n\nå•è°ƒæ—¶é’Ÿ\næµ‹é‡æŒç»­æ—¶é—´ï¼ˆä¿è¯é€’å¢ï¼‰\n\n\nLWW çš„é£é™©ï¼šæ—¶é’Ÿä¸åŒæ­¥å¯¼è‡´æ–°æ•°æ®è¢«æ—§æ•°æ®è¦†ç›–\nFencing TokenèŠ‚ç‚¹A è·å–é” token=33 â†’ æš‚åœèŠ‚ç‚¹B è·å–é” token=34 â†’ å†™å…¥æˆåŠŸèŠ‚ç‚¹A æ¢å¤ token=33 â†’ è¢«æ‹’ç»ï¼ˆ33 &lt; 34ï¼‰\n\nç³»ç»Ÿæ¨¡å‹\n\n\næ—¶åºå‡è®¾\nèŠ‚ç‚¹æ•…éšœå‡è®¾\n\n\n\nåŒæ­¥/éƒ¨åˆ†åŒæ­¥/å¼‚æ­¥\nå´©æºƒ-åœæ­¢/å´©æºƒ-æ¢å¤/æ‹œå åº­\n\n\næ­£ç¡®æ€§ï¼šå®‰å…¨æ€§ï¼ˆåäº‹ä¸å‘ç”Ÿï¼‰ + æ´»æ€§ï¼ˆå¥½äº‹æœ€ç»ˆå‘ç”Ÿï¼‰\n\nç¬¬9ç« ï¼šä¸€è‡´æ€§ä¸å…±è¯†ä¸€è‡´æ€§æ¨¡å‹å¼± â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ å¼ºæœ€ç»ˆä¸€è‡´æ€§    å› æœä¸€è‡´æ€§    é¡ºåºä¸€è‡´æ€§    çº¿æ€§ä¸€è‡´æ€§\n\nçº¿æ€§ä¸€è‡´æ€§\nç³»ç»Ÿè¡¨ç°å¾—å¥½åƒåªæœ‰ä¸€ä¸ªæ•°æ®å‰¯æœ¬ï¼Œæ‰€æœ‰æ“ä½œéƒ½æ˜¯åŸå­çš„\n\nåº”ç”¨ï¼šåˆ†å¸ƒå¼é”ã€é¢†å¯¼è€…é€‰ä¸¾ã€å”¯ä¸€æ€§çº¦æŸ\nCAP å®šç†ç½‘ç»œåˆ†åŒºæ—¶ï¼Œä¸€è‡´æ€§ä¸å¯ç”¨æ€§ä¸å¯å…¼å¾—\n\n\n\né€‰æ‹©\nå«ä¹‰\n\n\n\nCP\nä¿è¯ä¸€è‡´æ€§ï¼Œç‰ºç‰²å¯ç”¨æ€§\n\n\nAP\nä¿è¯å¯ç”¨æ€§ï¼Œç‰ºç‰²ä¸€è‡´æ€§\n\n\nå…±è¯†é—®é¢˜\nå¤šä¸ªèŠ‚ç‚¹å°±æŸä¸ªå€¼è¾¾æˆä¸€è‡´\n\næ€§è´¨ï¼šä¸€è‡´åŒæ„ã€å®Œæ•´æ€§ã€ç»ˆæ­¢æ€§ã€æœ‰æ•ˆæ€§\nFLP ä¸å¯èƒ½å®šç†ï¼šå¼‚æ­¥ç³»ç»Ÿä¸­å­˜åœ¨æ•…éšœèŠ‚ç‚¹æ—¶ï¼Œä¸å­˜åœ¨æ€»èƒ½è¾¾æˆå…±è¯†çš„ç®—æ³•\nPaxos ç®—æ³•é˜¶æ®µ1 (Prepare):æè®®è€… â”€â”€Prepare(n)â”€â”€&gt; æ¥å—è€…        &lt;â”€â”€Promiseâ”€â”€é˜¶æ®µ2 (Accept):æè®®è€… â”€â”€Accept(n,v)â”€â”€&gt; æ¥å—è€…        &lt;â”€â”€Acceptedâ”€â”€\n\nRaft ç®—æ³•æ¯” Paxos æ›´æ˜“ç†è§£\nè§’è‰²ï¼šé¢†å¯¼è€…ã€è·Ÿéšè€…ã€å€™é€‰äººä»»æœŸï¼šé€»è¾‘æ—¶é’Ÿï¼Œæ¯æ¬¡é€‰ä¸¾é€’å¢é€‰ä¸¾æµç¨‹ï¼š1. è·Ÿéšè€…è¶…æ—¶ â†’ å˜å€™é€‰äºº2. è¯·æ±‚æŠ•ç¥¨ â†’ è·å¤šæ•°ç¥¨3. æˆä¸ºé¢†å¯¼è€… â†’ å¤åˆ¶æ—¥å¿—\n\nå…±è¯†çš„åº”ç”¨\n\n\näº§å“\nç”¨é€”\n\n\n\nZooKeeper\nåè°ƒæœåŠ¡ã€é…ç½®ç®¡ç†\n\n\netcd\nKubernetes çŠ¶æ€å­˜å‚¨\n\n\nConsul\næœåŠ¡å‘ç°\n\n\n\næœ¬éƒ¨åˆ†è¦ç‚¹æ€»ç»“\nå¤åˆ¶çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯å¤„ç†æ•°æ®å˜æ›´\nåˆ†åŒºç­–ç•¥éœ€è¦æƒè¡¡èŒƒå›´æŸ¥è¯¢å’Œè´Ÿè½½å‡è¡¡\näº‹åŠ¡éš”ç¦»çº§åˆ«æ˜¯æ­£ç¡®æ€§å’Œæ€§èƒ½çš„æƒè¡¡\nåˆ†å¸ƒå¼ç³»ç»Ÿçš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯éƒ¨åˆ†å¤±æ•ˆ\nçº¿æ€§ä¸€è‡´æ€§æ˜¯æœ€å¼ºä¿è¯ï¼Œä½†ä»£ä»·é«˜æ˜‚\nRaft æ¯” Paxos æ›´æ˜“ç†è§£ï¼Œé€‚åˆå­¦ä¹ \n\n\nä¸Šä¸€éƒ¨åˆ†ï¼šæ•°æ®ç³»ç»ŸåŸºç¡€ | è¿”å›æ€»è§ˆ | ä¸‹ä¸€éƒ¨åˆ†ï¼šè¡ç”Ÿæ•°æ®\n","categories":["è¯»ä¹¦ç¬”è®°"],"tags":["DDIA","åˆ†å¸ƒå¼ç³»ç»Ÿ","ä¸€è‡´æ€§","å…±è¯†ç®—æ³•"]},{"title":"DDIA è¯»ä¹¦ç¬”è®°ï¼šæ•°æ®å¯†é›†å‹åº”ç”¨ç³»ç»Ÿè®¾è®¡","url":"/2025/12/28/DDIA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%80%BB%E8%A7%88/","content":"\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems\n\nè¿™æ˜¯ä¸€ä»½å…³äºã€Šæ•°æ®å¯†é›†å‹åº”ç”¨ç³»ç»Ÿè®¾è®¡ã€‹(DDIA) çš„å®Œæ•´è¯»ä¹¦ç¬”è®°ï¼Œæœ¬ä¹¦è¢«èª‰ä¸ºâ€æ•°æ®ç³»ç»Ÿé¢†åŸŸçš„åœ£ç»â€ã€‚\nä¹¦ç±ä¿¡æ¯\n\n\né¡¹ç›®\nå†…å®¹\n\n\n\nä¹¦å\nDesigning Data-Intensive Applications (DDIA)\n\n\nä¸­æ–‡å\næ•°æ®å¯†é›†å‹åº”ç”¨ç³»ç»Ÿè®¾è®¡\n\n\nä½œè€…\nMartin Kleppmannï¼ˆå‰‘æ¡¥å¤§å­¦åˆ†å¸ƒå¼ç³»ç»Ÿç ”ç©¶å‘˜ï¼‰\n\n\nå‡ºç‰ˆæ—¶é—´\n2017å¹´3æœˆ\n\n\næ ¸å¿ƒä¸»é¢˜æœ¬ä¹¦å›´ç»•ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µå±•å¼€ï¼š\n\nå¯é æ€§ (Reliability)ï¼šç³»ç»Ÿåœ¨é‡åˆ°æ•…éšœæ—¶ä»èƒ½æ­£ç¡®å·¥ä½œ\nå¯æ‰©å±•æ€§ (Scalability)ï¼šç³»ç»Ÿèƒ½å¤Ÿåº”å¯¹è´Ÿè½½å¢é•¿\nå¯ç»´æŠ¤æ€§ (Maintainability)ï¼šç³»ç»Ÿæ˜“äºç†è§£ã€ä¿®æ”¹å’Œæ‰©å±•\n\nå…¨ä¹¦ç»“æ„ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®ç³»ç»ŸåŸºç¡€\næŸ¥çœ‹è¯¦ç»†ç¬”è®°\n\n\n\n\nç« èŠ‚\næ ¸å¿ƒå†…å®¹\n\n\n\nç¬¬1ç« \nå¯é æ€§ã€å¯æ‰©å±•æ€§ã€å¯ç»´æŠ¤æ€§çš„å®šä¹‰ä¸å®è·µ\n\n\nç¬¬2ç« \nå…³ç³»æ¨¡å‹ã€æ–‡æ¡£æ¨¡å‹ã€å›¾æ¨¡å‹çš„å¯¹æ¯”ä¸é€‰æ‹©\n\n\nç¬¬3ç« \nå­˜å‚¨å¼•æ“åŸç†ï¼šB-Treeã€LSM-Treeã€OLTP vs OLAP\n\n\nç¬¬4ç« \næ•°æ®ç¼–ç æ ¼å¼ä¸æ¨¡å¼æ¼”åŒ–ï¼šJSONã€Protobufã€Avro\n\n\nç¬¬äºŒéƒ¨åˆ†ï¼šåˆ†å¸ƒå¼æ•°æ®\næŸ¥çœ‹è¯¦ç»†ç¬”è®°\n\n\n\n\nç« èŠ‚\næ ¸å¿ƒå†…å®¹\n\n\n\nç¬¬5ç« \næ•°æ®å¤åˆ¶ï¼šä¸»ä»ã€å¤šä¸»ã€æ— ä¸»å¤åˆ¶ç­–ç•¥\n\n\nç¬¬6ç« \næ•°æ®åˆ†åŒºï¼šåˆ†åŒºç­–ç•¥ã€å†å¹³è¡¡ã€è¯·æ±‚è·¯ç”±\n\n\nç¬¬7ç« \näº‹åŠ¡ï¼šACIDã€éš”ç¦»çº§åˆ«ã€åˆ†å¸ƒå¼äº‹åŠ¡\n\n\nç¬¬8ç« \nåˆ†å¸ƒå¼ç³»ç»ŸæŒ‘æˆ˜ï¼šç½‘ç»œã€æ—¶é’Ÿã€æ•…éšœæ¨¡å‹\n\n\nç¬¬9ç« \nä¸€è‡´æ€§ä¸å…±è¯†ï¼šCAPã€Paxosã€Raft\n\n\nç¬¬ä¸‰éƒ¨åˆ†ï¼šè¡ç”Ÿæ•°æ®\næŸ¥çœ‹è¯¦ç»†ç¬”è®°\n\n\n\n\nç« èŠ‚\næ ¸å¿ƒå†…å®¹\n\n\n\nç¬¬10ç« \næ‰¹å¤„ç†ï¼šMapReduceã€Sparkã€æ•°æ®æµå¼•æ“\n\n\nç¬¬11ç« \næµå¤„ç†ï¼šKafkaã€Flinkã€äº‹ä»¶æ—¶é—´ä¸æ°´ä½çº¿\n\n\nç¬¬12ç« \næ•°æ®ç³»ç»Ÿæœªæ¥ï¼šæ•°æ®é›†æˆã€ç«¯åˆ°ç«¯æ­£ç¡®æ€§ã€ä¼¦ç†\n\n\nå­¦ä¹ è·¯çº¿å…¥é—¨è·¯çº¿ï¼ˆé€‚åˆåˆå­¦è€…ï¼‰ç¬¬1ç«  â†’ ç¬¬2ç«  â†’ ç¬¬3ç«  â†’ ç¬¬4ç« ï¼ˆå»ºç«‹åŸºç¡€ï¼‰    â†“ç¬¬5ç«  â†’ ç¬¬6ç« ï¼ˆç†è§£åˆ†å¸ƒå¼åŸºç¡€ï¼‰    â†“ç¬¬10ç«  â†’ ç¬¬11ç« ï¼ˆäº†è§£æ•°æ®å¤„ç†ï¼‰\n\nè¿›é˜¶è·¯çº¿ï¼ˆé€‚åˆæœ‰ç»éªŒçš„å¼€å‘è€…ï¼‰ç¬¬7ç«  â†’ ç¬¬8ç«  â†’ ç¬¬9ç« ï¼ˆæ·±å…¥åˆ†å¸ƒå¼ï¼‰    â†“ç¬¬12ç« ï¼ˆå±•æœ›æœªæ¥ï¼‰    â†“å›é¡¾ç¬¬1-4ç« å¡«è¡¥çŸ¥è¯†ç©ºç™½\n\nä¸“é¢˜è·¯çº¿\n\n\næ–¹å‘\næ¨èé˜…è¯»é¡ºåº\n\n\n\næ•°æ®åº“\n2 â†’ 3 â†’ 5 â†’ 6 â†’ 7\n\n\nåˆ†å¸ƒå¼ç³»ç»Ÿ\n5 â†’ 6 â†’ 8 â†’ 9\n\n\næ•°æ®å·¥ç¨‹\n3 â†’ 10 â†’ 11 â†’ 12\n\n\næ ¸å¿ƒè¦ç‚¹é€Ÿè§ˆæ•°æ®æ¨¡å‹é€‰æ‹©å…³ç³»æ¨¡å‹ â”€â”€â”€â”€ ç»“æ„åŒ–æ•°æ®ã€å¤æ‚æŸ¥è¯¢ã€äº‹åŠ¡æ”¯æŒ     â†“æ–‡æ¡£æ¨¡å‹ â”€â”€â”€â”€ çµæ´»æ¨¡å¼ã€æ ‘çŠ¶ç»“æ„ã€å±€éƒ¨æ€§å¥½     â†“å›¾æ¨¡å‹ â”€â”€â”€â”€â”€ å¤æ‚å…³ç³»ã€ç¤¾äº¤ç½‘ç»œã€çŸ¥è¯†å›¾è°±\n\nå­˜å‚¨å¼•æ“å¯¹æ¯”\n\n\nå¼•æ“\nä¼˜åŒ–ç›®æ ‡\nå…¸å‹åº”ç”¨\n\n\n\nB-Tree\nè¯»å–ä¼˜åŒ–\nOLTP æ•°æ®åº“\n\n\nLSM-Tree\nå†™å…¥ä¼˜åŒ–\næ—¥å¿—ã€æ—¶åºæ•°æ®\n\n\nåˆ—å­˜å‚¨\nåˆ†æä¼˜åŒ–\nOLAPã€æ•°æ®ä»“åº“\n\n\nåˆ†å¸ƒå¼ç³»ç»Ÿæ ¸å¿ƒæƒè¡¡å®šç†ï¼šç½‘ç»œåˆ†åŒºæ—¶ï¼Œä¸€è‡´æ€§ä¸å¯ç”¨æ€§ä¸å¯å…¼å¾—\nå¤„ç†èŒƒå¼å¯¹æ¯”\n\n\nèŒƒå¼\næ•°æ®ç‰¹æ€§\nå»¶è¿Ÿ\nå…¸å‹æ¡†æ¶\n\n\n\næ‰¹å¤„ç†\næœ‰ç•Œã€é™æ€\nåˆ†é’Ÿ~å°æ—¶\nSpark, Hadoop\n\n\næµå¤„ç†\næ— ç•Œã€æŒç»­\næ¯«ç§’~ç§’\nFlink, Kafka Streams\n\n\nå»¶ä¼¸èµ„æº\nå®˜æ–¹ç½‘ç«™ï¼šdataintensive.net\nä¸­æ–‡ç¿»è¯‘ï¼šddia.vonng.com\nä½œè€…åšå®¢ï¼šmartin.kleppmann.com\n\n\næœ¬è¯»ä¹¦ç¬”è®°æ•´ç†äº 2025å¹´ï¼ŒåŸºäº DDIA ç¬¬ä¸€ç‰ˆå†…å®¹ç¼–å†™\n","categories":["è¯»ä¹¦ç¬”è®°"],"tags":["DDIA","æ•°æ®åº“","åˆ†å¸ƒå¼ç³»ç»Ÿ","ç³»ç»Ÿè®¾è®¡"]},{"title":"ç¥ç»ç½‘ç»œæœºå™¨é˜…è¯»ç†è§£ï¼šä» Attention åˆ° LLM","url":"/2019/11/22/Nuural-Approaches-to-Machine-Reading-Comprehension-and-Dialogue/","content":"æœ¬æ–‡ç»¼è¿°ç¥ç»ç½‘ç»œåœ¨æœºå™¨é˜…è¯»ç†è§£å’Œå¯¹è¯ç³»ç»Ÿä¸­çš„å‘å±•å†ç¨‹ï¼Œä»æ—©æœŸçš„æ³¨æ„åŠ›æœºåˆ¶åˆ°ç°ä»£å¤§è¯­è¨€æ¨¡å‹ã€‚\nå‘å±•æ—¶é—´çº¿2015-2016: æ³¨æ„åŠ›æœºåˆ¶å…´èµ·    â””â”€â”€ Attentive Reader, Impatient Reader, BiDAF2017-2018: æ·±åº¦äº¤äº’ä¸é¢„è®­ç»ƒ    â””â”€â”€ R-Net, QANet, BERT2019-2020: å¤§è§„æ¨¡é¢„è®­ç»ƒ    â””â”€â”€ RoBERTa, ALBERT, T52021-2023: å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£    â””â”€â”€ GPT-3, ChatGPT, GPT-4, LLaMA2024-: æ£€ç´¢å¢å¼ºä¸å¤šæ¨¡æ€    â””â”€â”€ RAG, Vision-Language Models\n\næ ¸å¿ƒæŠ€æœ¯æ¼”è¿›é˜¶æ®µä¸€ï¼šæ³¨æ„åŠ›æœºåˆ¶ (2015-2017)é—®é¢˜ï¼šå¦‚ä½•è®©æ¨¡å‹â€å…³æ³¨â€ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Ÿ\n\n\nä»£è¡¨æ¨¡å‹ï¼šAttentive Reader, BiDAF\né˜¶æ®µäºŒï¼šæ·±åº¦äº¤äº’ (2017-2018)é—®é¢˜ï¼šå¦‚ä½•å»ºæ¨¡é—®é¢˜å’Œä¸Šä¸‹æ–‡çš„å¤æ‚äº¤äº’ï¼Ÿ\næŠ€æœ¯ï¼šå¤šè½®æ³¨æ„åŠ›ã€è‡ªæ³¨æ„åŠ›ã€é—¨æ§æœºåˆ¶\n# å¤šè½®æ¨ç† (R-Net é£æ ¼)for layer in range(num_layers):    # è‡ªæ³¨æ„åŠ›    context = self_attention(context, context)    # äº¤å‰æ³¨æ„åŠ›    context = cross_attention(context, question)\n\né˜¶æ®µä¸‰ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (2018-2020)èŒƒå¼è½¬å˜ï¼šä» task-specific åˆ° pretrain-finetune\n$$\\theta^* = \\arg\\min_\\theta \\mathcal{L}{task}(\\text{PLM}\\theta(x), y)$$\nä»£è¡¨æ¨¡å‹ï¼šBERT, RoBERTa, ALBERT\nfrom transformers import AutoModelForQuestionAnsweringmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")# Fine-tune on SQuAD\n\né˜¶æ®µå››ï¼šå¤§è¯­è¨€æ¨¡å‹ (2020-è‡³ä»Š)èŒƒå¼è½¬å˜ï¼šä» fine-tuning åˆ° prompting\n# Few-shot promptingprompt = \"\"\"Context: The Eiffel Tower was built in 1889.Question: When was the Eiffel Tower built?Answer: 1889Context: {context}Question: {question}Answer:\"\"\"\n\næ¶æ„å¯¹æ¯”\n\n\næ¨¡å‹\nå‚æ•°é‡\nè®­ç»ƒèŒƒå¼\nSQuAD 2.0 F1\n\n\n\nBiDAF\n~2M\nä»é›¶è®­ç»ƒ\n77.3\n\n\nBERT-base\n110M\né¢„è®­ç»ƒ+å¾®è°ƒ\n88.5\n\n\nBERT-large\n340M\né¢„è®­ç»ƒ+å¾®è°ƒ\n90.9\n\n\nRoBERTa-large\n355M\né¢„è®­ç»ƒ+å¾®è°ƒ\n91.4\n\n\nGPT-3\n175B\nFew-shot\n~88\n\n\nGPT-4\n~1.8T\nZero-shot\n~95\n\n\nç°ä»£ MRC ç³»ç»Ÿè®¾è®¡RAG æ¶æ„class ModernMRC:    def __init__(self, retriever, reader):        self.retriever = retriever  # Dense retriever        self.reader = reader        # LLM        def answer(self, question: str, knowledge_base: str = None):        # 1. æ£€ç´¢        if knowledge_base:            docs = self.retriever.retrieve(question, knowledge_base)            context = \"\\n\\n\".join([d.text for d in docs])        else:            context = \"\"                # 2. é˜…è¯»ç†è§£/ç”Ÿæˆ        prompt = self._build_prompt(question, context)        answer = self.reader.generate(prompt)                # 3. åå¤„ç†ï¼ˆå¯é€‰ï¼šéªŒè¯ã€å¼•ç”¨ï¼‰        return self._postprocess(answer, docs)        def _build_prompt(self, question, context):        if context:            return f\"\"\"Based on the following context, answer the question.Context:{context}Question: {question}Answer:\"\"\"        else:            return f\"Question: {question}\\nAnswer:\"\n\nå¤šè·³æ¨ç†class MultiHopReasoner:    def __init__(self, retriever, llm, max_hops=3):        self.retriever = retriever        self.llm = llm        self.max_hops = max_hops        def reason(self, question):        reasoning_chain = []        current_query = question                for hop in range(self.max_hops):            # æ£€ç´¢            docs = self.retriever.retrieve(current_query)                        # ç”Ÿæˆä¸­é—´æ¨ç†            intermediate = self.llm.generate(                f\"Based on: {docs}\\nQuestion: {current_query}\\n\"                f\"Provide intermediate reasoning or the final answer:\"            )                        reasoning_chain.append({                'query': current_query,                'docs': docs,                'reasoning': intermediate            })                        # æ£€æŸ¥æ˜¯å¦å·²å¾—åˆ°ç­”æ¡ˆ            if self._is_final_answer(intermediate):                break                        # ç”Ÿæˆä¸‹ä¸€è·³æŸ¥è¯¢            current_query = self._generate_next_query(question, reasoning_chain)                return self._synthesize_answer(question, reasoning_chain)\n\nå¯¹è¯ç³»ç»Ÿä¸­çš„ MRCå¯¹è¯å¼é—®ç­”class ConversationalQA:    def __init__(self, mrc_model, history_length=5):        self.mrc_model = mrc_model        self.history = []        self.history_length = history_length        def ask(self, question, context=None):        # å°†å¯¹è¯å†å²çº³å…¥é—®é¢˜        contextualized_question = self._contextualize(question)                # è·å–ç­”æ¡ˆ        answer = self.mrc_model.answer(contextualized_question, context)                # æ›´æ–°å†å²        self.history.append({'q': question, 'a': answer})        if len(self.history) &gt; self.history_length:            self.history.pop(0)                return answer        def _contextualize(self, question):        if not self.history:            return question                history_text = \"\\n\".join([            f\"Q: {turn['q']}\\nA: {turn['a']}\"            for turn in self.history        ])                return f\"Conversation history:\\n{history_text}\\n\\nCurrent question: {question}\"\n\nè¯„ä¼°ä½“ç³»ä¼ ç»ŸæŒ‡æ ‡\n\n\næŒ‡æ ‡\nå®šä¹‰\né€‚ç”¨åœºæ™¯\n\n\n\nEM\nç²¾ç¡®åŒ¹é…\næŠ½å–å¼ QA\n\n\nF1\nToken é‡å \næŠ½å–å¼ QA\n\n\nBLEU\nN-gram é‡å \nç”Ÿæˆå¼ QA\n\n\nROUGE\nå¬å›å¯¼å‘é‡å \næ‘˜è¦ã€é•¿ç­”æ¡ˆ\n\n\nLLM æ—¶ä»£æŒ‡æ ‡# LLM-as-Judgedef llm_evaluate(question, reference, prediction):    prompt = f\"\"\"Evaluate the answer quality on a scale of 1-5:Question: {question}Reference Answer: {reference}Model Answer: {prediction}Criteria:- Correctness: Is the information accurate?- Completeness: Does it fully answer the question?- Conciseness: Is it appropriately brief?Score (1-5):\"\"\"        return llm.generate(prompt)\n\nå»¶ä¼¸é˜…è¯»\nReading Wikipedia to Answer Open-Domain Questions\nRAG Paper\nHotpotQA: Multi-hop Reasoning\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","KBQA","Deep learning","LLM"]},{"title":"å› æœå…³ç³»æ¨æ–­ä»‹ç»","url":"/2019/10/03/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/","content":"å› æœæ¨æ–­æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œç†è§£å› æœå…³ç³»å¯¹äºæ„å»ºå¯è§£é‡Šã€å¯ä¿¡èµ–çš„ AI ç³»ç»Ÿè‡³å…³é‡è¦ã€‚\nä¸ºä»€ä¹ˆéœ€è¦å› æœæ¨æ–­ï¼Ÿä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¾èµ–ç›¸å…³æ€§ï¼Œä½†ç›¸å…³æ€§ä¸ç­‰äºå› æœæ€§ã€‚ä¾‹å¦‚ï¼š\n\nå†°æ·‡æ·‹é”€é‡ä¸æººæ°´äº‹ä»¶æ­£ç›¸å…³ï¼ˆå…±åŒåŸå› ï¼šå¤å¤©ï¼‰\nLLM å¯èƒ½å­¦åˆ°è™šå‡ç›¸å…³æ€§ï¼Œå¯¼è‡´ hallucination\n\nå› æœæ¨æ–­å¸®åŠ©æˆ‘ä»¬ï¼š\n\nç†è§£å¹²é¢„æ•ˆæœï¼ˆå¦‚æœæˆ‘åš Xï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿï¼‰\nè¿›è¡Œåäº‹å®æ¨ç†ï¼ˆå¦‚æœå½“æ—¶åšäº† Yï¼Œç»“æœä¼šæ€æ ·ï¼Ÿï¼‰\næ„å»ºæ›´é²æ£’çš„æ¨¡å‹\n\næ ¸å¿ƒæ¦‚å¿µå› æœå›¾ (Causal Graph)ä½¿ç”¨æœ‰å‘æ— ç¯å›¾ (DAG) è¡¨ç¤ºå˜é‡ä¹‹é—´çš„å› æœå…³ç³»ï¼š\nX â†’ Y â†’ Z    (é“¾å¼ç»“æ„)X â† W â†’ Y    (æ··æ‚ç»“æ„)  X â†’ W â† Y    (å¯¹æ’ç»“æ„)\n\nç»“æ„å› æœæ¨¡å‹ (SCM)\nå…¶ä¸­  æ˜¯åŸå› ï¼Œ æ˜¯ç»“æœï¼Œ æ˜¯å™ªå£°é¡¹ã€‚\ndo ç®—å­ä¸å¹²é¢„åŒºåˆ†è§‚æµ‹å’Œå¹²é¢„ï¼š\n\nè§‚æµ‹ï¼š â€” çœ‹åˆ° X=x æ—¶ Y çš„åˆ†å¸ƒ\nå¹²é¢„ï¼š â€” å¼ºåˆ¶è®¾ç½® X=x æ—¶ Y çš„åˆ†å¸ƒ\n\nå› æœå‘ç°ç®—æ³•PC ç®—æ³•åŸºäºæ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒçš„ç»å…¸ç®—æ³•ï¼š\n# PC ç®—æ³•ä¼ªä»£ç def pc_algorithm(data, alpha=0.05):    # 1. åˆå§‹åŒ–å®Œå…¨å›¾    G = complete_graph(variables)        # 2. éª¨æ¶å­¦ä¹ ï¼šç§»é™¤æ¡ä»¶ç‹¬ç«‹çš„è¾¹    for (X, Y) in edges(G):        for S in subsets(neighbors):            if conditional_independent(X, Y, S, alpha):                remove_edge(G, X, Y)                sep_set[X, Y] = S        # 3. æ–¹å‘ç¡®å®šï¼šè¯†åˆ« v-structure    orient_v_structures(G, sep_set)        return G\n\nPython å®ç°å‚è€ƒï¼šfooSynaptic/py_pcalg\nç°ä»£æ–¹æ³•\n\n\næ–¹æ³•\nç‰¹ç‚¹\né€‚ç”¨åœºæ™¯\n\n\n\nNOTEARS\nè¿ç»­ä¼˜åŒ–ï¼Œå¯å¾®åˆ†\nçº¿æ€§/éçº¿æ€§å› æœå‘ç°\n\n\nDAG-GNN\nåŸºäºå›¾ç¥ç»ç½‘ç»œ\nå¤§è§„æ¨¡å› æœå›¾å­¦ä¹ \n\n\nCausal Transformer\nç»“åˆæ³¨æ„åŠ›æœºåˆ¶\næ—¶åºå› æœæ¨æ–­\n\n\nå› æœæ¨æ–­ä¸å¤§è¯­è¨€æ¨¡å‹LLM ä¸­çš„å› æœé—®é¢˜\nHallucinationï¼šæ¨¡å‹å­¦åˆ°è™šå‡ç›¸å…³æ€§\nBiasï¼šè®­ç»ƒæ•°æ®ä¸­çš„æ··æ‚å› ç´ \nRobustnessï¼šåˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›å·®\n\nè§£å†³æ–¹æ¡ˆ# å› æœæç¤º (Causal Prompting) ç¤ºä¾‹prompt = \"\"\"è¯·åˆ†æä»¥ä¸‹äº‹ä»¶çš„å› æœå…³ç³»ï¼Œè€Œéç›¸å…³æ€§ï¼šäº‹ä»¶A: å…¬å¸å¢åŠ å¹¿å‘ŠæŠ•å…¥äº‹ä»¶B: é”€å”®é¢ä¸Šå‡é—®ï¼šA æ˜¯å¦å¯¼è‡´äº† Bï¼Ÿè¯·è€ƒè™‘å¯èƒ½çš„æ··æ‚å› ç´ ã€‚\"\"\"\n\nå› æœæ¨ç†å¢å¼º RAGclass CausalRAG:    def __init__(self, retriever, causal_graph):        self.retriever = retriever        self.causal_graph = causal_graph        def retrieve(self, query):        # 1. è¯†åˆ«æŸ¥è¯¢ä¸­çš„å› æœå…³ç³»        cause, effect = extract_causal_pair(query)                # 2. åŸºäºå› æœå›¾è¿‡æ»¤æ— å…³æ–‡æ¡£        relevant_vars = self.causal_graph.ancestors(effect)                # 3. æ£€ç´¢å› æœç›¸å…³çš„æ–‡æ¡£        docs = self.retriever.search(query)        return filter_by_causal_relevance(docs, relevant_vars)\n\nå·¥å…·ä¸èµ„æº\n\n\nå·¥å…·\nè¯­è¨€\nåŠŸèƒ½\n\n\n\nDoWhy\nPython\nå› æœæ¨æ–­æ¡†æ¶\n\n\nCausalNex\nPython\nè´å¶æ–¯ç½‘ç»œ + å› æœå‘ç°\n\n\npgmpy\nPython\næ¦‚ç‡å›¾æ¨¡å‹\n\n\nTetrad\nJava\nå› æœæœç´¢ç®—æ³•\n\n\n# DoWhy ç¤ºä¾‹import dowhyfrom dowhy import CausalModelmodel = CausalModel(    data=df,    treatment='treatment',    outcome='outcome',    graph='digraph {treatment -&gt; outcome; confounder -&gt; treatment; confounder -&gt; outcome}')# è¯†åˆ«å› æœæ•ˆåº”identified = model.identify_effect()# ä¼°è®¡å› æœæ•ˆåº”estimate = model.estimate_effect(identified, method_name=\"backdoor.propensity_score_matching\")\n\nå»¶ä¼¸é˜…è¯»\nJudea Pearl, The Book of Why (2018)\nPeters et al., Elements of Causal Inference (2017)\nStanford CS 228: Probabilistic Graphical Models\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["bayesian network","machine learning","causality infer"]},{"title":"NLP å­¦ä¹ è·¯çº¿ï¼šä»åŸºç¡€åˆ°å¤§è¯­è¨€æ¨¡å‹","url":"/2019/10/31/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E2%80%94%E2%80%94%E8%AF%BB%E9%A6%99%E4%BE%AC%E7%A7%91%E6%8A%80%E6%9D%8E%E7%BA%A7%E4%B8%BA%E3%80%8A%E5%87%BA%E5%85%A5NLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BB%BA%E8%AE%AE%E3%80%8B%E6%96%87%E7%AB%A0/","content":"æœ¬æ–‡æ•´ç†äº† NLP é¢†åŸŸçš„å­¦ä¹ è·¯çº¿ï¼Œç»“åˆç»å…¸ç†è®ºä¸ç°ä»£å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ã€‚\næ¨èå­¦ä¹ èµ„æºç»å…¸æ•™æ\n\n\nä¹¦ç±\nå†…å®¹\néš¾åº¦\n\n\n\nSpeech and Language Processing (Jurafsky)\nNLP å…¨é¢ç»¼è¿°\nâ­â­\n\n\nIntroduction to Information Retrieval\nä¿¡æ¯æ£€ç´¢åŸºç¡€\nâ­â­\n\n\nPattern Recognition and Machine Learning\næœºå™¨å­¦ä¹ ç†è®º\nâ­â­â­â­\n\n\nDeep Learning (Goodfellow)\næ·±åº¦å­¦ä¹ åŸºç¡€\nâ­â­â­\n\n\nç°ä»£èµ„æº\nStanford CS224N: NLP with Deep Learning\nHugging Face Course\nLLM University by Cohere\n\né˜¶æ®µä¸€ï¼šNLP åŸºç¡€è¯­è¨€æ¨¡å‹åŸºç¡€N-gram æ¨¡å‹ï¼šN-1 é˜¶é©¬å°”å¯å¤«å‡è®¾\n\nfrom collections import defaultdictimport numpy as npclass NGramLM:    def __init__(self, n=3):        self.n = n        self.counts = defaultdict(lambda: defaultdict(int))        self.totals = defaultdict(int)        def train(self, corpus):        for sentence in corpus:            tokens = ['&lt;s&gt;'] * (self.n - 1) + sentence + ['&lt;/s&gt;']            for i in range(len(tokens) - self.n + 1):                context = tuple(tokens[i:i+self.n-1])                word = tokens[i+self.n-1]                self.counts[context][word] += 1                self.totals[context] += 1        def probability(self, word, context):        context = tuple(context[-(self.n-1):])        return self.counts[context][word] / max(self.totals[context], 1)\n\nè¯å‘é‡ä» One-hot åˆ° Dense Embedding çš„æ¼”è¿›ï¼š\n\n\n\næ–¹æ³•\nå¹´ä»½\nç‰¹ç‚¹\n\n\n\nOne-hot\n-\nç¨€ç–ï¼Œæ— è¯­ä¹‰\n\n\nWord2Vec\n2013\nåˆ†å¸ƒå¼è¡¨ç¤º\n\n\nGloVe\n2014\nå…¨å±€ç»Ÿè®¡\n\n\nFastText\n2016\nå­è¯ä¿¡æ¯\n\n\nELMo\n2018\nä¸Šä¸‹æ–‡ç›¸å…³\n\n\nBERT\n2018\nåŒå‘ä¸Šä¸‹æ–‡\n\n\né˜¶æ®µäºŒï¼šæ·±åº¦å­¦ä¹  NLPTransformer æ¶æ„import torchimport torch.nn as nnimport mathclass MultiHeadAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.d_k = d_model // n_heads        self.n_heads = n_heads                self.W_q = nn.Linear(d_model, d_model)        self.W_k = nn.Linear(d_model, d_model)        self.W_v = nn.Linear(d_model, d_model)        self.W_o = nn.Linear(d_model, d_model)        def forward(self, Q, K, V, mask=None):        batch_size = Q.size(0)                # Linear projections        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)                # Attention scores        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)                if mask is not None:            scores = scores.masked_fill(mask == 0, -1e9)                attn = torch.softmax(scores, dim=-1)        output = torch.matmul(attn, V)                # Concatenate and project        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)        return self.W_o(output)\n\næ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦è¡¨è¾¾\né˜¶æ®µä¸‰ï¼šå¤§è¯­è¨€æ¨¡å‹LLM æ¶æ„æ¼”è¿›GPT-1 (2018) â†’ GPT-2 â†’ GPT-3 â†’ ChatGPT â†’ GPT-4     â†“BERT â†’ RoBERTa â†’ DeBERTa     â†“T5 â†’ Flan-T5 â†’ UL2     â†“LLaMA â†’ LLaMA 2 â†’ Mistral â†’ Mixtral\n\nPrompt Engineering# 1. Zero-shotprompt = \"Translate to French: Hello, how are you?\"# 2. Few-shotprompt = \"\"\"Translate to French:Hello -&gt; BonjourGoodbye -&gt; Au revoirHow are you? -&gt;\"\"\"# 3. Chain-of-Thoughtprompt = \"\"\"Q: If I have 3 apples and buy 5 more, how many do I have?A: Let's think step by step.1. I start with 3 apples.2. I buy 5 more apples.3. Total = 3 + 5 = 8 apples.The answer is 8.Q: If I have 7 oranges and eat 2, how many remain?A: Let's think step by step.\"\"\"\n\nFine-tuning æŠ€æœ¯\n\n\næ–¹æ³•\nå¯è®­ç»ƒå‚æ•°\né€‚ç”¨åœºæ™¯\n\n\n\nFull Fine-tuning\n100%\nå¤§é‡æ•°æ®ï¼Œå……è¶³ç®—åŠ›\n\n\nLoRA\n0.1-1%\nèµ„æºå—é™\n\n\nQLoRA\n0.1%\næ¶ˆè´¹çº§ GPU\n\n\nPrefix Tuning\n0.1%\nå¤šä»»åŠ¡\n\n\nPrompt Tuning\n&lt;0.01%\næç«¯èµ„æºå—é™\n\n\nfrom peft import LoraConfig, get_peft_modellora_config = LoraConfig(    r=8,    lora_alpha=32,    target_modules=[\"q_proj\", \"v_proj\"],    lora_dropout=0.1,    bias=\"none\",)model = get_peft_model(base_model, lora_config)print(f\"Trainable params: {model.print_trainable_parameters()}\")\n\né˜¶æ®µå››ï¼šé«˜çº§ä¸»é¢˜æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)from langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.vectorstores import Chromafrom langchain.chains import RetrievalQA# æ„å»ºå‘é‡åº“embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh\")vectorstore = Chroma.from_documents(documents, embeddings)# åˆ›å»º RAG é“¾qa = RetrievalQA.from_chain_type(    llm=llm,    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}))\n\næ¨¡å‹è¯„ä¼°# å›°æƒ‘åº¦ (Perplexity)def perplexity(model, tokenizer, text):    encodings = tokenizer(text, return_tensors='pt')    max_length = model.config.n_positions        nlls = []    for i in range(0, encodings.input_ids.size(1), max_length):        begin_loc = max(i - max_length, 0)        end_loc = i + max_length        input_ids = encodings.input_ids[:, begin_loc:end_loc]        target_ids = input_ids.clone()        target_ids[:, :-1] = -100                with torch.no_grad():            outputs = model(input_ids, labels=target_ids)            nlls.append(outputs.loss)        return torch.exp(torch.stack(nlls).mean())\n\nå®è·µé¡¹ç›®å»ºè®®\nå…¥é—¨ï¼šæƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»\nè¿›é˜¶ï¼šå‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘\né«˜çº§ï¼šé—®ç­”ç³»ç»Ÿã€RAG åº”ç”¨\nä¸“å®¶ï¼šLLM é¢„è®­ç»ƒã€RLHF\n\nå»¶ä¼¸é˜…è¯»\nAttention Is All You Need\nBERT Paper\nLLaMA Paper\nLoRA Paper\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["LLM","machine learning","NLP"]},{"title":"æœºå™¨é˜…è¯»ç†è§£å®æˆ˜ï¼šä»é›¶æ„å»ºé—®ç­”ç³»ç»Ÿ","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%AE%9E%E8%B7%B5/","content":"æœ¬æ–‡ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªæœºå™¨é˜…è¯»ç†è§£ç³»ç»Ÿï¼Œæ¶µç›–æ•°æ®å¤„ç†ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒå’Œæ¨ç†çš„å®Œæ•´æµç¨‹ã€‚\nä»»åŠ¡å®šä¹‰ç»™å®šä¸Šä¸‹æ–‡  å’Œé—®é¢˜ ï¼Œé¢„æµ‹ç­”æ¡ˆ  åœ¨  ä¸­çš„ä½ç½®ï¼š\n\næ•°æ®å¤„ç†SQuAD æ•°æ®æ ¼å¼import jsonfrom dataclasses import dataclassfrom typing import List, Optional@dataclassclass Example:    context: str    question: str    answer_text: str    start_position: int    end_position: intdef load_squad(file_path: str) -&gt; List[Example]:    with open(file_path, 'r', encoding='utf-8') as f:        data = json.load(f)        examples = []    for article in data['data']:        for paragraph in article['paragraphs']:            context = paragraph['context']            for qa in paragraph['qas']:                question = qa['question']                if qa.get('is_impossible', False):                    continue                answer = qa['answers'][0]                examples.append(Example(                    context=context,                    question=question,                    answer_text=answer['text'],                    start_position=answer['answer_start'],                    end_position=answer['answer_start'] + len(answer['text'])                ))        return examples\n\nTokenizationfrom transformers import AutoTokenizerclass MRCTokenizer:    def __init__(self, model_name: str, max_length: int = 384, doc_stride: int = 128):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        self.doc_stride = doc_stride        def encode(self, example: Example):        # Tokenize question and context        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            stride=self.doc_stride,            return_overflowing_tokens=True,            return_offsets_mapping=True,            padding='max_length',        )                # æ‰¾åˆ°ç­”æ¡ˆåœ¨ token åºåˆ—ä¸­çš„ä½ç½®        offset_mapping = encoding['offset_mapping'][0]                start_token = None        end_token = None                for idx, (start, end) in enumerate(offset_mapping):            if start &lt;= example.start_position &lt; end:                start_token = idx            if start &lt; example.end_position &lt;= end:                end_token = idx                break                return {            'input_ids': encoding['input_ids'][0],            'attention_mask': encoding['attention_mask'][0],            'start_position': start_token or 0,            'end_position': end_token or 0,        }\n\næ¨¡å‹å®ç°åŸºäº BERT çš„ MRC æ¨¡å‹import torchimport torch.nn as nnfrom transformers import AutoModelclass MRCModel(nn.Module):    def __init__(self, model_name: str, dropout: float = 0.1):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size                self.dropout = nn.Dropout(dropout)        self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        start_positions: Optional[torch.Tensor] = None,        end_positions: Optional[torch.Tensor] = None,    ):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)                # (batch, seq_len, 1) -&gt; (batch, seq_len)        start_logits = self.start_classifier(sequence_output).squeeze(-1)        end_logits = self.end_classifier(sequence_output).squeeze(-1)                # Mask padding tokens        start_logits = start_logits.masked_fill(~attention_mask.bool(), -1e9)        end_logits = end_logits.masked_fill(~attention_mask.bool(), -1e9)                loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss()            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }\n\næ”¹è¿›ï¼šè”åˆ Start-End é¢„æµ‹class JointMRCModel(nn.Module):    \"\"\"è”åˆé¢„æµ‹ start å’Œ endï¼Œè€ƒè™‘ start-end ä¾èµ–\"\"\"        def __init__(self, model_name: str, max_answer_length: int = 30):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size        self.max_answer_length = max_answer_length                self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size * 2, 1)        def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        H = outputs.last_hidden_state  # (batch, seq_len, hidden)                # Start prediction        start_logits = self.start_classifier(H).squeeze(-1)                if self.training and start_positions is not None:            # è®­ç»ƒæ—¶ä½¿ç”¨çœŸå®çš„ start ä½ç½®            start_indices = start_positions.unsqueeze(-1).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)        else:            # æ¨ç†æ—¶ä½¿ç”¨é¢„æµ‹çš„ start ä½ç½®            start_indices = start_logits.argmax(dim=-1, keepdim=True).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)                # End prediction conditioned on start        start_repr_expanded = start_repr.unsqueeze(1).expand(-1, H.size(1), -1)        end_input = torch.cat([H, start_repr_expanded], dim=-1)        end_logits = self.end_classifier(end_input).squeeze(-1)                # åªå…è®¸ end &gt;= start ä¸”åœ¨ max_answer_length èŒƒå›´å†…        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®Œæ•´å®ç°éœ€è¦æ›´å¤æ‚çš„ mask                return {'start_logits': start_logits, 'end_logits': end_logits}\n\nè®­ç»ƒæµç¨‹from torch.utils.data import DataLoader, Datasetfrom transformers import get_linear_schedule_with_warmupfrom tqdm import tqdmdef train(model, train_dataloader, val_dataloader, epochs=3, lr=3e-5):    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)        total_steps = len(train_dataloader) * epochs    scheduler = get_linear_schedule_with_warmup(        optimizer,         num_warmup_steps=int(0.1 * total_steps),        num_training_steps=total_steps    )        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model.to(device)        best_f1 = 0    for epoch in range(epochs):        model.train()        total_loss = 0                for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}'):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(**batch)            loss = outputs['loss']                        loss.backward()            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                        optimizer.step()            scheduler.step()            optimizer.zero_grad()                        total_loss += loss.item()                avg_loss = total_loss / len(train_dataloader)        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')                # Validation        f1 = evaluate(model, val_dataloader, device)        print(f'Validation F1: {f1:.4f}')                if f1 &gt; best_f1:            best_f1 = f1            torch.save(model.state_dict(), 'best_model.pt')        return model\n\nè¯„ä¼°ä¸æ¨ç†import reimport stringfrom collections import Counterdef normalize_answer(s):    \"\"\"æ ‡å‡†åŒ–ç­”æ¡ˆç”¨äºè¯„ä¼°\"\"\"    def remove_articles(text):        return re.sub(r'\\b(a|an|the)\\b', ' ', text)        def white_space_fix(text):        return ' '.join(text.split())        def remove_punc(text):        exclude = set(string.punctuation)        return ''.join(ch for ch in text if ch not in exclude)        def lower(text):        return text.lower()        return white_space_fix(remove_articles(remove_punc(lower(s))))def compute_f1(pred: str, gold: str) -&gt; float:    pred_tokens = normalize_answer(pred).split()    gold_tokens = normalize_answer(gold).split()        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        if num_same == 0:        return 0        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        return 2 * precision * recall / (precision + recall)def predict(model, tokenizer, context: str, question: str, device):    \"\"\"å•æ¡æ¨ç†\"\"\"    model.eval()        encoding = tokenizer(        question, context,        max_length=384,        truncation='only_second',        return_tensors='pt'    )        encoding = {k: v.to(device) for k, v in encoding.items()}        with torch.no_grad():        outputs = model(**encoding)        start_idx = outputs['start_logits'].argmax().item()    end_idx = outputs['end_logits'].argmax().item()        # ç¡®ä¿ end &gt;= start    if end_idx &lt; start_idx:        end_idx = start_idx        # è§£ç ç­”æ¡ˆ    answer_tokens = encoding['input_ids'][0][start_idx:end_idx+1]    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)        return answer\n\nç°ä»£æ–¹æ³•ï¼šä½¿ç”¨ LLMå¯¹äºæ›´å¤æ‚çš„é—®ç­”éœ€æ±‚ï¼Œå¯ä»¥ä½¿ç”¨ LLMï¼š\nfrom openai import OpenAIdef llm_qa(context: str, question: str) -&gt; str:    client = OpenAI()        response = client.chat.completions.create(        model=\"gpt-4\",        messages=[            {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·è¯´'æ— æ³•å›ç­”'ã€‚\"},            {\"role\": \"user\", \"content\": f\"ä¸Šä¸‹æ–‡ï¼š{context}\\n\\né—®é¢˜ï¼š{question}\"}        ],        temperature=0    )        return response.choices[0].message.content\n\nå»¶ä¼¸é˜…è¯»\nSQuAD Dataset\nHugging Face QA Pipeline\nNatural Questions\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"å¼€ç¯‡","url":"/2019/10/03/%E5%BC%80%E7%AF%87/","content":"å„ä½è¯»è€…æœ‹å‹ä»¬å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ fooSynapticã€‚\næ¬¢è¿æ¥åˆ°æˆ‘çš„æŠ€æœ¯åšå®¢ï¼è¿™é‡Œè®°å½•æˆ‘åœ¨ AI å’Œ NLP é¢†åŸŸçš„å­¦ä¹ ä¸æ€è€ƒã€‚\nå…³äºè¿™ä¸ªåšå®¢è¿™ä¸ªåšå®¢ä¸»è¦è®°å½•ä»¥ä¸‹å†…å®¹ï¼š\n\nè‡ªç„¶è¯­è¨€å¤„ç† (NLP)ï¼šä»ä¼ ç»Ÿæ–¹æ³•åˆ°å¤§è¯­è¨€æ¨¡å‹\næœºå™¨å­¦ä¹ ï¼šç®—æ³•åŸç†ä¸å®ç°ç»†èŠ‚\næ·±åº¦å­¦ä¹ ï¼šæ¨¡å‹æ¶æ„ä¸è®­ç»ƒæŠ€å·§\næ•°å­¦åŸºç¡€ï¼šçº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºã€ä¼˜åŒ–ç†è®º\nå·¥ç¨‹å®è·µï¼šPythonã€PyTorchã€åˆ†å¸ƒå¼è®­ç»ƒ\n\næŠ€æœ¯æ ˆNLP: Transformers, LLMs, RAG, Prompt EngineeringML: PyTorch, JAX, scikit-learnInfra: CUDA, Triton, vLLM, DeepSpeed\n\nå…³äºæˆ‘NLP Researcherï¼Œä¸“æ³¨äºï¼š\n\nå¤§è¯­è¨€æ¨¡å‹ (LLM) è®­ç»ƒä¸æ¨ç†ä¼˜åŒ–\næ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)\næœºå™¨é˜…è¯»ç†è§£ä¸é—®ç­”ç³»ç»Ÿ\n\nGitHub: fooSynaptic\n\n\næ¬¢è¿äº¤æµè®¨è®ºï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["Introduction"]},{"title":"æœºå™¨é˜…è¯»ç†è§£ï¼šä»ä¼ ç»Ÿæ–¹æ³•åˆ°å¤§è¯­è¨€æ¨¡å‹","url":"/2019/10/03/%E5%BD%93%E6%88%91%E4%BB%AC%E6%8A%8A%E7%9B%AE%E5%85%89%E6%94%BE%E5%9C%A8%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","content":"\næ ¸å¿ƒé—®é¢˜ï¼šå½“æˆ‘ä»¬æœŸæœ›æœºå™¨â€ç†è§£â€æ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬çš„æœŸæœ›åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ\n\næœºå™¨é˜…è¯»ç†è§£çš„æ¼”è¿›ä¼ ç»Ÿ MRC (2015-2019)åŸºäº span extraction çš„æ–¹æ³•ï¼š\nè¾“å…¥: Context + Questionè¾“å‡º: (start_idx, end_idx)\n\nä»£è¡¨æ¨¡å‹ï¼šBiDAF, R-Net, QANet, BERT\nLLM æ—¶ä»£çš„ MRC (2020-è‡³ä»Š)ä»â€æŠ½å–â€åˆ°â€ç”Ÿæˆâ€çš„èŒƒå¼è½¬å˜ï¼š\nè¾“å…¥: Context + Question + Instructionè¾“å‡º: è‡ªç”±å½¢å¼çš„ç­”æ¡ˆ\n\nä»»åŠ¡åˆ†ç±»ä¸éš¾åº¦\n\n\nç±»å‹\nä¼ ç»Ÿæ–¹æ³•\nLLM æ–¹æ³•\néš¾åº¦\n\n\n\næŠ½å–å¼\nâœ… æ“…é•¿\nâœ… æ“…é•¿\nâ­\n\n\nå¤šè·³æ¨ç†\nâŒ å›°éš¾\nâš ï¸ æœ‰é™\nâ­â­â­\n\n\næ•°å€¼æ¨ç†\nâŒ å‡ ä¹ä¸èƒ½\nâš ï¸ éœ€è¦ CoT\nâ­â­â­â­\n\n\nå¸¸è¯†æ¨ç†\nâŒ ä¸èƒ½\nâœ… è¾ƒå¥½\nâ­â­â­\n\n\nå¼€æ”¾ç”Ÿæˆ\nâŒ ä¸èƒ½\nâœ… æ“…é•¿\nâ­â­\n\n\nç°ä»£æ–¹æ³•ï¼šRAGæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation) ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆçš„ä¼˜åŠ¿ï¼š\nclass RAGSystem:    def __init__(self, retriever, generator):        self.retriever = retriever  # e.g., Dense Retriever        self.generator = generator  # e.g., LLM        def answer(self, question: str) -&gt; str:        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£        docs = self.retriever.retrieve(question, top_k=5)                # 2. æ„å»ºä¸Šä¸‹æ–‡        context = \"\\n\\n\".join([d.text for d in docs])                # 3. ç”Ÿæˆç­”æ¡ˆ        prompt = f\"\"\"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š{context}é—®é¢˜ï¼š{question}ç­”æ¡ˆï¼š\"\"\"                return self.generator.generate(prompt)\n\næ£€ç´¢å™¨é€‰æ‹©\n\n\næ£€ç´¢å™¨\nç‰¹ç‚¹\né€‚ç”¨åœºæ™¯\n\n\n\nBM25\nå…³é”®è¯åŒ¹é…ï¼Œå¿«é€Ÿ\nçŸ­æŸ¥è¯¢ï¼Œç²¾ç¡®åŒ¹é…\n\n\nDense Retriever\nè¯­ä¹‰åŒ¹é…\nè¯­ä¹‰ç›¸ä¼¼æŸ¥è¯¢\n\n\nColBERT\nå»¶è¿Ÿäº¤äº’\nå¹³è¡¡æ•ˆç‡ä¸æ•ˆæœ\n\n\nHybrid\nç»“åˆç¨€ç–+ç¨ å¯†\nç”Ÿäº§ç¯å¢ƒ\n\n\nChain-of-Thought æ¨ç†å¯¹äºéœ€è¦æ¨ç†çš„é—®é¢˜ï¼ŒCoT prompting æ˜¾è‘—æå‡æ•ˆæœï¼š\n# æ ‡å‡† Promptingprompt_standard = \"Q: å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œç»™äº†å°çº¢2ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ\\nA:\"# Chain-of-Thought Prompting  prompt_cot = \"\"\"Q: å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œç»™äº†å°çº¢2ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼ŸA: è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š1. å°æ˜æœ€åˆæœ‰ 5 ä¸ªè‹¹æœ2. ä»–ç»™äº†å°çº¢ 2 ä¸ªè‹¹æœ3. å‰©ä½™è‹¹æœæ•° = 5 - 2 = 3ç­”æ¡ˆæ˜¯ 3 ä¸ªè‹¹æœã€‚\"\"\"\n\nè¯„ä¼°æŒ‡æ ‡ä¼ ç»ŸæŒ‡æ ‡\nğŸ™\nLLM æ—¶ä»£çš„æŒ‡æ ‡# ä½¿ç”¨ LLM ä½œä¸ºè¯„ä¼°å™¨def llm_evaluate(question, gold_answer, pred_answer):    prompt = f\"\"\"è¯„ä¼°é¢„æµ‹ç­”æ¡ˆçš„è´¨é‡ï¼ˆ1-5åˆ†ï¼‰ï¼šé—®é¢˜ï¼š{question}æ ‡å‡†ç­”æ¡ˆï¼š{gold_answer}é¢„æµ‹ç­”æ¡ˆï¼š{pred_answer}è¯„åˆ†æ ‡å‡†ï¼š5åˆ† - å®Œå…¨æ­£ç¡®ä¸”ä¿¡æ¯å®Œæ•´4åˆ† - åŸºæœ¬æ­£ç¡®ï¼Œç•¥æœ‰é—æ¼3åˆ† - éƒ¨åˆ†æ­£ç¡®2åˆ† - æœ‰ç›¸å…³ä¿¡æ¯ä½†ä¸æ­£ç¡®1åˆ† - å®Œå…¨é”™è¯¯åˆ†æ•°ï¼š\"\"\"    return llm.generate(prompt)\n\nå®è·µå»ºè®®ä½•æ—¶ç”¨ä¼ ç»Ÿ MRC\nç­”æ¡ˆæ˜ç¡®åœ¨æ–‡æ¡£ä¸­\néœ€è¦ç²¾ç¡®çš„ä½ç½®æ ‡æ³¨\nä½å»¶è¿Ÿè¦æ±‚\nèµ„æºå—é™\n\nä½•æ—¶ç”¨ RAG + LLM\néœ€è¦æ•´åˆå¤šä¸ªæ–‡æ¡£\nç­”æ¡ˆéœ€è¦æ¨ç†æˆ–æ€»ç»“\nå¼€æ”¾åŸŸé—®ç­”\nç”¨æˆ·æœŸæœ›è‡ªç„¶è¯­è¨€å›ç­”\n\nä»£ç ç¤ºä¾‹ï¼šç°ä»£ RAG ç³»ç»Ÿfrom langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.chat_models import ChatOpenAIfrom langchain.chains import RetrievalQA# åˆå§‹åŒ–ç»„ä»¶embeddings = OpenAIEmbeddings()vectorstore = FAISS.load_local(\"my_index\", embeddings)llm = ChatOpenAI(model=\"gpt-4\", temperature=0)# åˆ›å»º RAG é“¾qa_chain = RetrievalQA.from_chain_type(    llm=llm,    chain_type=\"stuff\",  # æˆ– \"map_reduce\", \"refine\"    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),    return_source_documents=True)# ä½¿ç”¨result = qa_chain({\"query\": \"ä»€ä¹ˆæ˜¯æœºå™¨é˜…è¯»ç†è§£ï¼Ÿ\"})print(result[\"result\"])\n\nå»¶ä¼¸é˜…è¯»\nSQuAD 2.0\nNatural Questions\nRAG Paper\nLangChain Documentation\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","Deep learning","LLM","RAG"]},{"title":"DDIA Part 3ï¼šè¡ç”Ÿæ•°æ®","url":"/2025/12/28/DDIA-Part3-%E8%A1%8D%E7%94%9F%E6%95%B0%E6%8D%AE/","content":"æœ¬æ–‡æ˜¯ DDIA ç¬¬ä¸‰éƒ¨åˆ†çš„è¯»ä¹¦ç¬”è®°ï¼Œæ¶µç›–ç¬¬ 10-12 ç« ï¼šæ‰¹å¤„ç†ã€æµå¤„ç†ã€æ•°æ®ç³»ç»Ÿçš„æœªæ¥ã€‚\n\nç¬¬10ç« ï¼šæ‰¹å¤„ç†ä¸‰ç§ç³»ç»Ÿç±»å‹\n\n\nç±»å‹\nç‰¹ç‚¹\nå»¶è¿Ÿ\nç¤ºä¾‹\n\n\n\nåœ¨çº¿æœåŠ¡\nè¯·æ±‚-å“åº”\næ¯«ç§’\nWeb API\n\n\næ‰¹å¤„ç†\nå¤§é‡æ•°æ®ï¼Œé«˜åå\nåˆ†é’Ÿ~å°æ—¶\nMapReduce\n\n\næµå¤„ç†\næŒç»­å¤„ç†æ•°æ®æµ\næ¯«ç§’~ç§’\nKafka Streams\n\n\nUnix å“²å­¦\næ¯ä¸ªç¨‹åºåšå¥½ä¸€ä»¶äº‹ï¼Œè¾“å‡ºå¯æˆä¸ºå¦ä¸€ç¨‹åºçš„è¾“å…¥\n\ncat access.log |   awk '{print $7}' |    # æå– URL  sort | uniq -c |      # è®¡æ•°  sort -rn | head -10   # å–å‰10\n\nMapReduceè¾“å…¥ â†’ Map â†’ Shuffle(æŒ‰é”®åˆ†ç»„) â†’ Reduce â†’ è¾“å‡ºç¤ºä¾‹ï¼ˆè¯é¢‘ç»Ÿè®¡ï¼‰:è¾“å…¥: \"hello world hello\"Map:  (hello,1) (world,1) (hello,1)Shuffle: hello:[1,1], world:[1]Reduce: (hello,2) (world,1)\n\nåˆ†å¸ƒå¼æ‰§è¡Œï¼šæ•°æ®æœ¬åœ°æ€§åŸåˆ™ï¼Œå°½é‡åœ¨æ•°æ®æ‰€åœ¨èŠ‚ç‚¹æ‰§è¡Œ Map\nJoin ç±»å‹\n\n\nç±»å‹\nè¯´æ˜\né€‚ç”¨åœºæ™¯\n\n\n\nReduce ç«¯ Join\nShuffle åæŒ‰é”®åˆå¹¶\né€šç”¨\n\n\nå¹¿æ’­ Join\nå°è¡¨å¹¿æ’­åˆ°æ‰€æœ‰ Map\nå¤§è¡¨ Join å°è¡¨\n\n\nåˆ†åŒº Join\nä¸¤è¡¨ç›¸åŒåˆ†åŒºç­–ç•¥\né¢„åˆ†åŒºæ•°æ®\n\n\nç°ä»£æ‰¹å¤„ç†æ¡†æ¶\n\n\næ¡†æ¶\nç‰¹ç‚¹\n\n\n\nSpark\nRDD æŠ½è±¡ï¼Œå†…å­˜ç¼“å­˜ï¼Œè¿­ä»£å‹å¥½\n\n\nFlink\næµæ‰¹ä¸€ä½“ï¼Œå¢é‡å¤„ç†\n\n\nvs MapReduceï¼š\n\n\n\næ–¹é¢\nMapReduce\næ•°æ®æµå¼•æ“\n\n\n\nç¼–ç¨‹æ¨¡å‹\nMap/Reduce\nä»»æ„ DAG\n\n\nä¸­é—´æ•°æ®\nå†™å…¥ HDFS\nå†…å­˜æµè½¬\n\n\nè¿­ä»£æ”¯æŒ\næ•ˆç‡ä½\né«˜æ•ˆ\n\n\n\nç¬¬11ç« ï¼šæµå¤„ç†æ‰¹å¤„ç† vs æµå¤„ç†\n\n\næ–¹é¢\næ‰¹å¤„ç†\næµå¤„ç†\n\n\n\næ•°æ®\næœ‰ç•Œï¼Œé™æ€\næ— ç•Œï¼ŒæŒç»­åˆ°è¾¾\n\n\nå»¶è¿Ÿ\nåˆ†é’Ÿ~å°æ—¶\næ¯«ç§’~ç§’\n\n\nç»“æœ\nä¸€æ¬¡æ€§è¾“å‡º\næŒç»­æ›´æ–°\n\n\næ¶ˆæ¯ç³»ç»Ÿä¼ ç»Ÿæ¶ˆæ¯é˜Ÿåˆ— (RabbitMQ)ï¼š\n\næ¶ˆæ¯å¤„ç†ååˆ é™¤\næ¯æ¡æ¶ˆæ¯åªè¢«ä¸€ä¸ªæ¶ˆè´¹è€…å¤„ç†\n\næ—¥å¿—å‹æ¶ˆæ¯ç³»ç»Ÿ (Kafka)ï¼š\n\næ¶ˆæ¯æŒä¹…åŒ–ï¼Œå¯é‡æ”¾\nåˆ†åŒºä¿åº\nå¤šæ¶ˆè´¹è€…ç»„ç‹¬ç«‹æ¶ˆè´¹\n\nåˆ†åŒº0: [msg0][msg3][msg6] â†’ æ¶ˆè´¹è€…Aåˆ†åŒº1: [msg1][msg4][msg7] â†’ æ¶ˆè´¹è€…Båˆ†åŒº2: [msg2][msg5][msg8] â†’ æ¶ˆè´¹è€…C\n\nå˜æ›´æ•°æ®æ•è· (CDC)åº”ç”¨ â†’ æ•°æ®åº“ â†’ CDCå·¥å…·(Debezium) â†’ Kafka â†’ æ´¾ç”Ÿç³»ç»Ÿ\n\nåº”ç”¨ï¼šåŒæ­¥æœç´¢ç´¢å¼•ã€å¾®æœåŠ¡é›†æˆã€å®æ—¶ ETL\näº‹ä»¶æº¯æºä¼ ç»Ÿæ–¹å¼ï¼šåªå­˜å½“å‰çŠ¶æ€â”Œ balance = 1000 â”äº‹ä»¶æº¯æºï¼šå­˜å‚¨æ‰€æœ‰äº‹ä»¶â”‚ 1. åˆ›å»ºè´¦æˆ· balance=0    â”‚â”‚ 2. å­˜æ¬¾ +500             â”‚â”‚ 3. å­˜æ¬¾ +800             â”‚â”‚ 4. å–æ¬¾ -300             â”‚â”” å½“å‰: 0+500+800-300=1000 â”˜\n\nä¼˜åŠ¿ï¼šå®Œæ•´å®¡è®¡ã€å¯é‡æ”¾ã€è°ƒè¯•å‹å¥½\næ—¶é—´è¯­ä¹‰\n\n\nç±»å‹\nå®šä¹‰\n\n\n\näº‹ä»¶æ—¶é—´\näº‹ä»¶å®é™…å‘ç”Ÿçš„æ—¶é—´\n\n\nå¤„ç†æ—¶é—´\näº‹ä»¶åˆ°è¾¾å¤„ç†å™¨çš„æ—¶é—´\n\n\nçª—å£ç±»å‹æ»šåŠ¨çª—å£ï¼šâ”Œâ”€â”€â”â”Œâ”€â”€â”â”Œâ”€â”€â” (å›ºå®šå¤§å°ï¼Œä¸é‡å )æ»‘åŠ¨çª—å£ï¼šâ”Œâ”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”  (å›ºå®šå¤§å°ï¼Œå¯é‡å )ä¼šè¯çª—å£ï¼šâ”Œâ”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â” (åŸºäºæ´»åŠ¨é—´éš™)\n\næ°´ä½çº¿ (Watermark)\nä¸ä¼šå†æœ‰ â‰¤ è¯¥æ—¶é—´çš„äº‹ä»¶åˆ°è¾¾\n\nè¿Ÿåˆ°äº‹ä»¶å¤„ç†ï¼šä¸¢å¼ƒã€æ›´æ–°ç»“æœã€ä¾§è¾“å‡º\næµå¤„ç†æ¡†æ¶\n\n\næ¡†æ¶\nç‰¹ç‚¹\n\n\n\nKafka Streams\nè½»é‡çº§åº“ï¼Œä¸ Kafka ç´§å¯†é›†æˆ\n\n\nFlink\nçœŸæ­£çš„æµå¤„ç†ï¼Œå¼ºä¸€è‡´æ€§\n\n\nSpark Streaming\nå¾®æ‰¹å¤„ç†\n\n\næµè¡¨å¯¹å¶æµ â†’ è¡¨ï¼šèšåˆ/ç´¯ç§¯ï¼ˆç‰©åŒ–è§†å›¾ï¼‰è¡¨ â†’ æµï¼šæ•è·å˜æ›´ï¼ˆå˜æ›´æ—¥å¿—ï¼‰\n\n-- KSQL: æµè½¬è¡¨CREATE TABLE page_counts AS  SELECT page_id, COUNT(*) as views  FROM pageviews  GROUP BY page_id;\n\n\nç¬¬12ç« ï¼šæ•°æ®ç³»ç»Ÿçš„æœªæ¥æ•°æ®é›†æˆç°çŠ¶ï¼šå¤šç§ä¸“ç”¨å·¥å…·ï¼ˆPostgreSQL + Redis + ES + Kafkaï¼‰\nä»¥æ—¥å¿—ä¸ºä¸­å¿ƒçš„æ¶æ„ï¼š\nå†™å…¥ â†’ äº‹ä»¶æ—¥å¿—(Kafka) â†’ æ•°æ®åº“              â†“        â†’ æœç´¢ç´¢å¼•                       â†’ ç¼“å­˜\n\nLambda vs Kappaï¼š\n\n\n\næ¶æ„\nç‰¹ç‚¹\n\n\n\nLambda\næ‰¹å¤„ç†+æµå¤„ç†åŒè·¯å¾„ï¼Œéœ€ç»´æŠ¤ä¸¤å¥—ä»£ç \n\n\nKappa\nåªç”¨æµå¤„ç†ï¼Œæ—¥å¿—ä¿ç•™è¶³å¤Ÿé•¿æ”¯æŒé‡æ”¾\n\n\nåˆ†æ‹†æ•°æ®åº“ä¼ ç»Ÿæ•°æ®åº“ = å­˜å‚¨ + äº‹åŠ¡ + ç´¢å¼• + å¤åˆ¶ + æŸ¥è¯¢ä¼˜åŒ–åˆ†æ‹†åï¼šKafka(æ—¥å¿—) â†’ Flink(å¤„ç†) â†’ RocksDB(å­˜å‚¨)æ¯ä¸ªç»„ä»¶ä¸“æ³¨ä¸€ä»¶äº‹\n\nä¸»æ•°æ® vs è¡ç”Ÿæ•°æ®ï¼š\n\n\n\nç±»å‹\nè¯´æ˜\n\n\n\nä¸»æ•°æ®\nçœŸç›¸æ¥æºï¼Œå†™å…¥æ—¶çš„è¾“å…¥\n\n\nè¡ç”Ÿæ•°æ®\nä»ä¸»æ•°æ®è®¡ç®—ï¼Œå¯é‡å»ºï¼ˆç´¢å¼•ã€ç¼“å­˜ï¼‰\n\n\nç«¯åˆ°ç«¯æ­£ç¡®æ€§å¹‚ç­‰æ€§ï¼šæ‰§è¡Œå¤šæ¬¡ä¸æ‰§è¡Œä¸€æ¬¡æ•ˆæœç›¸åŒ\nå¹‚ç­‰ï¼šSET x = 5éå¹‚ç­‰ï¼šINCR xï¼ˆéœ€è¦å»é‡ï¼‰\n\nç«¯åˆ°ç«¯è®ºè¯ï¼šåªåœ¨æ¯å±‚ä¿è¯æ­£ç¡®ä¸å¤Ÿï¼Œéœ€åœ¨æœ€ç»ˆç”¨æˆ·å±‚é¢éªŒè¯\nå®¡è®¡ä¸å¯è¿½æº¯ä¸å¯å˜æ—¥å¿—çš„ä¼˜åŠ¿ï¼š\nä¼ ç»Ÿï¼šUPDATE email='new@...'ï¼ˆå†å²ä¸¢å¤±ï¼‰äº‹ä»¶æ—¥å¿—ï¼š  1. æ³¨å†Œ email='old@...'  2. æ›´æ–° email='new@...'ï¼ˆå®Œæ•´å†å²ï¼‰\n\nä¼¦ç†è€ƒé‡\n\n\né—®é¢˜\nè€ƒé‡\n\n\n\néšç§\næ”¶é›†ä»€ä¹ˆæ•°æ®ï¼Ÿç”¨äºä»€ä¹ˆï¼Ÿ\n\n\nåè§\nç®—æ³•æ˜¯å¦å­˜åœ¨æ­§è§†ï¼Ÿ\n\n\né€æ˜åº¦\nç”¨æˆ·èƒ½å¦ç†è§£å†³ç­–è¿‡ç¨‹ï¼Ÿ\n\n\né—®è´£\nå‡ºé”™æ—¶è°è´Ÿè´£ï¼Ÿ\n\n\nè®¾è®¡åŸåˆ™\n\n\nåŸåˆ™\nè¯´æ˜\n\n\n\nç®€å•æ€§\né¿å…ä¸å¿…è¦çš„å¤æ‚æ€§\n\n\nå¯ç»„åˆæ€§\nä½¿ç”¨å¯ç»„åˆçš„ç»„ä»¶\n\n\nå¯è§‚æµ‹æ€§\nä¾¿äºç†è§£ç³»ç»Ÿè¡Œä¸º\n\n\nå¯æ¼”åŒ–æ€§\nèƒ½é€‚åº”å˜åŒ–çš„éœ€æ±‚\n\n\næœªæ¥è¶‹åŠ¿1. æµæ‰¹ä¸€ä½“ï¼šæ‰¹å¤„ç† = æœ‰ç•Œæµ2. å£°æ˜å¼æ•°æ®ç®¡ç†ï¼šæè¿°æƒ³è¦ä»€ä¹ˆ3. è‡ªåŠ¨åŒ–è¿ç»´ï¼šè‡ªåŠ¨è°ƒä¼˜ã€æ‰©ç¼©å®¹4. è¾¹ç¼˜è®¡ç®—ï¼šæ•°æ®å¤„ç†é è¿‘äº§ç”Ÿåœ°\n\n\nå…¨ä¹¦æ€»ç»“æ ¸å¿ƒä¸»é¢˜\n\n\nä¸»é¢˜\nç« èŠ‚\nè¦ç‚¹\n\n\n\næ•°æ®å­˜å‚¨\n2, 3, 4\né€‰æ‹©åˆé€‚çš„æ•°æ®æ¨¡å‹å’Œå­˜å‚¨å¼•æ“\n\n\nåˆ†å¸ƒå¼\n5-9\nç†è§£åˆ†å¸ƒå¼ç³»ç»Ÿçš„æƒè¡¡\n\n\næ•°æ®å¤„ç†\n10, 11\næ‰¹å¤„ç†ä¸æµå¤„ç†çš„ç»Ÿä¸€\n\n\nç³»ç»Ÿè®¾è®¡\n1, 12\nå¯é ã€å¯æ‰©å±•ã€å¯ç»´æŠ¤\n\n\nå…³é”®è¦ç‚¹\næ²¡æœ‰é“¶å¼¹ï¼šä¸åŒå·¥å…·é€‚åˆä¸åŒåœºæ™¯\næ—¥å¿—æ˜¯å…³é”®ï¼šäº‹ä»¶æ—¥å¿—æ˜¯æ•°æ®é›†æˆçš„åŸºç¡€\nè¡ç”Ÿæ•°æ®å¯é‡å»ºï¼šä¸»æ•°æ®æ˜¯çœŸç›¸æ¥æº\nç«¯åˆ°ç«¯æ­£ç¡®æ€§ï¼šåªåœ¨æŸä¸€å±‚ä¿è¯ä¸å¤Ÿ\næŠ€æœ¯é€‰æ‹©æœ‰ç¤¾ä¼šå½±å“ï¼šéœ€è¦è€ƒè™‘ä¼¦ç†é—®é¢˜\nç®€å•æ€§æ˜¯ç¾å¾·ï¼šé¿å…ä¸å¿…è¦çš„å¤æ‚æ€§\n\n\næ¨èé˜…è¯»\n\n\nä¹¦ç±\nä¸»é¢˜\n\n\n\nã€ŠDatabase Internalsã€‹\næ•°æ®åº“å†…éƒ¨åŸç†\n\n\nã€ŠStreaming Systemsã€‹\næµå¤„ç†æ·±å…¥\n\n\nã€ŠBuilding Microservicesã€‹\nå¾®æœåŠ¡æ¶æ„\n\n\nã€ŠClean Architectureã€‹\nè½¯ä»¶æ¶æ„\n\n\n\nä¸Šä¸€éƒ¨åˆ†ï¼šåˆ†å¸ƒå¼æ•°æ® | è¿”å›æ€»è§ˆ\n","categories":["è¯»ä¹¦ç¬”è®°"],"tags":["DDIA","æ‰¹å¤„ç†","æµå¤„ç†","æ•°æ®å·¥ç¨‹"]},{"title":"çŸ©é˜µåˆ†è§£ï¼šä» SVD åˆ°ç°ä»£ AI åº”ç”¨","url":"/2019/10/03/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B9%8B%E4%B8%80%EF%BC%9ATruncate-SVD-%E5%92%8Crandom-SVD/","content":"çŸ©é˜µåˆ†è§£æ˜¯æœºå™¨å­¦ä¹ çš„åŸºçŸ³æŠ€æœ¯ï¼Œä»ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿåˆ°ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰ï¼Œéƒ½ç¦»ä¸å¼€çŸ©é˜µåˆ†è§£çš„æ€æƒ³ã€‚\nå¥‡å¼‚å€¼åˆ†è§£ (SVD)åŸºæœ¬å½¢å¼ä»»æ„çŸ©é˜µ  å¯ä»¥åˆ†è§£ä¸ºï¼š\n\nå…¶ä¸­ï¼š\n\nï¼šå·¦å¥‡å¼‚å‘é‡ï¼ˆæ­£äº¤çŸ©é˜µï¼‰\nï¼šå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µ\nï¼šå³å¥‡å¼‚å‘é‡ï¼ˆæ­£äº¤çŸ©é˜µï¼‰\n\nTruncated SVDä¿ç•™å‰  ä¸ªæœ€å¤§å¥‡å¼‚å€¼ï¼š\n\nè¿™æ˜¯æœ€ä¼˜çš„ç§©  è¿‘ä¼¼ï¼ˆEckart-Young å®šç†ï¼‰ï¼š\n\nRandomized SVDå½“çŸ©é˜µè§„æ¨¡å·¨å¤§æ—¶ï¼Œç²¾ç¡® SVD è®¡ç®—ä»£ä»·è¿‡é«˜ã€‚Randomized SVD æä¾›äº†é«˜æ•ˆçš„è¿‘ä¼¼æ–¹æ³•ã€‚\nç®—æ³•å®ç°import numpy as npfrom scipy import linalgdef randomized_svd(A, n_components, n_oversamples=10, n_iter=4):    \"\"\"    Randomized SVD for large matrices.        Args:        A: Input matrix (m x n)        n_components: Number of singular values to compute        n_oversamples: Additional random vectors for accuracy        n_iter: Number of power iterations        Returns:        U, s, Vt: Truncated SVD components    \"\"\"    m, n = A.shape    n_random = n_components + n_oversamples        # Step 1: Random projection    Q = np.random.randn(n, n_random)        # Step 2: Power iteration for accuracy    for _ in range(n_iter):        Q, _ = linalg.lu(A @ Q, permute_l=True)        Q, _ = linalg.lu(A.T @ Q, permute_l=True)        Q, _ = linalg.qr(A @ Q, mode='economic')        # Step 3: Project and compute SVD    B = Q.T @ A    Uhat, s, Vt = linalg.svd(B, full_matrices=False)    U = Q @ Uhat        return U[:, :n_components], s[:n_components], Vt[:n_components, :]\n\nå¤æ‚åº¦å¯¹æ¯”\n\n\næ–¹æ³•\næ—¶é—´å¤æ‚åº¦\nç©ºé—´å¤æ‚åº¦\n\n\n\nç²¾ç¡® SVD\n\n\n\n\nRandomized SVD\n\n\n\n\nTruncated SVD (Lanczos)\n\n\n\n\nç°ä»£åº”ç”¨ï¼šLoRALoRA (Low-Rank Adaptation) æ˜¯å¤§è¯­è¨€æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œç›´æ¥åˆ©ç”¨äº†ä½ç§©åˆ†è§£çš„æ€æƒ³ã€‚\nLoRA åŸç†é¢„è®­ç»ƒæƒé‡  å›ºå®šï¼Œåªè®­ç»ƒä½ç§©å¢é‡ï¼š\n\nå…¶ä¸­ ï¼Œï¼Œã€‚\nå®ç°ç¤ºä¾‹import torchimport torch.nn as nnclass LoRALayer(nn.Module):    def __init__(self, in_features, out_features, rank=4, alpha=1.0):        super().__init__()        self.rank = rank        self.alpha = alpha                # åŸå§‹æƒé‡ï¼ˆå†»ç»“ï¼‰        self.W = nn.Linear(in_features, out_features, bias=False)        self.W.weight.requires_grad = False                # ä½ç§©åˆ†è§£        self.A = nn.Linear(in_features, rank, bias=False)        self.B = nn.Linear(rank, out_features, bias=False)                # åˆå§‹åŒ–        nn.init.kaiming_uniform_(self.A.weight)        nn.init.zeros_(self.B.weight)                self.scaling = alpha / rank        def forward(self, x):        # W(x) + scaling * B(A(x))        return self.W(x) + self.scaling * self.B(self.A(x))\n\nå‚æ•°æ•ˆç‡å¯¹äº LLaMA-7Bï¼š\n\n\n\næ–¹æ³•\nå¯è®­ç»ƒå‚æ•°\næ˜¾å­˜å ç”¨\n\n\n\nå…¨é‡å¾®è°ƒ\n7B (100%)\n~140GB\n\n\nLoRA (r=8)\n4.7M (0.07%)\n~14GB\n\n\nLoRA (r=16)\n9.4M (0.13%)\n~16GB\n\n\nå…¶ä»–åº”ç”¨1. æ¨èç³»ç»ŸçŸ©é˜µåˆ†è§£ç”¨äºååŒè¿‡æ»¤ï¼š\n\n# ä½¿ç”¨ surprise åº“from surprise import SVD, Dataset, Readerreader = Reader(rating_scale=(1, 5))data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)model = SVD(n_factors=100)model.fit(trainset)\n\n2. æ–‡æœ¬è¡¨ç¤º (LSA)æ½œåœ¨è¯­ä¹‰åˆ†æï¼š\nfrom sklearn.decomposition import TruncatedSVDfrom sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(max_features=10000)X = vectorizer.fit_transform(documents)svd = TruncatedSVD(n_components=100)X_reduced = svd.fit_transform(X)\n\n3. å›¾åƒå‹ç¼©from PIL import Imageimport numpy as npdef compress_image(image_path, n_components=50):    img = np.array(Image.open(image_path).convert('L'))    U, s, Vt = np.linalg.svd(img, full_matrices=False)        # ä¿ç•™å‰ n_components ä¸ªå¥‡å¼‚å€¼    compressed = U[:, :n_components] @ np.diag(s[:n_components]) @ Vt[:n_components, :]        return compressed.astype(np.uint8)\n\næ•°å€¼ç¨³å®šæ€§æ¡ä»¶æ•°\næ¡ä»¶æ•°è¿‡å¤§ä¼šå¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚\næ­£åˆ™åŒ– SVDdef regularized_svd(A, lambda_reg=0.01):    \"\"\"Add regularization for numerical stability.\"\"\"    U, s, Vt = np.linalg.svd(A, full_matrices=False)    s_reg = s / (s**2 + lambda_reg)    return U, s_reg, Vt\n\nå»¶ä¼¸é˜…è¯»\nHalko et al., Finding Structure with Randomness (2011)\nHu et al., LoRA: Low-Rank Adaptation of Large Language Models (2021)\nNumPy SVD Documentation\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["LLM","machine learning","linear algebra"]},{"title":"BiDAF è®ºæ–‡è§£è¯»ï¼šåŒå‘æ³¨æ„åŠ›æµæœºåˆ¶","url":"/2019/11/19/%E8%AE%BA%E6%96%87%E6%A2%97%E6%A6%82%EF%BC%9ABi-Directional-Attention-Flow-for-Machine-Comprehension/","content":"BiDAF (Bi-Directional Attention Flow) æ˜¯æœºå™¨é˜…è¯»ç†è§£é¢†åŸŸçš„ç»å…¸æ¨¡å‹ï¼Œå…¶åŒå‘æ³¨æ„åŠ›æœºåˆ¶å¯¹åç»­ Transformer æ¶æ„äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚\næ ¸å¿ƒåˆ›æ–°1. Memory-less Attentionä¼ ç»ŸåŠ¨æ€æ³¨æ„åŠ› vs BiDAF çš„æ— è®°å¿†æ³¨æ„åŠ›ï¼š\n\n\n\nç‰¹æ€§\nDynamic Attention\nMemory-less Attention\n\n\n\nä¾èµ–\nå‰ä¸€æ—¶é—´æ­¥çš„ attended vector\nä»…å½“å‰ query å’Œ context\n\n\nä¼˜åŠ¿\nå¯å»ºæ¨¡æ—¶åºä¾èµ–\né¿å…é”™è¯¯ç´¯ç§¯\n\n\nç¼ºç‚¹\né”™è¯¯ä¼šä¼ æ’­\næ— æ³•å»ºæ¨¡é•¿ç¨‹ä¾èµ–\n\n\n2. åŒå‘æ³¨æ„åŠ›åŒæ—¶è®¡ç®—ï¼š\n\nContext-to-Query (C2Q)ï¼šæ¯ä¸ª context è¯æœ€ç›¸å…³çš„ query è¯\nQuery-to-Context (Q2C)ï¼šå¯¹å›ç­”é—®é¢˜æœ€å…³é”®çš„ context è¯\n\næ¨¡å‹æ¶æ„Input â†’ Embedding â†’ Encoding â†’ Attention â†’ Modeling â†’ Output  â”‚         â”‚           â”‚          â”‚           â”‚         â”‚ è¯å‘é‡    å­—ç¬¦CNN     BiLSTM    åŒå‘æ³¨æ„åŠ›   BiLSTM   Spané¢„æµ‹\n\næ•°å­¦è¡¨è¾¾ç›¸ä¼¼åº¦çŸ©é˜µï¼š\n\nå…¶ä¸­  æ˜¯ context è¡¨ç¤ºï¼Œ æ˜¯ query è¡¨ç¤ºã€‚\nC2Q Attentionï¼š\n$$\\tilde{U}i = \\sum_j a{ij} U_j, \\quad a_i = \\text{softmax}(S_i)$$\nQ2C Attentionï¼š\n\nèåˆè¡¨ç¤ºï¼š\n\nPyTorch å®ç°import torchimport torch.nn as nnclass BiDAFAttention(nn.Module):    def __init__(self, hidden_size):        super().__init__()        self.W = nn.Linear(hidden_size * 3, 1, bias=False)        def forward(self, context, query, c_mask, q_mask):        \"\"\"        Args:            context: (batch, c_len, hidden)            query: (batch, q_len, hidden)            c_mask: (batch, c_len)            q_mask: (batch, q_len)        \"\"\"        batch, c_len, hidden = context.size()        q_len = query.size(1)                # æ‰©å±•ç»´åº¦ä»¥è®¡ç®—æ‰€æœ‰ (i, j) å¯¹        c_expand = context.unsqueeze(2).expand(-1, -1, q_len, -1)        q_expand = query.unsqueeze(1).expand(-1, c_len, -1, -1)                # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ S        cq = torch.cat([c_expand, q_expand, c_expand * q_expand], dim=-1)        S = self.W(cq).squeeze(-1)  # (batch, c_len, q_len)                # Mask        q_mask_expand = q_mask.unsqueeze(1).expand(-1, c_len, -1)        S = S.masked_fill(~q_mask_expand, -1e9)                # C2Q attention        a = torch.softmax(S, dim=-1)        c2q = torch.bmm(a, query)  # (batch, c_len, hidden)                # Q2C attention        b = torch.softmax(S.max(dim=-1)[0], dim=-1)        q2c = torch.bmm(b.unsqueeze(1), context)  # (batch, 1, hidden)        q2c = q2c.expand(-1, c_len, -1)                # èåˆ        G = torch.cat([context, c2q, context * c2q, context * q2c], dim=-1)                return G\n\nä¸ Transformer çš„å¯¹æ¯”\n\n\nç‰¹æ€§\nBiDAF\nTransformer\n\n\n\næ³¨æ„åŠ›æ–¹å‘\nåŒå‘ï¼ˆC2Q, Q2Cï¼‰\nå…¨æ–¹å‘è‡ªæ³¨æ„åŠ›\n\n\nä½ç½®ç¼–ç \nBiLSTM éšå¼ç¼–ç \næ˜¾å¼ä½ç½®ç¼–ç \n\n\nå¹¶è¡ŒåŒ–\nå—é™äº RNN\nå®Œå…¨å¹¶è¡Œ\n\n\né•¿è·ç¦»ä¾èµ–\nå—é™\nç†è®ºä¸Šæ— é™\n\n\nå‚æ•°é‡\nè¾ƒå°‘\nè¾ƒå¤š\n\n\nç°ä»£æ¼”è¿›BiDAF çš„æ€æƒ³åœ¨ç°ä»£æ¨¡å‹ä¸­çš„ä½“ç°ï¼š\n1. Cross-Attention in Transformerclass CrossAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.mha = nn.MultiheadAttention(d_model, n_heads)        def forward(self, query, key_value):        # query æ¥è‡ªä¸€ä¸ªåºåˆ—ï¼Œkey/value æ¥è‡ªå¦ä¸€ä¸ªåºåˆ—        return self.mha(query, key_value, key_value)\n\n2. FiD (Fusion-in-Decoder)ç”¨äº RAG çš„æ¶æ„ï¼Œç±»ä¼¼ BiDAF çš„èåˆæ€æƒ³ï¼š\nclass FiD(nn.Module):    def __init__(self, encoder, decoder):        super().__init__()        self.encoder = encoder        self.decoder = decoder        def forward(self, question, passages):        # ç‹¬ç«‹ç¼–ç æ¯ä¸ª passage        encoded = []        for passage in passages:            enc = self.encoder(question + passage)            encoded.append(enc)                # èåˆè§£ç         fused = torch.cat(encoded, dim=1)        return self.decoder(fused)\n\nå®éªŒç»“æœï¼ˆåŸè®ºæ–‡ï¼‰åœ¨ SQuAD 1.1 ä¸Šçš„è¡¨ç°ï¼š\n\n\n\næ¨¡å‹\nEM\nF1\n\n\n\nBiDAF\n67.7\n77.3\n\n\nBiDAF + Self Attention\n72.1\n81.1\n\n\nBERT-base\n80.8\n88.5\n\n\nGPT-4 (few-shot)\n~90\n~95\n\n\nå»¶ä¼¸é˜…è¯»\nBiDAF Paper\nAttention Is All You Need\nBERT for QA\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","attention","deep learning"]},{"title":"MRC æ¨¡å‹å®ç°ï¼šä» TensorFlow åˆ° PyTorch","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E5%8E%BB%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%87%E6%9C%AC%E5%B9%B6%E4%B8%94%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%EF%BC%88tensorflow%E5%AE%9E%E6%88%98%EF%BC%89/","content":"æœ¬æ–‡ä»‹ç»æœºå™¨é˜…è¯»ç†è§£æ¨¡å‹çš„å®Œæ•´å®ç°ï¼Œæ¶µç›–ç»å…¸æ¶æ„å’Œç°ä»£æœ€ä½³å®è·µã€‚\né—®é¢˜å®šä¹‰è¾“å…¥ï¼š\n\né—®é¢˜ \næ–‡æ¡£ \n\nè¾“å‡ºï¼š\n\nç­”æ¡ˆèµ·å§‹ä½ç½® \nç­”æ¡ˆç»“æŸä½ç½® \n\nç»å…¸æ¶æ„Input â†’ Embedding â†’ Encoding â†’ Matching â†’ Fusion â†’ Decoding\n\nå„å±‚è¯¦è§£\n\n\nå±‚\nåŠŸèƒ½\nç°ä»£æ›¿ä»£\n\n\n\nEmbedding\nToken â†’ Vector\nSubword Tokenization\n\n\nEncoding\nåºåˆ—ç¼–ç \nTransformer Encoder\n\n\nMatching\nQ-P äº¤äº’\nCross-Attention\n\n\nFusion\nä¿¡æ¯èåˆ\nSelf-Attention\n\n\nDecoding\nSpan é¢„æµ‹\nLinear + Softmax\n\n\nPyTorch å®ç°å®Œæ•´æ¨¡å‹import torchimport torch.nn as nnimport torch.nn.functional as Ffrom transformers import AutoModel, AutoTokenizerclass MRCModel(nn.Module):    \"\"\"åŸºäº Transformer çš„ MRC æ¨¡å‹\"\"\"        def __init__(        self,         model_name: str = \"bert-base-chinese\",        dropout: float = 0.1,        max_answer_length: int = 30    ):        super().__init__()        self.encoder = AutoModel.from_pretrained(model_name)        hidden_size = self.encoder.config.hidden_size        self.max_answer_length = max_answer_length                self.dropout = nn.Dropout(dropout)        self.start_fc = nn.Linear(hidden_size, 1)        self.end_fc = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        token_type_ids: torch.Tensor = None,        start_positions: torch.Tensor = None,        end_positions: torch.Tensor = None,    ):        # ç¼–ç         outputs = self.encoder(            input_ids=input_ids,            attention_mask=attention_mask,            token_type_ids=token_type_ids,        )        sequence_output = self.dropout(outputs.last_hidden_state)                # é¢„æµ‹ start/end        start_logits = self.start_fc(sequence_output).squeeze(-1)        end_logits = self.end_fc(sequence_output).squeeze(-1)                # Mask padding        mask = attention_mask.bool()        start_logits = start_logits.masked_fill(~mask, float('-inf'))        end_logits = end_logits.masked_fill(~mask, float('-inf'))                # è®¡ç®—æŸå¤±        loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }        def decode(        self,        start_logits: torch.Tensor,        end_logits: torch.Tensor,        attention_mask: torch.Tensor,    ):        \"\"\"è§£ç æœ€ä½³ç­”æ¡ˆ span\"\"\"        batch_size, seq_len = start_logits.shape                # è®¡ç®—æ‰€æœ‰æœ‰æ•ˆ (start, end) å¯¹çš„åˆ†æ•°        start_probs = F.softmax(start_logits, dim=-1)        end_probs = F.softmax(end_logits, dim=-1)                results = []        for b in range(batch_size):            best_score = float('-inf')            best_start, best_end = 0, 0                        for start in range(seq_len):                if not attention_mask[b, start]:                    continue                for end in range(start, min(start + self.max_answer_length, seq_len)):                    if not attention_mask[b, end]:                        continue                    score = start_probs[b, start] * end_probs[b, end]                    if score &gt; best_score:                        best_score = score                        best_start, best_end = start, end                        results.append((best_start, best_end))                return results\n\næ•°æ®å¤„ç†from dataclasses import dataclassfrom typing import List, Optionalimport json@dataclassclass MRCExample:    qid: str    question: str    context: str    answer: Optional[str] = None    start_position: Optional[int] = None@dataclassclass MRCFeature:    input_ids: List[int]    attention_mask: List[int]    token_type_ids: List[int]    start_position: int    end_position: int    offset_mapping: List[tuple]class MRCProcessor:    def __init__(self, model_name: str, max_length: int = 512):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        def process(self, example: MRCExample) -&gt; MRCFeature:        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            return_offsets_mapping=True,            padding='max_length',        )                # å®šä½ç­”æ¡ˆä½ç½®        start_token, end_token = 0, 0        if example.start_position is not None:            offset = encoding['offset_mapping']            for idx, (start, end) in enumerate(offset):                if start &lt;= example.start_position &lt; end:                    start_token = idx                if start &lt; example.start_position + len(example.answer) &lt;= end:                    end_token = idx                    break                return MRCFeature(            input_ids=encoding['input_ids'],            attention_mask=encoding['attention_mask'],            token_type_ids=encoding.get('token_type_ids', [0] * len(encoding['input_ids'])),            start_position=start_token,            end_position=end_token,            offset_mapping=encoding['offset_mapping'],        )\n\nè®­ç»ƒå¾ªç¯from torch.utils.data import DataLoaderfrom torch.optim import AdamWfrom transformers import get_schedulerfrom tqdm import tqdmdef train_epoch(model, dataloader, optimizer, scheduler, device):    model.train()    total_loss = 0        for batch in tqdm(dataloader, desc=\"Training\"):        batch = {k: v.to(device) for k, v in batch.items()}                outputs = model(**batch)        loss = outputs['loss']                loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()        scheduler.step()        optimizer.zero_grad()                total_loss += loss.item()        return total_loss / len(dataloader)def evaluate(model, dataloader, device):    model.eval()    predictions = []        with torch.no_grad():        for batch in tqdm(dataloader, desc=\"Evaluating\"):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(                input_ids=batch['input_ids'],                attention_mask=batch['attention_mask'],                token_type_ids=batch.get('token_type_ids'),            )                        spans = model.decode(                outputs['start_logits'],                outputs['end_logits'],                batch['attention_mask'],            )            predictions.extend(spans)        return predictions# ä¸»è®­ç»ƒæµç¨‹def main():    # é…ç½®    model_name = \"bert-base-chinese\"    batch_size = 16    learning_rate = 3e-5    num_epochs = 3        # åˆå§‹åŒ–    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model = MRCModel(model_name).to(device)        # ä¼˜åŒ–å™¨    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)    scheduler = get_scheduler(        \"linear\",        optimizer=optimizer,        num_warmup_steps=500,        num_training_steps=num_epochs * len(train_dataloader),    )        # è®­ç»ƒ    for epoch in range(num_epochs):        loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")                # éªŒè¯        predictions = evaluate(model, val_dataloader, device)        f1 = compute_f1(predictions, val_labels)        print(f\"Validation F1: {f1:.4f}\")\n\nè¯„ä¼°æŒ‡æ ‡import reimport stringfrom collections import Counterdef normalize_answer(s: str) -&gt; str:    \"\"\"æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬\"\"\"    s = s.lower()    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)    s = ''.join(ch for ch in s if ch not in string.punctuation)    s = ' '.join(s.split())    return sdef compute_f1(prediction: str, ground_truth: str) -&gt; float:    pred_tokens = normalize_answer(prediction).split()    gold_tokens = normalize_answer(ground_truth).split()        if not pred_tokens or not gold_tokens:        return int(pred_tokens == gold_tokens)        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        if precision + recall == 0:        return 0        return 2 * precision * recall / (precision + recall)def compute_em(prediction: str, ground_truth: str) -&gt; float:    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\nä¸ç°ä»£æ–¹æ³•å¯¹æ¯”\n\n\næ–¹é¢\nç»å…¸ MRC (BiDAF)\nBERT-based\nLLM-based\n\n\n\nå‚æ•°é‡\n~2M\n110M-340M\n7B-70B+\n\n\nè®­ç»ƒæ•°æ®\nTask-specific\né¢„è®­ç»ƒ+å¾®è°ƒ\nå¤§è§„æ¨¡é¢„è®­ç»ƒ\n\n\næ¨ç†æ–¹å¼\nSpan extraction\nSpan extraction\nGeneration\n\n\né•¿æ–‡æ¡£\néœ€è¦åˆ‡åˆ†\néœ€è¦åˆ‡åˆ†\næ›´å¤§ä¸Šä¸‹æ–‡çª—å£\n\n\nå¤šè·³æ¨ç†\nå›°éš¾\næœ‰é™\nè¾ƒå¥½\n\n\nç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é‡åŒ–æ¨ç†import torch.quantization as quant# åŠ¨æ€é‡åŒ–model_int8 = quant.quantize_dynamic(    model.cpu(),    {nn.Linear},    dtype=torch.qint8)\n\nONNX å¯¼å‡ºimport torch.onnxdummy_input = {    'input_ids': torch.ones(1, 512, dtype=torch.long),    'attention_mask': torch.ones(1, 512, dtype=torch.long),    'token_type_ids': torch.zeros(1, 512, dtype=torch.long),}torch.onnx.export(    model,    (dummy_input['input_ids'], dummy_input['attention_mask'], dummy_input['token_type_ids']),    \"mrc_model.onnx\",    input_names=['input_ids', 'attention_mask', 'token_type_ids'],    output_names=['start_logits', 'end_logits'],    dynamic_axes={        'input_ids': {0: 'batch', 1: 'seq'},        'attention_mask': {0: 'batch', 1: 'seq'},        'token_type_ids': {0: 'batch', 1: 'seq'},    })\n\nå»¶ä¼¸é˜…è¯»\nHugging Face Question Answering\nSQuAD Leaderboard\nCMRC 2018 ä¸­æ–‡é˜…è¯»ç†è§£\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"æ¡ä»¶éšæœºåœºï¼šåŸç†ä¸å®ç°","url":"/2019/11/19/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0/","content":"æ¡ä»¶éšæœºåœº (CRF) æ˜¯åºåˆ—æ ‡æ³¨çš„ç»å…¸æ¨¡å‹ï¼Œå°½ç®¡æ·±åº¦å­¦ä¹ æ—¶ä»£ BERT ç­‰æ¨¡å‹å¤§æ”¾å¼‚å½©ï¼ŒCRF å±‚ä»ç„¶åœ¨ NERã€è¯æ€§æ ‡æ³¨ç­‰ä»»åŠ¡ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚\nä¸ºä»€ä¹ˆéœ€è¦ CRFï¼Ÿç‹¬ç«‹åˆ†ç±»çš„é—®é¢˜å¦‚æœå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åˆ†ç±»ï¼š\n\nä¼šå¯¼è‡´æ ‡ç­¾ä¸ä¸€è‡´ï¼Œä¾‹å¦‚ï¼š\nè¾“å…¥: \"åŒ— äº¬ æ˜¯ ä¸­ å›½ é¦– éƒ½\"é”™è¯¯: B-LOC I-PER O B-LOC I-LOC I-LOC I-LOCæ­£ç¡®: B-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC\n\nCRF çš„è§£å†³æ–¹æ¡ˆCRF å»ºæ¨¡æ•´ä¸ªåºåˆ—çš„è”åˆæ¦‚ç‡ï¼Œè€ƒè™‘æ ‡ç­¾ä¹‹é—´çš„è½¬ç§»çº¦æŸã€‚\næ•°å­¦åŸç†æ¡ä»¶æ¦‚ç‡\nå…¶ä¸­ï¼š\n\nï¼šå‘å°„åˆ†æ•°ï¼ˆemission scoreï¼‰\nï¼šè½¬ç§»åˆ†æ•°ï¼ˆtransition scoreï¼‰\nï¼šé…åˆ†å‡½æ•°ï¼ˆå½’ä¸€åŒ–é¡¹ï¼‰\n\né…åˆ†å‡½æ•°\nç›´æ¥è®¡ç®—å¤æ‚åº¦ä¸º ï¼Œä½¿ç”¨å‰å‘ç®—æ³•å¯é™è‡³ ã€‚\nPyTorch å®ç°CRF Layerimport torchimport torch.nn as nnclass CRF(nn.Module):    def __init__(self, num_tags, batch_first=True):        super().__init__()        self.num_tags = num_tags        self.batch_first = batch_first                # è½¬ç§»çŸ©é˜µ: transitions[i, j] = ä»æ ‡ç­¾ j è½¬ç§»åˆ°æ ‡ç­¾ i çš„åˆ†æ•°        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))                # èµ·å§‹å’Œç»“æŸè½¬ç§»        self.start_transitions = nn.Parameter(torch.randn(num_tags))        self.end_transitions = nn.Parameter(torch.randn(num_tags))        def forward(self, emissions, tags, mask=None):        \"\"\"è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±\"\"\"        if mask is None:            mask = torch.ones_like(tags, dtype=torch.bool)                if self.batch_first:            emissions = emissions.transpose(0, 1)            tags = tags.transpose(0, 1)            mask = mask.transpose(0, 1)                # è®¡ç®—åˆ†å­ï¼ˆæ­£ç¡®è·¯å¾„çš„åˆ†æ•°ï¼‰        numerator = self._compute_score(emissions, tags, mask)                # è®¡ç®—åˆ†æ¯ï¼ˆé…åˆ†å‡½æ•°ï¼‰        denominator = self._compute_normalizer(emissions, mask)                # è´Ÿå¯¹æ•°ä¼¼ç„¶        return (denominator - numerator).mean()        def _compute_score(self, emissions, tags, mask):        \"\"\"è®¡ç®—ç»™å®šæ ‡ç­¾åºåˆ—çš„åˆ†æ•°\"\"\"        seq_len, batch_size = tags.shape                # èµ·å§‹åˆ†æ•°        score = self.start_transitions[tags[0]]        score += emissions[0, torch.arange(batch_size), tags[0]]                for i in range(1, seq_len):            # è½¬ç§»åˆ†æ•° + å‘å°„åˆ†æ•°            score += self.transitions[tags[i], tags[i-1]] * mask[i]            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]                # ç»“æŸåˆ†æ•°        last_tag_idx = mask.sum(dim=0) - 1        last_tags = tags.gather(0, last_tag_idx.unsqueeze(0)).squeeze(0)        score += self.end_transitions[last_tags]                return score        def _compute_normalizer(self, emissions, mask):        \"\"\"å‰å‘ç®—æ³•è®¡ç®—é…åˆ†å‡½æ•°\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # åˆå§‹åŒ–        score = self.start_transitions + emissions[0]                for i in range(1, seq_len):            # broadcast: (batch, num_tags, 1) + (num_tags, num_tags) + (batch, 1, num_tags)            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score = torch.logsumexp(next_score, dim=1)                        # åº”ç”¨ mask            score = torch.where(mask[i].unsqueeze(1), next_score, score)                # æ·»åŠ ç»“æŸåˆ†æ•°        score += self.end_transitions                return torch.logsumexp(score, dim=1)        def decode(self, emissions, mask=None):        \"\"\"Viterbi è§£ç \"\"\"        if mask is None:            mask = torch.ones(emissions.shape[:2], dtype=torch.bool, device=emissions.device)                if self.batch_first:            emissions = emissions.transpose(0, 1)            mask = mask.transpose(0, 1)                return self._viterbi_decode(emissions, mask)        def _viterbi_decode(self, emissions, mask):        \"\"\"Viterbi ç®—æ³•\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # åˆå§‹åŒ–        score = self.start_transitions + emissions[0]        history = []                for i in range(1, seq_len):            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score, indices = next_score.max(dim=1)                        score = torch.where(mask[i].unsqueeze(1), next_score, score)            history.append(indices)                # æ·»åŠ ç»“æŸåˆ†æ•°        score += self.end_transitions                # å›æº¯        best_tags_list = []        _, best_last_tag = score.max(dim=1)                for idx in range(batch_size):            best_tags = [best_last_tag[idx].item()]            seq_length = int(mask[:, idx].sum().item())                        for hist in reversed(history[:seq_length-1]):                best_last_tag_idx = best_tags[-1]                best_tags.append(hist[idx, best_last_tag_idx].item())                        best_tags.reverse()            best_tags_list.append(best_tags)                return best_tags_list\n\nä¸ BiLSTM ç»“åˆclass BiLSTM_CRF(nn.Module):    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):        super().__init__()        self.embedding = nn.Embedding(vocab_size, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2,                            num_layers=2, bidirectional=True, batch_first=True)        self.fc = nn.Linear(hidden_dim, num_tags)        self.crf = CRF(num_tags)        def forward(self, x, tags, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf(emissions, tags, mask)        def predict(self, x, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf.decode(emissions, mask)\n\nç°ä»£åº”ç”¨ï¼šBERT + CRFå°½ç®¡ BERT å·²ç»å¾ˆå¼ºå¤§ï¼Œä½† CRF å±‚ä»èƒ½å¸¦æ¥ä¸€è‡´æ€§æå‡ï¼š\nfrom transformers import BertModelclass BERT_CRF(nn.Module):    def __init__(self, bert_name, num_tags):        super().__init__()        self.bert = BertModel.from_pretrained(bert_name)        self.dropout = nn.Dropout(0.1)        self.fc = nn.Linear(self.bert.config.hidden_size, num_tags)        self.crf = CRF(num_tags)        def forward(self, input_ids, attention_mask, tags=None):        outputs = self.bert(input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)        emissions = self.fc(sequence_output)                if tags is not None:            return self.crf(emissions, tags, attention_mask.bool())        else:            return self.crf.decode(emissions, attention_mask.bool())\n\næ€§èƒ½å¯¹æ¯”ï¼ˆCoNLL-2003 NERï¼‰\n\n\næ¨¡å‹\nF1\n\n\n\nBiLSTM\n88.2\n\n\nBiLSTM + CRF\n90.1\n\n\nBERT\n92.4\n\n\nBERT + CRF\n92.8\n\n\nRoBERTa + CRF\n93.2\n\n\nè®­ç»ƒæŠ€å·§1. æ ‡ç­¾å¹³æ»‘def label_smoothing_loss(crf, emissions, tags, mask, epsilon=0.1):    \"\"\"å¸¦æ ‡ç­¾å¹³æ»‘çš„ CRF æŸå¤±\"\"\"    nll_loss = crf(emissions, tags, mask)        # å‡åŒ€åˆ†å¸ƒçš„æŸå¤±    uniform_loss = -torch.logsumexp(emissions, dim=-1).mean()        return (1 - epsilon) * nll_loss + epsilon * uniform_loss\n\n2. çº¦æŸè§£ç # æ·»åŠ ç¡¬çº¦æŸï¼šB-X åé¢åªèƒ½æ¥ I-X æˆ– Odef add_constraints(transitions, tag2idx):    for tag_from, idx_from in tag2idx.items():        for tag_to, idx_to in tag2idx.items():            if tag_from.startswith('B-') or tag_from.startswith('I-'):                entity = tag_from[2:]                if tag_to.startswith('I-') and tag_to[2:] != entity:                    transitions.data[idx_to, idx_from] = -1e9\n\nå»¶ä¼¸é˜…è¯»\nLafferty et al., Conditional Random Fields (2001)\nHuang et al., Bidirectional LSTM-CRF Models for Sequence Tagging (2015)\npytorch-crf Documentation\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["machine learning","NLP","CRF"]}]