[{"title":"NLP 学习路线：从基础到大语言模型","url":"/2019/10/31/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E2%80%94%E2%80%94%E8%AF%BB%E9%A6%99%E4%BE%AC%E7%A7%91%E6%8A%80%E6%9D%8E%E7%BA%A7%E4%B8%BA%E3%80%8A%E5%87%BA%E5%85%A5NLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BB%BA%E8%AE%AE%E3%80%8B%E6%96%87%E7%AB%A0/","content":"本文整理了 NLP 领域的学习路线，结合经典理论与现代大语言模型技术。\n推荐学习资源经典教材\n\n\n书籍\n内容\n难度\n\n\n\nSpeech and Language Processing (Jurafsky)\nNLP 全面综述\n⭐⭐\n\n\nIntroduction to Information Retrieval\n信息检索基础\n⭐⭐\n\n\nPattern Recognition and Machine Learning\n机器学习理论\n⭐⭐⭐⭐\n\n\nDeep Learning (Goodfellow)\n深度学习基础\n⭐⭐⭐\n\n\n现代资源\nStanford CS224N: NLP with Deep Learning\nHugging Face Course\nLLM University by Cohere\n\n阶段一：NLP 基础语言模型基础N-gram 模型：N-1 阶马尔可夫假设\n$$P(w_k | w_1, …, w_{k-1}) \\approx P(w_k | w_{k-n+1}, …, w_{k-1})$$\nfrom collections import defaultdictimport numpy as npclass NGramLM:    def __init__(self, n=3):        self.n = n        self.counts = defaultdict(lambda: defaultdict(int))        self.totals = defaultdict(int)        def train(self, corpus):        for sentence in corpus:            tokens = [&#x27;&lt;s&gt;&#x27;] * (self.n - 1) + sentence + [&#x27;&lt;/s&gt;&#x27;]            for i in range(len(tokens) - self.n + 1):                context = tuple(tokens[i:i+self.n-1])                word = tokens[i+self.n-1]                self.counts[context][word] += 1                self.totals[context] += 1        def probability(self, word, context):        context = tuple(context[-(self.n-1):])        return self.counts[context][word] / max(self.totals[context], 1)\n\n词向量从 One-hot 到 Dense Embedding 的演进：\n\n\n\n方法\n年份\n特点\n\n\n\nOne-hot\n-\n稀疏，无语义\n\n\nWord2Vec\n2013\n分布式表示\n\n\nGloVe\n2014\n全局统计\n\n\nFastText\n2016\n子词信息\n\n\nELMo\n2018\n上下文相关\n\n\nBERT\n2018\n双向上下文\n\n\n阶段二：深度学习 NLPTransformer 架构import torchimport torch.nn as nnimport mathclass MultiHeadAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.d_k = d_model // n_heads        self.n_heads = n_heads                self.W_q = nn.Linear(d_model, d_model)        self.W_k = nn.Linear(d_model, d_model)        self.W_v = nn.Linear(d_model, d_model)        self.W_o = nn.Linear(d_model, d_model)        def forward(self, Q, K, V, mask=None):        batch_size = Q.size(0)                # Linear projections        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)                # Attention scores        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)                if mask is not None:            scores = scores.masked_fill(mask == 0, -1e9)                attn = torch.softmax(scores, dim=-1)        output = torch.matmul(attn, V)                # Concatenate and project        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)        return self.W_o(output)\n\n注意力机制的数学表达$$\\text{Attention}(Q, K, V) &#x3D; \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n阶段三：大语言模型LLM 架构演进GPT-1 (2018) → GPT-2 → GPT-3 → ChatGPT → GPT-4     ↓BERT → RoBERTa → DeBERTa     ↓T5 → Flan-T5 → UL2     ↓LLaMA → LLaMA 2 → Mistral → Mixtral\n\nPrompt Engineering# 1. Zero-shotprompt = &quot;Translate to French: Hello, how are you?&quot;# 2. Few-shotprompt = &quot;&quot;&quot;Translate to French:Hello -&gt; BonjourGoodbye -&gt; Au revoirHow are you? -&gt;&quot;&quot;&quot;# 3. Chain-of-Thoughtprompt = &quot;&quot;&quot;Q: If I have 3 apples and buy 5 more, how many do I have?A: Let&#x27;s think step by step.1. I start with 3 apples.2. I buy 5 more apples.3. Total = 3 + 5 = 8 apples.The answer is 8.Q: If I have 7 oranges and eat 2, how many remain?A: Let&#x27;s think step by step.&quot;&quot;&quot;\n\nFine-tuning 技术\n\n\n方法\n可训练参数\n适用场景\n\n\n\nFull Fine-tuning\n100%\n大量数据，充足算力\n\n\nLoRA\n0.1-1%\n资源受限\n\n\nQLoRA\n0.1%\n消费级 GPU\n\n\nPrefix Tuning\n0.1%\n多任务\n\n\nPrompt Tuning\n&lt;0.01%\n极端资源受限\n\n\nfrom peft import LoraConfig, get_peft_modellora_config = LoraConfig(    r=8,    lora_alpha=32,    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],    lora_dropout=0.1,    bias=&quot;none&quot;,)model = get_peft_model(base_model, lora_config)print(f&quot;Trainable params: &#123;model.print_trainable_parameters()&#125;&quot;)\n\n阶段四：高级主题检索增强生成 (RAG)from langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.vectorstores import Chromafrom langchain.chains import RetrievalQA# 构建向量库embeddings = HuggingFaceEmbeddings(model_name=&quot;BAAI/bge-small-zh&quot;)vectorstore = Chroma.from_documents(documents, embeddings)# 创建 RAG 链qa = RetrievalQA.from_chain_type(    llm=llm,    retriever=vectorstore.as_retriever(search_kwargs=&#123;&quot;k&quot;: 3&#125;))\n\n模型评估# 困惑度 (Perplexity)def perplexity(model, tokenizer, text):    encodings = tokenizer(text, return_tensors=&#x27;pt&#x27;)    max_length = model.config.n_positions        nlls = []    for i in range(0, encodings.input_ids.size(1), max_length):        begin_loc = max(i - max_length, 0)        end_loc = i + max_length        input_ids = encodings.input_ids[:, begin_loc:end_loc]        target_ids = input_ids.clone()        target_ids[:, :-1] = -100                with torch.no_grad():            outputs = model(input_ids, labels=target_ids)            nlls.append(outputs.loss)        return torch.exp(torch.stack(nlls).mean())\n\n实践项目建议\n入门：情感分析、文本分类\n进阶：命名实体识别、机器翻译\n高级：问答系统、RAG 应用\n专家：LLM 预训练、RLHF\n\n延伸阅读\nAttention Is All You Need\nBERT Paper\nLLaMA Paper\nLoRA Paper\n\n\n\n转载请注明出处\n\n","tags":["NLP","machine learning","LLM"]},{"title":"神经网络机器阅读理解：从 Attention 到 LLM","url":"/2019/11/22/Nuural-Approaches-to-Machine-Reading-Comprehension-and-Dialogue/","content":"本文综述神经网络在机器阅读理解和对话系统中的发展历程，从早期的注意力机制到现代大语言模型。\n发展时间线2015-2016: 注意力机制兴起    └── Attentive Reader, Impatient Reader, BiDAF2017-2018: 深度交互与预训练    └── R-Net, QANet, BERT2019-2020: 大规模预训练    └── RoBERTa, ALBERT, T52021-2023: 大语言模型时代    └── GPT-3, ChatGPT, GPT-4, LLaMA2024-: 检索增强与多模态    └── RAG, Vision-Language Models\n\n核心技术演进阶段一：注意力机制 (2015-2017)问题：如何让模型”关注”与问题相关的上下文？\n$$\\alpha_i &#x3D; \\text{softmax}(s(h_i, q))$$\n$$c &#x3D; \\sum_i \\alpha_i h_i$$\n代表模型：Attentive Reader, BiDAF\n阶段二：深度交互 (2017-2018)问题：如何建模问题和上下文的复杂交互？\n技术：多轮注意力、自注意力、门控机制\n# 多轮推理 (R-Net 风格)for layer in range(num_layers):    # 自注意力    context = self_attention(context, context)    # 交叉注意力    context = cross_attention(context, question)\n\n阶段三：预训练语言模型 (2018-2020)范式转变：从 task-specific 到 pretrain-finetune\n$$\\theta^* &#x3D; \\arg\\min_\\theta \\mathcal{L}{task}(\\text{PLM}\\theta(x), y)$$\n代表模型：BERT, RoBERTa, ALBERT\nfrom transformers import AutoModelForQuestionAnsweringmodel = AutoModelForQuestionAnswering.from_pretrained(&quot;bert-base-uncased&quot;)# Fine-tune on SQuAD\n\n阶段四：大语言模型 (2020-至今)范式转变：从 fine-tuning 到 prompting\n# Few-shot promptingprompt = &quot;&quot;&quot;Context: The Eiffel Tower was built in 1889.Question: When was the Eiffel Tower built?Answer: 1889Context: &#123;context&#125;Question: &#123;question&#125;Answer:&quot;&quot;&quot;\n\n架构对比\n\n\n模型\n参数量\n训练范式\nSQuAD 2.0 F1\n\n\n\nBiDAF\n~2M\n从零训练\n77.3\n\n\nBERT-base\n110M\n预训练+微调\n88.5\n\n\nBERT-large\n340M\n预训练+微调\n90.9\n\n\nRoBERTa-large\n355M\n预训练+微调\n91.4\n\n\nGPT-3\n175B\nFew-shot\n~88\n\n\nGPT-4\n~1.8T\nZero-shot\n~95\n\n\n现代 MRC 系统设计RAG 架构class ModernMRC:    def __init__(self, retriever, reader):        self.retriever = retriever  # Dense retriever        self.reader = reader        # LLM        def answer(self, question: str, knowledge_base: str = None):        # 1. 检索        if knowledge_base:            docs = self.retriever.retrieve(question, knowledge_base)            context = &quot;\\n\\n&quot;.join([d.text for d in docs])        else:            context = &quot;&quot;                # 2. 阅读理解/生成        prompt = self._build_prompt(question, context)        answer = self.reader.generate(prompt)                # 3. 后处理（可选：验证、引用）        return self._postprocess(answer, docs)        def _build_prompt(self, question, context):        if context:            return f&quot;&quot;&quot;Based on the following context, answer the question.Context:&#123;context&#125;Question: &#123;question&#125;Answer:&quot;&quot;&quot;        else:            return f&quot;Question: &#123;question&#125;\\nAnswer:&quot;\n\n多跳推理class MultiHopReasoner:    def __init__(self, retriever, llm, max_hops=3):        self.retriever = retriever        self.llm = llm        self.max_hops = max_hops        def reason(self, question):        reasoning_chain = []        current_query = question                for hop in range(self.max_hops):            # 检索            docs = self.retriever.retrieve(current_query)                        # 生成中间推理            intermediate = self.llm.generate(                f&quot;Based on: &#123;docs&#125;\\nQuestion: &#123;current_query&#125;\\n&quot;                f&quot;Provide intermediate reasoning or the final answer:&quot;            )                        reasoning_chain.append(&#123;                &#x27;query&#x27;: current_query,                &#x27;docs&#x27;: docs,                &#x27;reasoning&#x27;: intermediate            &#125;)                        # 检查是否已得到答案            if self._is_final_answer(intermediate):                break                        # 生成下一跳查询            current_query = self._generate_next_query(question, reasoning_chain)                return self._synthesize_answer(question, reasoning_chain)\n\n对话系统中的 MRC对话式问答class ConversationalQA:    def __init__(self, mrc_model, history_length=5):        self.mrc_model = mrc_model        self.history = []        self.history_length = history_length        def ask(self, question, context=None):        # 将对话历史纳入问题        contextualized_question = self._contextualize(question)                # 获取答案        answer = self.mrc_model.answer(contextualized_question, context)                # 更新历史        self.history.append(&#123;&#x27;q&#x27;: question, &#x27;a&#x27;: answer&#125;)        if len(self.history) &gt; self.history_length:            self.history.pop(0)                return answer        def _contextualize(self, question):        if not self.history:            return question                history_text = &quot;\\n&quot;.join([            f&quot;Q: &#123;turn[&#x27;q&#x27;]&#125;\\nA: &#123;turn[&#x27;a&#x27;]&#125;&quot;            for turn in self.history        ])                return f&quot;Conversation history:\\n&#123;history_text&#125;\\n\\nCurrent question: &#123;question&#125;&quot;\n\n评估体系传统指标\n\n\n指标\n定义\n适用场景\n\n\n\nEM\n精确匹配\n抽取式 QA\n\n\nF1\nToken 重叠\n抽取式 QA\n\n\nBLEU\nN-gram 重叠\n生成式 QA\n\n\nROUGE\n召回导向重叠\n摘要、长答案\n\n\nLLM 时代指标# LLM-as-Judgedef llm_evaluate(question, reference, prediction):    prompt = f&quot;&quot;&quot;Evaluate the answer quality on a scale of 1-5:Question: &#123;question&#125;Reference Answer: &#123;reference&#125;Model Answer: &#123;prediction&#125;Criteria:- Correctness: Is the information accurate?- Completeness: Does it fully answer the question?- Conciseness: Is it appropriately brief?Score (1-5):&quot;&quot;&quot;        return llm.generate(prompt)\n\n延伸阅读\nReading Wikipedia to Answer Open-Domain Questions\nRAG Paper\nHotpotQA: Multi-hop Reasoning\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\n\n转载请注明出处\n\n","tags":["LLM","MRC","KBQA","Deep learning"]},{"title":"因果关系推断介绍","url":"/2019/10/03/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/","content":"因果推断是机器学习领域的重要研究方向，特别是在大语言模型时代，理解因果关系对于构建可解释、可信赖的 AI 系统至关重要。\n为什么需要因果推断？传统机器学习依赖相关性，但相关性不等于因果性。例如：\n\n冰淇淋销量与溺水事件正相关（共同原因：夏天）\nLLM 可能学到虚假相关性，导致 hallucination\n\n因果推断帮助我们：\n\n理解干预效果（如果我做 X，会发生什么？）\n进行反事实推理（如果当时做了 Y，结果会怎样？）\n构建更鲁棒的模型\n\n核心概念因果图 (Causal Graph)使用有向无环图 (DAG) 表示变量之间的因果关系：\nX → Y → Z    (链式结构)X ← W → Y    (混杂结构)  X → W ← Y    (对撞结构)\n\n结构因果模型 (SCM)$$Y &#x3D; f(X, U_Y)$$\n其中 $X$ 是原因，$Y$ 是结果，$U_Y$ 是噪声项。\ndo 算子与干预区分观测和干预：\n\n观测：$P(Y|X&#x3D;x)$ — 看到 X&#x3D;x 时 Y 的分布\n干预：$P(Y|do(X&#x3D;x))$ — 强制设置 X&#x3D;x 时 Y 的分布\n\n因果发现算法PC 算法基于条件独立性检验的经典算法：\n# PC 算法伪代码def pc_algorithm(data, alpha=0.05):    # 1. 初始化完全图    G = complete_graph(variables)        # 2. 骨架学习：移除条件独立的边    for (X, Y) in edges(G):        for S in subsets(neighbors):            if conditional_independent(X, Y, S, alpha):                remove_edge(G, X, Y)                sep_set[X, Y] = S        # 3. 方向确定：识别 v-structure    orient_v_structures(G, sep_set)        return G\n\nPython 实现参考：fooSynaptic&#x2F;py_pcalg\n现代方法\n\n\n方法\n特点\n适用场景\n\n\n\nNOTEARS\n连续优化，可微分\n线性&#x2F;非线性因果发现\n\n\nDAG-GNN\n基于图神经网络\n大规模因果图学习\n\n\nCausal Transformer\n结合注意力机制\n时序因果推断\n\n\n因果推断与大语言模型LLM 中的因果问题\nHallucination：模型学到虚假相关性\nBias：训练数据中的混杂因素\nRobustness：分布外泛化能力差\n\n解决方案# 因果提示 (Causal Prompting) 示例prompt = &quot;&quot;&quot;请分析以下事件的因果关系，而非相关性：事件A: 公司增加广告投入事件B: 销售额上升问：A 是否导致了 B？请考虑可能的混杂因素。&quot;&quot;&quot;\n\n因果推理增强 RAGclass CausalRAG:    def __init__(self, retriever, causal_graph):        self.retriever = retriever        self.causal_graph = causal_graph        def retrieve(self, query):        # 1. 识别查询中的因果关系        cause, effect = extract_causal_pair(query)                # 2. 基于因果图过滤无关文档        relevant_vars = self.causal_graph.ancestors(effect)                # 3. 检索因果相关的文档        docs = self.retriever.search(query)        return filter_by_causal_relevance(docs, relevant_vars)\n\n工具与资源\n\n\n工具\n语言\n功能\n\n\n\nDoWhy\nPython\n因果推断框架\n\n\nCausalNex\nPython\n贝叶斯网络 + 因果发现\n\n\npgmpy\nPython\n概率图模型\n\n\nTetrad\nJava\n因果搜索算法\n\n\n# DoWhy 示例import dowhyfrom dowhy import CausalModelmodel = CausalModel(    data=df,    treatment=&#x27;treatment&#x27;,    outcome=&#x27;outcome&#x27;,    graph=&#x27;digraph &#123;treatment -&gt; outcome; confounder -&gt; treatment; confounder -&gt; outcome&#125;&#x27;)# 识别因果效应identified = model.identify_effect()# 估计因果效应estimate = model.estimate_effect(identified, method_name=&quot;backdoor.propensity_score_matching&quot;)\n\n延伸阅读\nJudea Pearl, The Book of Why (2018)\nPeters et al., Elements of Causal Inference (2017)\nStanford CS 228: Probabilistic Graphical Models\n\n\n\n转载请注明出处\n\n","tags":["machine learning","bayesian network","causality infer"]},{"title":"MRC 模型实现：从 TensorFlow 到 PyTorch","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E5%8E%BB%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%87%E6%9C%AC%E5%B9%B6%E4%B8%94%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%EF%BC%88tensorflow%E5%AE%9E%E6%88%98%EF%BC%89/","content":"本文介绍机器阅读理解模型的完整实现，涵盖经典架构和现代最佳实践。\n问题定义输入：\n\n问题 $Q &#x3D; (q_1, q_2, …, q_m)$\n文档 $P &#x3D; (p_1, p_2, …, p_n)$\n\n输出：\n\n答案起始位置 $start \\in [1, n]$\n答案结束位置 $end \\in [start, n]$\n\n经典架构Input → Embedding → Encoding → Matching → Fusion → Decoding\n\n各层详解\n\n\n层\n功能\n现代替代\n\n\n\nEmbedding\nToken → Vector\nSubword Tokenization\n\n\nEncoding\n序列编码\nTransformer Encoder\n\n\nMatching\nQ-P 交互\nCross-Attention\n\n\nFusion\n信息融合\nSelf-Attention\n\n\nDecoding\nSpan 预测\nLinear + Softmax\n\n\nPyTorch 实现完整模型import torchimport torch.nn as nnimport torch.nn.functional as Ffrom transformers import AutoModel, AutoTokenizerclass MRCModel(nn.Module):    &quot;&quot;&quot;基于 Transformer 的 MRC 模型&quot;&quot;&quot;        def __init__(        self,         model_name: str = &quot;bert-base-chinese&quot;,        dropout: float = 0.1,        max_answer_length: int = 30    ):        super().__init__()        self.encoder = AutoModel.from_pretrained(model_name)        hidden_size = self.encoder.config.hidden_size        self.max_answer_length = max_answer_length                self.dropout = nn.Dropout(dropout)        self.start_fc = nn.Linear(hidden_size, 1)        self.end_fc = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        token_type_ids: torch.Tensor = None,        start_positions: torch.Tensor = None,        end_positions: torch.Tensor = None,    ):        # 编码        outputs = self.encoder(            input_ids=input_ids,            attention_mask=attention_mask,            token_type_ids=token_type_ids,        )        sequence_output = self.dropout(outputs.last_hidden_state)                # 预测 start/end        start_logits = self.start_fc(sequence_output).squeeze(-1)        end_logits = self.end_fc(sequence_output).squeeze(-1)                # Mask padding        mask = attention_mask.bool()        start_logits = start_logits.masked_fill(~mask, float(&#x27;-inf&#x27;))        end_logits = end_logits.masked_fill(~mask, float(&#x27;-inf&#x27;))                # 计算损失        loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return &#123;            &#x27;loss&#x27;: loss,            &#x27;start_logits&#x27;: start_logits,            &#x27;end_logits&#x27;: end_logits,        &#125;        def decode(        self,        start_logits: torch.Tensor,        end_logits: torch.Tensor,        attention_mask: torch.Tensor,    ):        &quot;&quot;&quot;解码最佳答案 span&quot;&quot;&quot;        batch_size, seq_len = start_logits.shape                # 计算所有有效 (start, end) 对的分数        start_probs = F.softmax(start_logits, dim=-1)        end_probs = F.softmax(end_logits, dim=-1)                results = []        for b in range(batch_size):            best_score = float(&#x27;-inf&#x27;)            best_start, best_end = 0, 0                        for start in range(seq_len):                if not attention_mask[b, start]:                    continue                for end in range(start, min(start + self.max_answer_length, seq_len)):                    if not attention_mask[b, end]:                        continue                    score = start_probs[b, start] * end_probs[b, end]                    if score &gt; best_score:                        best_score = score                        best_start, best_end = start, end                        results.append((best_start, best_end))                return results\n\n数据处理from dataclasses import dataclassfrom typing import List, Optionalimport json@dataclassclass MRCExample:    qid: str    question: str    context: str    answer: Optional[str] = None    start_position: Optional[int] = None@dataclassclass MRCFeature:    input_ids: List[int]    attention_mask: List[int]    token_type_ids: List[int]    start_position: int    end_position: int    offset_mapping: List[tuple]class MRCProcessor:    def __init__(self, model_name: str, max_length: int = 512):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        def process(self, example: MRCExample) -&gt; MRCFeature:        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation=&#x27;only_second&#x27;,            return_offsets_mapping=True,            padding=&#x27;max_length&#x27;,        )                # 定位答案位置        start_token, end_token = 0, 0        if example.start_position is not None:            offset = encoding[&#x27;offset_mapping&#x27;]            for idx, (start, end) in enumerate(offset):                if start &lt;= example.start_position &lt; end:                    start_token = idx                if start &lt; example.start_position + len(example.answer) &lt;= end:                    end_token = idx                    break                return MRCFeature(            input_ids=encoding[&#x27;input_ids&#x27;],            attention_mask=encoding[&#x27;attention_mask&#x27;],            token_type_ids=encoding.get(&#x27;token_type_ids&#x27;, [0] * len(encoding[&#x27;input_ids&#x27;])),            start_position=start_token,            end_position=end_token,            offset_mapping=encoding[&#x27;offset_mapping&#x27;],        )\n\n训练循环from torch.utils.data import DataLoaderfrom torch.optim import AdamWfrom transformers import get_schedulerfrom tqdm import tqdmdef train_epoch(model, dataloader, optimizer, scheduler, device):    model.train()    total_loss = 0        for batch in tqdm(dataloader, desc=&quot;Training&quot;):        batch = &#123;k: v.to(device) for k, v in batch.items()&#125;                outputs = model(**batch)        loss = outputs[&#x27;loss&#x27;]                loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()        scheduler.step()        optimizer.zero_grad()                total_loss += loss.item()        return total_loss / len(dataloader)def evaluate(model, dataloader, device):    model.eval()    predictions = []        with torch.no_grad():        for batch in tqdm(dataloader, desc=&quot;Evaluating&quot;):            batch = &#123;k: v.to(device) for k, v in batch.items()&#125;                        outputs = model(                input_ids=batch[&#x27;input_ids&#x27;],                attention_mask=batch[&#x27;attention_mask&#x27;],                token_type_ids=batch.get(&#x27;token_type_ids&#x27;),            )                        spans = model.decode(                outputs[&#x27;start_logits&#x27;],                outputs[&#x27;end_logits&#x27;],                batch[&#x27;attention_mask&#x27;],            )            predictions.extend(spans)        return predictions# 主训练流程def main():    # 配置    model_name = &quot;bert-base-chinese&quot;    batch_size = 16    learning_rate = 3e-5    num_epochs = 3        # 初始化    device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)    model = MRCModel(model_name).to(device)        # 优化器    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)    scheduler = get_scheduler(        &quot;linear&quot;,        optimizer=optimizer,        num_warmup_steps=500,        num_training_steps=num_epochs * len(train_dataloader),    )        # 训练    for epoch in range(num_epochs):        loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)        print(f&quot;Epoch &#123;epoch+1&#125;, Loss: &#123;loss:.4f&#125;&quot;)                # 验证        predictions = evaluate(model, val_dataloader, device)        f1 = compute_f1(predictions, val_labels)        print(f&quot;Validation F1: &#123;f1:.4f&#125;&quot;)\n\n评估指标import reimport stringfrom collections import Counterdef normalize_answer(s: str) -&gt; str:    &quot;&quot;&quot;标准化答案文本&quot;&quot;&quot;    s = s.lower()    s = re.sub(r&#x27;\\b(a|an|the)\\b&#x27;, &#x27; &#x27;, s)    s = &#x27;&#x27;.join(ch for ch in s if ch not in string.punctuation)    s = &#x27; &#x27;.join(s.split())    return sdef compute_f1(prediction: str, ground_truth: str) -&gt; float:    pred_tokens = normalize_answer(prediction).split()    gold_tokens = normalize_answer(ground_truth).split()        if not pred_tokens or not gold_tokens:        return int(pred_tokens == gold_tokens)        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        if precision + recall == 0:        return 0        return 2 * precision * recall / (precision + recall)def compute_em(prediction: str, ground_truth: str) -&gt; float:    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\n与现代方法对比\n\n\n方面\n经典 MRC (BiDAF)\nBERT-based\nLLM-based\n\n\n\n参数量\n~2M\n110M-340M\n7B-70B+\n\n\n训练数据\nTask-specific\n预训练+微调\n大规模预训练\n\n\n推理方式\nSpan extraction\nSpan extraction\nGeneration\n\n\n长文档\n需要切分\n需要切分\n更大上下文窗口\n\n\n多跳推理\n困难\n有限\n较好\n\n\n生产环境优化量化推理import torch.quantization as quant# 动态量化model_int8 = quant.quantize_dynamic(    model.cpu(),    &#123;nn.Linear&#125;,    dtype=torch.qint8)\n\nONNX 导出import torch.onnxdummy_input = &#123;    &#x27;input_ids&#x27;: torch.ones(1, 512, dtype=torch.long),    &#x27;attention_mask&#x27;: torch.ones(1, 512, dtype=torch.long),    &#x27;token_type_ids&#x27;: torch.zeros(1, 512, dtype=torch.long),&#125;torch.onnx.export(    model,    (dummy_input[&#x27;input_ids&#x27;], dummy_input[&#x27;attention_mask&#x27;], dummy_input[&#x27;token_type_ids&#x27;]),    &quot;mrc_model.onnx&quot;,    input_names=[&#x27;input_ids&#x27;, &#x27;attention_mask&#x27;, &#x27;token_type_ids&#x27;],    output_names=[&#x27;start_logits&#x27;, &#x27;end_logits&#x27;],    dynamic_axes=&#123;        &#x27;input_ids&#x27;: &#123;0: &#x27;batch&#x27;, 1: &#x27;seq&#x27;&#125;,        &#x27;attention_mask&#x27;: &#123;0: &#x27;batch&#x27;, 1: &#x27;seq&#x27;&#125;,        &#x27;token_type_ids&#x27;: &#123;0: &#x27;batch&#x27;, 1: &#x27;seq&#x27;&#125;,    &#125;)\n\n延伸阅读\nHugging Face Question Answering\nSQuAD Leaderboard\nCMRC 2018 中文阅读理解\n\n\n\n转载请注明出处\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"机器阅读理解实战：从零构建问答系统","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%AE%9E%E8%B7%B5/","content":"本文从零开始实现一个机器阅读理解系统，涵盖数据处理、模型构建、训练和推理的完整流程。\n任务定义给定上下文 $C$ 和问题 $Q$，预测答案 $A$ 在 $C$ 中的位置：\n$$(start, end) &#x3D; \\arg\\max_{i,j} P(start&#x3D;i, end&#x3D;j | C, Q)$$\n数据处理SQuAD 数据格式import jsonfrom dataclasses import dataclassfrom typing import List, Optional@dataclassclass Example:    context: str    question: str    answer_text: str    start_position: int    end_position: intdef load_squad(file_path: str) -&gt; List[Example]:    with open(file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:        data = json.load(f)        examples = []    for article in data[&#x27;data&#x27;]:        for paragraph in article[&#x27;paragraphs&#x27;]:            context = paragraph[&#x27;context&#x27;]            for qa in paragraph[&#x27;qas&#x27;]:                question = qa[&#x27;question&#x27;]                if qa.get(&#x27;is_impossible&#x27;, False):                    continue                answer = qa[&#x27;answers&#x27;][0]                examples.append(Example(                    context=context,                    question=question,                    answer_text=answer[&#x27;text&#x27;],                    start_position=answer[&#x27;answer_start&#x27;],                    end_position=answer[&#x27;answer_start&#x27;] + len(answer[&#x27;text&#x27;])                ))        return examples\n\nTokenizationfrom transformers import AutoTokenizerclass MRCTokenizer:    def __init__(self, model_name: str, max_length: int = 384, doc_stride: int = 128):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        self.doc_stride = doc_stride        def encode(self, example: Example):        # Tokenize question and context        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation=&#x27;only_second&#x27;,            stride=self.doc_stride,            return_overflowing_tokens=True,            return_offsets_mapping=True,            padding=&#x27;max_length&#x27;,        )                # 找到答案在 token 序列中的位置        offset_mapping = encoding[&#x27;offset_mapping&#x27;][0]                start_token = None        end_token = None                for idx, (start, end) in enumerate(offset_mapping):            if start &lt;= example.start_position &lt; end:                start_token = idx            if start &lt; example.end_position &lt;= end:                end_token = idx                break                return &#123;            &#x27;input_ids&#x27;: encoding[&#x27;input_ids&#x27;][0],            &#x27;attention_mask&#x27;: encoding[&#x27;attention_mask&#x27;][0],            &#x27;start_position&#x27;: start_token or 0,            &#x27;end_position&#x27;: end_token or 0,        &#125;\n\n模型实现基于 BERT 的 MRC 模型import torchimport torch.nn as nnfrom transformers import AutoModelclass MRCModel(nn.Module):    def __init__(self, model_name: str, dropout: float = 0.1):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size                self.dropout = nn.Dropout(dropout)        self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        start_positions: Optional[torch.Tensor] = None,        end_positions: Optional[torch.Tensor] = None,    ):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)                # (batch, seq_len, 1) -&gt; (batch, seq_len)        start_logits = self.start_classifier(sequence_output).squeeze(-1)        end_logits = self.end_classifier(sequence_output).squeeze(-1)                # Mask padding tokens        start_logits = start_logits.masked_fill(~attention_mask.bool(), -1e9)        end_logits = end_logits.masked_fill(~attention_mask.bool(), -1e9)                loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss()            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return &#123;            &#x27;loss&#x27;: loss,            &#x27;start_logits&#x27;: start_logits,            &#x27;end_logits&#x27;: end_logits,        &#125;\n\n改进：联合 Start-End 预测class JointMRCModel(nn.Module):    &quot;&quot;&quot;联合预测 start 和 end，考虑 start-end 依赖&quot;&quot;&quot;        def __init__(self, model_name: str, max_answer_length: int = 30):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size        self.max_answer_length = max_answer_length                self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size * 2, 1)        def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        H = outputs.last_hidden_state  # (batch, seq_len, hidden)                # Start prediction        start_logits = self.start_classifier(H).squeeze(-1)                if self.training and start_positions is not None:            # 训练时使用真实的 start 位置            start_indices = start_positions.unsqueeze(-1).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)        else:            # 推理时使用预测的 start 位置            start_indices = start_logits.argmax(dim=-1, keepdim=True).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)                # End prediction conditioned on start        start_repr_expanded = start_repr.unsqueeze(1).expand(-1, H.size(1), -1)        end_input = torch.cat([H, start_repr_expanded], dim=-1)        end_logits = self.end_classifier(end_input).squeeze(-1)                # 只允许 end &gt;= start 且在 max_answer_length 范围内        # 这里简化处理，完整实现需要更复杂的 mask                return &#123;&#x27;start_logits&#x27;: start_logits, &#x27;end_logits&#x27;: end_logits&#125;\n\n训练流程from torch.utils.data import DataLoader, Datasetfrom transformers import get_linear_schedule_with_warmupfrom tqdm import tqdmdef train(model, train_dataloader, val_dataloader, epochs=3, lr=3e-5):    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)        total_steps = len(train_dataloader) * epochs    scheduler = get_linear_schedule_with_warmup(        optimizer,         num_warmup_steps=int(0.1 * total_steps),        num_training_steps=total_steps    )        device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)    model.to(device)        best_f1 = 0    for epoch in range(epochs):        model.train()        total_loss = 0                for batch in tqdm(train_dataloader, desc=f&#x27;Epoch &#123;epoch+1&#125;&#x27;):            batch = &#123;k: v.to(device) for k, v in batch.items()&#125;                        outputs = model(**batch)            loss = outputs[&#x27;loss&#x27;]                        loss.backward()            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                        optimizer.step()            scheduler.step()            optimizer.zero_grad()                        total_loss += loss.item()                avg_loss = total_loss / len(train_dataloader)        print(f&#x27;Epoch &#123;epoch+1&#125;, Loss: &#123;avg_loss:.4f&#125;&#x27;)                # Validation        f1 = evaluate(model, val_dataloader, device)        print(f&#x27;Validation F1: &#123;f1:.4f&#125;&#x27;)                if f1 &gt; best_f1:            best_f1 = f1            torch.save(model.state_dict(), &#x27;best_model.pt&#x27;)        return model\n\n评估与推理import reimport stringfrom collections import Counterdef normalize_answer(s):    &quot;&quot;&quot;标准化答案用于评估&quot;&quot;&quot;    def remove_articles(text):        return re.sub(r&#x27;\\b(a|an|the)\\b&#x27;, &#x27; &#x27;, text)        def white_space_fix(text):        return &#x27; &#x27;.join(text.split())        def remove_punc(text):        exclude = set(string.punctuation)        return &#x27;&#x27;.join(ch for ch in text if ch not in exclude)        def lower(text):        return text.lower()        return white_space_fix(remove_articles(remove_punc(lower(s))))def compute_f1(pred: str, gold: str) -&gt; float:    pred_tokens = normalize_answer(pred).split()    gold_tokens = normalize_answer(gold).split()        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        if num_same == 0:        return 0        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        return 2 * precision * recall / (precision + recall)def predict(model, tokenizer, context: str, question: str, device):    &quot;&quot;&quot;单条推理&quot;&quot;&quot;    model.eval()        encoding = tokenizer(        question, context,        max_length=384,        truncation=&#x27;only_second&#x27;,        return_tensors=&#x27;pt&#x27;    )        encoding = &#123;k: v.to(device) for k, v in encoding.items()&#125;        with torch.no_grad():        outputs = model(**encoding)        start_idx = outputs[&#x27;start_logits&#x27;].argmax().item()    end_idx = outputs[&#x27;end_logits&#x27;].argmax().item()        # 确保 end &gt;= start    if end_idx &lt; start_idx:        end_idx = start_idx        # 解码答案    answer_tokens = encoding[&#x27;input_ids&#x27;][0][start_idx:end_idx+1]    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)        return answer\n\n现代方法：使用 LLM对于更复杂的问答需求，可以使用 LLM：\nfrom openai import OpenAIdef llm_qa(context: str, question: str) -&gt; str:    client = OpenAI()        response = client.chat.completions.create(        model=&quot;gpt-4&quot;,        messages=[            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个问答助手。根据给定的上下文回答问题。如果答案不在上下文中，请说&#x27;无法回答&#x27;。&quot;&#125;,            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;上下文：&#123;context&#125;\\n\\n问题：&#123;question&#125;&quot;&#125;        ],        temperature=0    )        return response.choices[0].message.content\n\n延伸阅读\nSQuAD Dataset\nHugging Face QA Pipeline\nNatural Questions\n\n\n\n转载请注明出处\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"机器阅读理解：从传统方法到大语言模型","url":"/2019/10/03/%E5%BD%93%E6%88%91%E4%BB%AC%E6%8A%8A%E7%9B%AE%E5%85%89%E6%94%BE%E5%9C%A8%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","content":"\n核心问题：当我们期望机器”理解”文本时，我们的期望到底是什么？\n\n机器阅读理解的演进传统 MRC (2015-2019)基于 span extraction 的方法：\n输入: Context + Question输出: (start_idx, end_idx)\n\n代表模型：BiDAF, R-Net, QANet, BERT\nLLM 时代的 MRC (2020-至今)从”抽取”到”生成”的范式转变：\n输入: Context + Question + Instruction输出: 自由形式的答案\n\n任务分类与难度\n\n\n类型\n传统方法\nLLM 方法\n难度\n\n\n\n抽取式\n✅ 擅长\n✅ 擅长\n⭐\n\n\n多跳推理\n❌ 困难\n⚠️ 有限\n⭐⭐⭐\n\n\n数值推理\n❌ 几乎不能\n⚠️ 需要 CoT\n⭐⭐⭐⭐\n\n\n常识推理\n❌ 不能\n✅ 较好\n⭐⭐⭐\n\n\n开放生成\n❌ 不能\n✅ 擅长\n⭐⭐\n\n\n现代方法：RAG检索增强生成 (Retrieval-Augmented Generation) 结合了检索和生成的优势：\nclass RAGSystem:    def __init__(self, retriever, generator):        self.retriever = retriever  # e.g., Dense Retriever        self.generator = generator  # e.g., LLM        def answer(self, question: str) -&gt; str:        # 1. 检索相关文档        docs = self.retriever.retrieve(question, top_k=5)                # 2. 构建上下文        context = &quot;\\n\\n&quot;.join([d.text for d in docs])                # 3. 生成答案        prompt = f&quot;&quot;&quot;基于以下文档回答问题：&#123;context&#125;问题：&#123;question&#125;答案：&quot;&quot;&quot;                return self.generator.generate(prompt)\n\n检索器选择\n\n\n检索器\n特点\n适用场景\n\n\n\nBM25\n关键词匹配，快速\n短查询，精确匹配\n\n\nDense Retriever\n语义匹配\n语义相似查询\n\n\nColBERT\n延迟交互\n平衡效率与效果\n\n\nHybrid\n结合稀疏+稠密\n生产环境\n\n\nChain-of-Thought 推理对于需要推理的问题，CoT prompting 显著提升效果：\n# 标准 Promptingprompt_standard = &quot;Q: 小明有5个苹果，给了小红2个，还剩几个？\\nA:&quot;# Chain-of-Thought Prompting  prompt_cot = &quot;&quot;&quot;Q: 小明有5个苹果，给了小红2个，还剩几个？A: 让我们一步步思考：1. 小明最初有 5 个苹果2. 他给了小红 2 个苹果3. 剩余苹果数 = 5 - 2 = 3答案是 3 个苹果。&quot;&quot;&quot;\n\n评估指标传统指标$$\\text{F1} &#x3D; 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n$$\\text{EM (Exact Match)} &#x3D; \\mathbb{1}[\\text{pred} &#x3D; \\text{gold}]$$\nLLM 时代的指标# 使用 LLM 作为评估器def llm_evaluate(question, gold_answer, pred_answer):    prompt = f&quot;&quot;&quot;评估预测答案的质量（1-5分）：问题：&#123;question&#125;标准答案：&#123;gold_answer&#125;预测答案：&#123;pred_answer&#125;评分标准：5分 - 完全正确且信息完整4分 - 基本正确，略有遗漏3分 - 部分正确2分 - 有相关信息但不正确1分 - 完全错误分数：&quot;&quot;&quot;    return llm.generate(prompt)\n\n实践建议何时用传统 MRC\n答案明确在文档中\n需要精确的位置标注\n低延迟要求\n资源受限\n\n何时用 RAG + LLM\n需要整合多个文档\n答案需要推理或总结\n开放域问答\n用户期望自然语言回答\n\n代码示例：现代 RAG 系统from langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.chat_models import ChatOpenAIfrom langchain.chains import RetrievalQA# 初始化组件embeddings = OpenAIEmbeddings()vectorstore = FAISS.load_local(&quot;my_index&quot;, embeddings)llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0)# 创建 RAG 链qa_chain = RetrievalQA.from_chain_type(    llm=llm,    chain_type=&quot;stuff&quot;,  # 或 &quot;map_reduce&quot;, &quot;refine&quot;    retriever=vectorstore.as_retriever(search_kwargs=&#123;&quot;k&quot;: 5&#125;),    return_source_documents=True)# 使用result = qa_chain(&#123;&quot;query&quot;: &quot;什么是机器阅读理解？&quot;&#125;)print(result[&quot;result&quot;])\n\n延伸阅读\nSQuAD 2.0\nNatural Questions\nRAG Paper\nLangChain Documentation\n\n\n\n转载请注明出处\n\n","tags":["LLM","MRC","Deep learning","RAG"]},{"title":"开篇","url":"/2019/10/03/%E5%BC%80%E7%AF%87/","content":"各位读者朋友们大家好，我是 fooSynaptic。\n欢迎来到我的技术博客！这里记录我在 AI 和 NLP 领域的学习与思考。\n关于这个博客这个博客主要记录以下内容：\n\n自然语言处理 (NLP)：从传统方法到大语言模型\n机器学习：算法原理与实现细节\n深度学习：模型架构与训练技巧\n数学基础：线性代数、概率论、优化理论\n工程实践：Python、PyTorch、分布式训练\n\n技术栈NLP: Transformers, LLMs, RAG, Prompt EngineeringML: PyTorch, JAX, scikit-learnInfra: CUDA, Triton, vLLM, DeepSpeed\n\n关于我NLP Researcher，专注于：\n\n大语言模型 (LLM) 训练与推理优化\n检索增强生成 (RAG)\n机器阅读理解与问答系统\n\nGitHub: fooSynaptic\n\n\n欢迎交流讨论，转载请注明出处\n\n","tags":["Introduction"]},{"title":"条件随机场：原理与实现","url":"/2019/11/19/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0/","content":"条件随机场 (CRF) 是序列标注的经典模型，尽管深度学习时代 BERT 等模型大放异彩，CRF 层仍然在 NER、词性标注等任务中发挥关键作用。\n为什么需要 CRF？独立分类的问题如果对每个位置独立分类：\n$$\\hat{y}_i &#x3D; \\arg\\max_y P(y | x_i)$$\n会导致标签不一致，例如：\n输入: &quot;北 京 是 中 国 首 都&quot;错误: B-LOC I-PER O B-LOC I-LOC I-LOC I-LOC正确: B-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC\n\nCRF 的解决方案CRF 建模整个序列的联合概率，考虑标签之间的转移约束。\n数学原理条件概率$$P(Y|X) &#x3D; \\frac{1}{Z(X)} \\exp\\left(\\sum_{t&#x3D;1}^{T} \\left(\\phi(y_t, x, t) + \\psi(y_{t-1}, y_t)\\right)\\right)$$\n其中：\n\n$\\phi(y_t, x, t)$：发射分数（emission score）\n$\\psi(y_{t-1}, y_t)$：转移分数（transition score）\n$Z(X)$：配分函数（归一化项）\n\n配分函数$$Z(X) &#x3D; \\sum_{y \\in \\mathcal{Y}^T} \\exp\\left(\\sum_{t&#x3D;1}^{T} \\left(\\phi(y_t, x, t) + \\psi(y_{t-1}, y_t)\\right)\\right)$$\n直接计算复杂度为 $O(|\\mathcal{Y}|^T)$，使用前向算法可降至 $O(T \\cdot |\\mathcal{Y}|^2)$。\nPyTorch 实现CRF Layerimport torchimport torch.nn as nnclass CRF(nn.Module):    def __init__(self, num_tags, batch_first=True):        super().__init__()        self.num_tags = num_tags        self.batch_first = batch_first                # 转移矩阵: transitions[i, j] = 从标签 j 转移到标签 i 的分数        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))                # 起始和结束转移        self.start_transitions = nn.Parameter(torch.randn(num_tags))        self.end_transitions = nn.Parameter(torch.randn(num_tags))        def forward(self, emissions, tags, mask=None):        &quot;&quot;&quot;计算负对数似然损失&quot;&quot;&quot;        if mask is None:            mask = torch.ones_like(tags, dtype=torch.bool)                if self.batch_first:            emissions = emissions.transpose(0, 1)            tags = tags.transpose(0, 1)            mask = mask.transpose(0, 1)                # 计算分子（正确路径的分数）        numerator = self._compute_score(emissions, tags, mask)                # 计算分母（配分函数）        denominator = self._compute_normalizer(emissions, mask)                # 负对数似然        return (denominator - numerator).mean()        def _compute_score(self, emissions, tags, mask):        &quot;&quot;&quot;计算给定标签序列的分数&quot;&quot;&quot;        seq_len, batch_size = tags.shape                # 起始分数        score = self.start_transitions[tags[0]]        score += emissions[0, torch.arange(batch_size), tags[0]]                for i in range(1, seq_len):            # 转移分数 + 发射分数            score += self.transitions[tags[i], tags[i-1]] * mask[i]            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]                # 结束分数        last_tag_idx = mask.sum(dim=0) - 1        last_tags = tags.gather(0, last_tag_idx.unsqueeze(0)).squeeze(0)        score += self.end_transitions[last_tags]                return score        def _compute_normalizer(self, emissions, mask):        &quot;&quot;&quot;前向算法计算配分函数&quot;&quot;&quot;        seq_len, batch_size, num_tags = emissions.shape                # 初始化        score = self.start_transitions + emissions[0]                for i in range(1, seq_len):            # broadcast: (batch, num_tags, 1) + (num_tags, num_tags) + (batch, 1, num_tags)            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score = torch.logsumexp(next_score, dim=1)                        # 应用 mask            score = torch.where(mask[i].unsqueeze(1), next_score, score)                # 添加结束分数        score += self.end_transitions                return torch.logsumexp(score, dim=1)        def decode(self, emissions, mask=None):        &quot;&quot;&quot;Viterbi 解码&quot;&quot;&quot;        if mask is None:            mask = torch.ones(emissions.shape[:2], dtype=torch.bool, device=emissions.device)                if self.batch_first:            emissions = emissions.transpose(0, 1)            mask = mask.transpose(0, 1)                return self._viterbi_decode(emissions, mask)        def _viterbi_decode(self, emissions, mask):        &quot;&quot;&quot;Viterbi 算法&quot;&quot;&quot;        seq_len, batch_size, num_tags = emissions.shape                # 初始化        score = self.start_transitions + emissions[0]        history = []                for i in range(1, seq_len):            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score, indices = next_score.max(dim=1)                        score = torch.where(mask[i].unsqueeze(1), next_score, score)            history.append(indices)                # 添加结束分数        score += self.end_transitions                # 回溯        best_tags_list = []        _, best_last_tag = score.max(dim=1)                for idx in range(batch_size):            best_tags = [best_last_tag[idx].item()]            seq_length = int(mask[:, idx].sum().item())                        for hist in reversed(history[:seq_length-1]):                best_last_tag_idx = best_tags[-1]                best_tags.append(hist[idx, best_last_tag_idx].item())                        best_tags.reverse()            best_tags_list.append(best_tags)                return best_tags_list\n\n与 BiLSTM 结合class BiLSTM_CRF(nn.Module):    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):        super().__init__()        self.embedding = nn.Embedding(vocab_size, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2,                            num_layers=2, bidirectional=True, batch_first=True)        self.fc = nn.Linear(hidden_dim, num_tags)        self.crf = CRF(num_tags)        def forward(self, x, tags, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf(emissions, tags, mask)        def predict(self, x, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf.decode(emissions, mask)\n\n现代应用：BERT + CRF尽管 BERT 已经很强大，但 CRF 层仍能带来一致性提升：\nfrom transformers import BertModelclass BERT_CRF(nn.Module):    def __init__(self, bert_name, num_tags):        super().__init__()        self.bert = BertModel.from_pretrained(bert_name)        self.dropout = nn.Dropout(0.1)        self.fc = nn.Linear(self.bert.config.hidden_size, num_tags)        self.crf = CRF(num_tags)        def forward(self, input_ids, attention_mask, tags=None):        outputs = self.bert(input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)        emissions = self.fc(sequence_output)                if tags is not None:            return self.crf(emissions, tags, attention_mask.bool())        else:            return self.crf.decode(emissions, attention_mask.bool())\n\n性能对比（CoNLL-2003 NER）\n\n\n模型\nF1\n\n\n\nBiLSTM\n88.2\n\n\nBiLSTM + CRF\n90.1\n\n\nBERT\n92.4\n\n\nBERT + CRF\n92.8\n\n\nRoBERTa + CRF\n93.2\n\n\n训练技巧1. 标签平滑def label_smoothing_loss(crf, emissions, tags, mask, epsilon=0.1):    &quot;&quot;&quot;带标签平滑的 CRF 损失&quot;&quot;&quot;    nll_loss = crf(emissions, tags, mask)        # 均匀分布的损失    uniform_loss = -torch.logsumexp(emissions, dim=-1).mean()        return (1 - epsilon) * nll_loss + epsilon * uniform_loss\n\n2. 约束解码# 添加硬约束：B-X 后面只能接 I-X 或 Odef add_constraints(transitions, tag2idx):    for tag_from, idx_from in tag2idx.items():        for tag_to, idx_to in tag2idx.items():            if tag_from.startswith(&#x27;B-&#x27;) or tag_from.startswith(&#x27;I-&#x27;):                entity = tag_from[2:]                if tag_to.startswith(&#x27;I-&#x27;) and tag_to[2:] != entity:                    transitions.data[idx_to, idx_from] = -1e9\n\n延伸阅读\nLafferty et al., Conditional Random Fields (2001)\nHuang et al., Bidirectional LSTM-CRF Models for Sequence Tagging (2015)\npytorch-crf Documentation\n\n\n\n转载请注明出处\n\n","tags":["NLP","machine learning","CRF"]},{"title":"矩阵分解：从 SVD 到现代 AI 应用","url":"/2019/10/03/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B9%8B%E4%B8%80%EF%BC%9ATruncate-SVD-%E5%92%8Crandom-SVD/","content":"矩阵分解是机器学习的基石技术，从传统的推荐系统到现代大语言模型的参数高效微调（LoRA），都离不开矩阵分解的思想。\n奇异值分解 (SVD)基本形式任意矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 可以分解为：\n$$A &#x3D; U \\Sigma V^T$$\n其中：\n\n$U \\in \\mathbb{R}^{m \\times m}$：左奇异向量（正交矩阵）\n$\\Sigma \\in \\mathbb{R}^{m \\times n}$：奇异值对角矩阵\n$V \\in \\mathbb{R}^{n \\times n}$：右奇异向量（正交矩阵）\n\nTruncated SVD保留前 $r$ 个最大奇异值：\n$$A \\approx A_r &#x3D; U_r \\Sigma_r V_r^T$$\n这是最优的秩 $r$ 近似（Eckart-Young 定理）：\n$$A_r &#x3D; \\arg\\min_{\\text{rank}(B) &#x3D; r} |A - B|_F$$\nRandomized SVD当矩阵规模巨大时，精确 SVD 计算代价过高。Randomized SVD 提供了高效的近似方法。\n算法实现import numpy as npfrom scipy import linalgdef randomized_svd(A, n_components, n_oversamples=10, n_iter=4):    &quot;&quot;&quot;    Randomized SVD for large matrices.        Args:        A: Input matrix (m x n)        n_components: Number of singular values to compute        n_oversamples: Additional random vectors for accuracy        n_iter: Number of power iterations        Returns:        U, s, Vt: Truncated SVD components    &quot;&quot;&quot;    m, n = A.shape    n_random = n_components + n_oversamples        # Step 1: Random projection    Q = np.random.randn(n, n_random)        # Step 2: Power iteration for accuracy    for _ in range(n_iter):        Q, _ = linalg.lu(A @ Q, permute_l=True)        Q, _ = linalg.lu(A.T @ Q, permute_l=True)        Q, _ = linalg.qr(A @ Q, mode=&#x27;economic&#x27;)        # Step 3: Project and compute SVD    B = Q.T @ A    Uhat, s, Vt = linalg.svd(B, full_matrices=False)    U = Q @ Uhat        return U[:, :n_components], s[:n_components], Vt[:n_components, :]\n\n复杂度对比\n\n\n方法\n时间复杂度\n空间复杂度\n\n\n\n精确 SVD\n$O(mn \\cdot \\min(m,n))$\n$O(mn)$\n\n\nRandomized SVD\n$O(mn \\cdot r)$\n$O((m+n) \\cdot r)$\n\n\nTruncated SVD (Lanczos)\n$O(mn \\cdot r)$\n$O((m+n) \\cdot r)$\n\n\n现代应用：LoRALoRA (Low-Rank Adaptation) 是大语言模型参数高效微调的核心技术，直接利用了低秩分解的思想。\nLoRA 原理预训练权重 $W_0$ 固定，只训练低秩增量：\n$$W &#x3D; W_0 + \\Delta W &#x3D; W_0 + BA$$\n其中 $B \\in \\mathbb{R}^{d \\times r}$，$A \\in \\mathbb{R}^{r \\times k}$，$r \\ll \\min(d, k)$。\n实现示例import torchimport torch.nn as nnclass LoRALayer(nn.Module):    def __init__(self, in_features, out_features, rank=4, alpha=1.0):        super().__init__()        self.rank = rank        self.alpha = alpha                # 原始权重（冻结）        self.W = nn.Linear(in_features, out_features, bias=False)        self.W.weight.requires_grad = False                # 低秩分解        self.A = nn.Linear(in_features, rank, bias=False)        self.B = nn.Linear(rank, out_features, bias=False)                # 初始化        nn.init.kaiming_uniform_(self.A.weight)        nn.init.zeros_(self.B.weight)                self.scaling = alpha / rank        def forward(self, x):        # W(x) + scaling * B(A(x))        return self.W(x) + self.scaling * self.B(self.A(x))\n\n参数效率对于 LLaMA-7B：\n\n\n\n方法\n可训练参数\n显存占用\n\n\n\n全量微调\n7B (100%)\n~140GB\n\n\nLoRA (r&#x3D;8)\n4.7M (0.07%)\n~14GB\n\n\nLoRA (r&#x3D;16)\n9.4M (0.13%)\n~16GB\n\n\n其他应用1. 推荐系统矩阵分解用于协同过滤：\n$$R \\approx U V^T$$\n# 使用 surprise 库from surprise import SVD, Dataset, Readerreader = Reader(rating_scale=(1, 5))data = Dataset.load_from_df(df[[&#x27;user&#x27;, &#x27;item&#x27;, &#x27;rating&#x27;]], reader)model = SVD(n_factors=100)model.fit(trainset)\n\n2. 文本表示 (LSA)潜在语义分析：\nfrom sklearn.decomposition import TruncatedSVDfrom sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(max_features=10000)X = vectorizer.fit_transform(documents)svd = TruncatedSVD(n_components=100)X_reduced = svd.fit_transform(X)\n\n3. 图像压缩from PIL import Imageimport numpy as npdef compress_image(image_path, n_components=50):    img = np.array(Image.open(image_path).convert(&#x27;L&#x27;))    U, s, Vt = np.linalg.svd(img, full_matrices=False)        # 保留前 n_components 个奇异值    compressed = U[:, :n_components] @ np.diag(s[:n_components]) @ Vt[:n_components, :]        return compressed.astype(np.uint8)\n\n数值稳定性条件数$$\\kappa(A) &#x3D; \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}$$\n条件数过大会导致数值不稳定。\n正则化 SVDdef regularized_svd(A, lambda_reg=0.01):    &quot;&quot;&quot;Add regularization for numerical stability.&quot;&quot;&quot;    U, s, Vt = np.linalg.svd(A, full_matrices=False)    s_reg = s / (s**2 + lambda_reg)    return U, s_reg, Vt\n\n延伸阅读\nHalko et al., Finding Structure with Randomness (2011)\nHu et al., LoRA: Low-Rank Adaptation of Large Language Models (2021)\nNumPy SVD Documentation\n\n\n\n转载请注明出处\n\n","tags":["machine learning","LLM","linear algebra"]},{"title":"BiDAF 论文解读：双向注意力流机制","url":"/2019/11/19/%E8%AE%BA%E6%96%87%E6%A2%97%E6%A6%82%EF%BC%9ABi-Directional-Attention-Flow-for-Machine-Comprehension/","content":"BiDAF (Bi-Directional Attention Flow) 是机器阅读理解领域的经典模型，其双向注意力机制对后续 Transformer 架构产生了深远影响。\n核心创新1. Memory-less Attention传统动态注意力 vs BiDAF 的无记忆注意力：\n\n\n\n特性\nDynamic Attention\nMemory-less Attention\n\n\n\n依赖\n前一时间步的 attended vector\n仅当前 query 和 context\n\n\n优势\n可建模时序依赖\n避免错误累积\n\n\n缺点\n错误会传播\n无法建模长程依赖\n\n\n2. 双向注意力同时计算：\n\nContext-to-Query (C2Q)：每个 context 词最相关的 query 词\nQuery-to-Context (Q2C)：对回答问题最关键的 context 词\n\n模型架构Input → Embedding → Encoding → Attention → Modeling → Output  │         │           │          │           │         │ 词向量    字符CNN     BiLSTM    双向注意力   BiLSTM   Span预测\n\n数学表达相似度矩阵：\n$$S_{ij} &#x3D; \\alpha(H_i, U_j) &#x3D; w^T[H_i; U_j; H_i \\odot U_j]$$\n其中 $H \\in \\mathbb{R}^{T \\times d}$ 是 context 表示，$U \\in \\mathbb{R}^{J \\times d}$ 是 query 表示。\nC2Q Attention：\n$$\\tilde{U}i &#x3D; \\sum_j a{ij} U_j, \\quad a_i &#x3D; \\text{softmax}(S_i)$$\nQ2C Attention：\n$$\\tilde{H} &#x3D; \\sum_i b_i H_i, \\quad b &#x3D; \\text{softmax}(\\max_j S_{:j})$$\n融合表示：\n$$G_i &#x3D; [H_i; \\tilde{U}_i; H_i \\odot \\tilde{U}_i; H_i \\odot \\tilde{H}]$$\nPyTorch 实现import torchimport torch.nn as nnclass BiDAFAttention(nn.Module):    def __init__(self, hidden_size):        super().__init__()        self.W = nn.Linear(hidden_size * 3, 1, bias=False)        def forward(self, context, query, c_mask, q_mask):        &quot;&quot;&quot;        Args:            context: (batch, c_len, hidden)            query: (batch, q_len, hidden)            c_mask: (batch, c_len)            q_mask: (batch, q_len)        &quot;&quot;&quot;        batch, c_len, hidden = context.size()        q_len = query.size(1)                # 扩展维度以计算所有 (i, j) 对        c_expand = context.unsqueeze(2).expand(-1, -1, q_len, -1)        q_expand = query.unsqueeze(1).expand(-1, c_len, -1, -1)                # 计算相似度矩阵 S        cq = torch.cat([c_expand, q_expand, c_expand * q_expand], dim=-1)        S = self.W(cq).squeeze(-1)  # (batch, c_len, q_len)                # Mask        q_mask_expand = q_mask.unsqueeze(1).expand(-1, c_len, -1)        S = S.masked_fill(~q_mask_expand, -1e9)                # C2Q attention        a = torch.softmax(S, dim=-1)        c2q = torch.bmm(a, query)  # (batch, c_len, hidden)                # Q2C attention        b = torch.softmax(S.max(dim=-1)[0], dim=-1)        q2c = torch.bmm(b.unsqueeze(1), context)  # (batch, 1, hidden)        q2c = q2c.expand(-1, c_len, -1)                # 融合        G = torch.cat([context, c2q, context * c2q, context * q2c], dim=-1)                return G\n\n与 Transformer 的对比\n\n\n特性\nBiDAF\nTransformer\n\n\n\n注意力方向\n双向（C2Q, Q2C）\n全方向自注意力\n\n\n位置编码\nBiLSTM 隐式编码\n显式位置编码\n\n\n并行化\n受限于 RNN\n完全并行\n\n\n长距离依赖\n受限\n理论上无限\n\n\n参数量\n较少\n较多\n\n\n现代演进BiDAF 的思想在现代模型中的体现：\n1. Cross-Attention in Transformerclass CrossAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.mha = nn.MultiheadAttention(d_model, n_heads)        def forward(self, query, key_value):        # query 来自一个序列，key/value 来自另一个序列        return self.mha(query, key_value, key_value)\n\n2. FiD (Fusion-in-Decoder)用于 RAG 的架构，类似 BiDAF 的融合思想：\nclass FiD(nn.Module):    def __init__(self, encoder, decoder):        super().__init__()        self.encoder = encoder        self.decoder = decoder        def forward(self, question, passages):        # 独立编码每个 passage        encoded = []        for passage in passages:            enc = self.encoder(question + passage)            encoded.append(enc)                # 融合解码        fused = torch.cat(encoded, dim=1)        return self.decoder(fused)\n\n实验结果（原论文）在 SQuAD 1.1 上的表现：\n\n\n\n模型\nEM\nF1\n\n\n\nBiDAF\n67.7\n77.3\n\n\nBiDAF + Self Attention\n72.1\n81.1\n\n\nBERT-base\n80.8\n88.5\n\n\nGPT-4 (few-shot)\n~90\n~95\n\n\n延伸阅读\nBiDAF Paper\nAttention Is All You Need\nBERT for QA\n\n\n\n转载请注明出处\n\n","tags":["MRC","attention","deep learning"]}]