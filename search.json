[{"title":"DDIA Part 1：数据系统基础","url":"/2025/12/28/DDIA-Part1-%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/","content":"本文是 DDIA 第一部分的读书笔记，涵盖第 1-4 章：可靠性与可扩展性、数据模型、存储引擎、数据编码。\n\n第1章：可靠性、可扩展性与可维护性数据密集型应用的组成现代数据密集型应用通常由多个组件组合：\n┌─────────────────────────────────────────┐│         数据密集型应用架构               │├─────────────────────────────────────────┤│  数据库 → 缓存 → 搜索索引               ││  流处理 → 批处理 → 消息队列             │└─────────────────────────────────────────┘\n\n可靠性 (Reliability)\n系统在面对故障时仍能正确运行\n\n故障类型与应对：\n\n\n\n故障类型\n应对策略\n\n\n\n硬件故障\nRAID、双电源、多副本\n\n\n软件错误\n测试、隔离、监控、快速重启\n\n\n人为错误\n沙箱环境、灰度发布、快速回滚\n\n\n可扩展性 (Scalability)\n系统应对负载增长的能力\n\n性能指标：使用百分位数而非平均值\n\n\n\n百分位\n含义\n\n\n\np50\n中位数，典型响应时间\n\n\np95\n95%请求快于此值\n\n\np99\n常用于 SLA 标准\n\n\n扩展策略：\n\n纵向扩展：使用更强大的机器\n横向扩展：使用多台普通机器\n弹性扩展：根据负载自动增减资源\n\n可维护性 (Maintainability)\n\n\n方面\n目标\n\n\n\n可操作性\n运维团队能轻松保持系统运行\n\n\n简单性\n新工程师能快速理解系统\n\n\n可演化性\n能轻松修改和扩展系统\n\n\n\n第2章：数据模型与查询语言三种数据模型对比\n\n\n模型\n特点\n适用场景\n\n\n\n关系模型\n结构化、规范化、SQL\n事务处理、复杂查询\n\n\n文档模型\n灵活模式、嵌套结构\n树状数据、快速迭代\n\n\n图模型\n多对多关系\n社交网络、知识图谱\n\n\n关系模型-- 规范化设计：使用外键CREATE TABLE users (    user_id INT PRIMARY KEY,    name VARCHAR(100),    position_id INT REFERENCES positions(position_id));\n\n优势：数据一致性、灵活查询、事务支持局限：对象-关系阻抗不匹配、模式僵化\n文档模型{  \"user_id\": 1,  \"name\": \"张三\",  \"positions\": [    {\"title\": \"工程师\", \"company\": \"ABC公司\"},    {\"title\": \"技术总监\", \"company\": \"XYZ公司\"}  ]}\n\nSchema-on-read vs Schema-on-write：\n\n关系数据库：写入时验证模式（静态类型）\n文档数据库：读取时解释结构（动态类型）\n\n图模型// Neo4j Cypher 查询MATCH (alice:Person {name: 'Alice'})-[:FOLLOWS]-&gt;()-[:FOLLOWS]-&gt;(fof)RETURN fof.name\n\n声明式 vs 命令式\n\n\n类型\n特点\n示例\n\n\n\n声明式\n描述想要什么结果\nSQL, Cypher\n\n\n命令式\n描述如何得到结果\n编程语言循环\n\n\n\n第3章：存储与检索两大存储引擎家族\n\n\n类型\n优化目标\n代表产品\n\n\n\n日志结构 (LSM)\n写入优化\nRocksDB, Cassandra\n\n\n原地更新 (B-Tree)\n读取优化\nMySQL, PostgreSQL\n\n\nLSM-Tree 结构Level 0 (内存):┌────────────┐│  Memtable  │ ← 当前写入（平衡树）└────────────┘      ↓ 达到阈值，写入磁盘Level 1-N (磁盘):┌────┐ ┌────┐ ┌────┐│SS1 │ │SS2 │ │SS3 │ ← SSTable（排序键）└────┘ └────┘ └────┘\n\n读取流程：Memtable → 最新 SSTable → … → 最老 SSTable\n优化技术：\n\n布隆过滤器：快速判断键是否存在\n压缩策略：Size-Tiered（写密集）、Leveled（读密集）\n\nB-Tree 结构          ┌───────────────┐          │    [30, 70]   │ ← 根节点          └───────────────┘         /       │        \\┌───────┐  ┌───────────┐  ┌───────┐│[10,20]│  │[40,50,60] │  │[80,90]│ ← 叶子节点└───────┘  └───────────┘  └───────┘\n\nWAL (预写日志)：先写日志，再更新数据，保证崩溃恢复\nB-Tree vs LSM-Tree\n\n\n特性\nB-Tree\nLSM-Tree\n\n\n\n写入\n原地更新\n追加写入\n\n\n读取\n快（一次定位）\n可能检查多个文件\n\n\n写放大\n较低\n较高（压缩开销）\n\n\n空间利用\n可能碎片化\n更紧凑\n\n\nOLTP vs OLAP\n\n\n特性\nOLTP\nOLAP\n\n\n\n操作\n增删改查\n复杂查询、聚合\n\n\n数据量\nGB~TB\nTB~PB\n\n\n用户\n应用程序\n分析师\n\n\n存储\n行存储\n列存储\n\n\n列存储优势：只读取需要的列、压缩友好、向量化处理\n\n第4章：数据编码与演化兼容性概念时间线: v1 ──&gt; v2 ──&gt; v3 ──&gt; v4后向兼容：新代码能读取旧数据 (v3 读 v1 数据 ✓)前向兼容：旧代码能读取新数据 (v1 读 v3 数据 ✓)\n\n滚动升级：新旧版本代码同时运行，需要双向兼容\n编码格式对比\n\n\n格式\n可读性\n空间效率\n模式演化\n\n\n\nJSON\n高\n低\n手动\n\n\nProtobuf\n无\n高\n支持\n\n\nThrift\n无\n高\n支持\n\n\nAvro\n无\n最高\n支持\n\n\nProtocol Buffersmessage Person {  required string user_name = 1;  optional int64 favorite_number = 2;  repeated string interests = 3;}\n\n关键规则：字段名可改，字段标签（数字）不能改\n兼容性规则：\n\n\n\n操作\n后向兼容\n前向兼容\n\n\n\n添加可选字段\n✓\n✓\n\n\n删除可选字段\n✓\n✓\n\n\n添加必填字段\n✗\n✗\n\n\nAvro特点：读写模式分离，不存储字段标签，更紧凑\n{  \"type\": \"record\",  \"name\": \"Person\",  \"fields\": [    {\"name\": \"userName\", \"type\": \"string\"},    {\"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null}  ]}\n\n数据流模式\n\n\n模式\n场景\n兼容性考虑\n\n\n\n数据库\n持久存储\n数据可能比代码更持久\n\n\n服务调用\nREST/RPC\nAPI 版本控制\n\n\n消息传递\n队列/Actor\n生产者消费者解耦\n\n\n\n本部分要点总结\n可靠性、可扩展性、可维护性是优秀系统的三大支柱\n数据模型选择取决于数据结构和查询需求\nLSM-Tree 优化写入，B-Tree 优化读取\n列存储适合 OLAP，行存储适合 OLTP\n二进制编码比 JSON 更紧凑高效\n模式演化需要保证前向和后向兼容\n\n\n返回总览 | 下一部分：分布式数据\n","categories":["读书笔记"],"tags":["DDIA","数据库","存储引擎","数据模型"]},{"title":"DDIA Part 2：分布式数据","url":"/2025/12/28/DDIA-Part2-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE/","content":"本文是 DDIA 第二部分的读书笔记，涵盖第 5-9 章：数据复制、分区、事务、分布式挑战、一致性与共识。\n\n第5章：数据复制复制的目的\n\n\n目的\n说明\n\n\n\n高可用性\n部分节点故障时系统仍可用\n\n\n降低延迟\n数据放在离用户更近的地方\n\n\n提高读吞吐\n多个副本并行处理读请求\n\n\n主从复制客户端 ──写入──&gt; 主节点 ──复制日志──&gt; 从节点1                              └──&gt; 从节点2\n\n同步 vs 异步复制：\n\n\n\n方式\n优点\n缺点\n\n\n\n同步\n数据一致\n延迟高，可用性差\n\n\n异步\n延迟低\n可能数据丢失\n\n\n半同步\n平衡\n实现复杂\n\n\n复制延迟问题\n\n\n问题\n说明\n解决方案\n\n\n\n读己之写\n写后读可能读到旧数据\n修改的数据从主节点读\n\n\n单调读\n刷新后可能看到更旧的数据\n每个用户固定副本\n\n\n一致前缀读\n因果关系被打乱\n相关写入同一分区\n\n\n多主复制适用场景：多数据中心、离线客户端、协作编辑\n写冲突解决：\n\n\n\n策略\n说明\n\n\n\n最后写入胜出 (LWW)\n时间戳最新的覆盖\n\n\n合并值\n如拼接 “A/B”\n\n\nCRDT\n特殊数据结构自动合并\n\n\n提示用户\n类似 Git 冲突\n\n\n无主复制 (Dynamo 风格)Quorum 公式：\nN=3, W=2, R=2:写入需要 2 个副本确认读取需要查询 2 个副本至少 1 个副本既写入又读取，保证读到最新值\n\n\n第6章：数据分区分区策略\n\n\n策略\n优点\n缺点\n\n\n\n按键范围\n支持范围查询\n可能热点\n\n\n按键哈希\n分布均匀\n失去范围查询\n\n\n一致性哈希：添加/删除节点只影响相邻区间\n热点问题明星发微博 → 所有请求发往同一分区\n\n解决方案：拆分热键（添加随机后缀）、本地缓存、限流\n二级索引分区\n\n\n类型\n特点\n\n\n\n本地索引\n每分区维护自己的索引，查询需 scatter/gather\n\n\n全局索引\n索引本身分区，读快写慢（异步更新）\n\n\n再平衡策略\n\n\n策略\n说明\n产品示例\n\n\n\n固定分区数\n预创建大量分区\nElasticsearch\n\n\n动态分区\n自动拆分合并\nHBase\n\n\n按节点比例\n每节点固定分区数\nCassandra\n\n\n请求路由方案1: 客户端直连任意节点 → 转发方案2: 路由层（知道分区映射）方案3: 客户端感知分区（如使用 ZooKeeper）\n\n\n第7章：事务ACID 特性\n\n\n特性\n含义\n\n\n\n原子性\n全部成功或全部失败\n\n\n一致性\n从有效状态到有效状态\n\n\n隔离性\n并发事务互不干扰\n\n\n持久性\n提交后数据不丢失\n\n\n隔离级别\n\n\n级别\n防止问题\n\n\n\n读已提交\n脏读、脏写\n\n\n快照隔离\n不可重复读\n\n\n串行化\n所有并发异常\n\n\n丢失更新问题事务1: 读取 counter=10     写入 counter=11事务2:      读取 counter=10        写入 counter=11期望: 12，实际: 11（事务1的更新丢失）\n\n解决：原子操作、显式锁定、Compare-and-Set\n写偏斜医院规则：至少1名医生值班Alice 查询：2人值班，取消自己Bob 同时：2人值班，取消自己结果：无人值班！\n\n解决：串行化隔离、物化冲突、显式锁定\n串行化实现\n\n\n方式\n特点\n产品\n\n\n\n实际串行\n单线程执行\nRedis, VoltDB\n\n\n两阶段锁 (2PL)\n读写互斥\n传统数据库\n\n\n串行化快照隔离 (SSI)\n乐观并发\nPostgreSQL 9.1+\n\n\n分布式事务 (2PC)阶段1: 协调者 ──准备?──&gt; 所有参与者阶段2: 协调者 ──提交/回滚──&gt; 所有参与者问题：协调者故障时参与者阻塞\n\n\n第8章：分布式系统的挑战部分失效\n\n\n单机系统\n分布式系统\n\n\n\n要么工作要么不工作\n部分可能工作\n\n\n故障通常完全\n故障通常部分\n\n\n不可靠的网络请求可能：丢失、延迟、重复响应可能：丢失、延迟无法区分：网络故障 vs 节点故障\n\n超时困境：太短误判正常节点，太长恢复慢\n不可靠的时钟\n\n\n时钟类型\n用途\n\n\n\n日历时钟\n当前时间（可能跳跃）\n\n\n单调时钟\n测量持续时间（保证递增）\n\n\nLWW 的风险：时钟不同步导致新数据被旧数据覆盖\nFencing Token节点A 获取锁 token=33 → 暂停节点B 获取锁 token=34 → 写入成功节点A 恢复 token=33 → 被拒绝（33 &lt; 34）\n\n系统模型\n\n\n时序假设\n节点故障假设\n\n\n\n同步/部分同步/异步\n崩溃-停止/崩溃-恢复/拜占庭\n\n\n正确性：安全性（坏事不发生） + 活性（好事最终发生）\n\n第9章：一致性与共识一致性模型弱 ←─────────────────────────────────→ 强最终一致性    因果一致性    顺序一致性    线性一致性\n\n线性一致性\n系统表现得好像只有一个数据副本，所有操作都是原子的\n\n应用：分布式锁、领导者选举、唯一性约束\nCAP 定理网络分区时，一致性与可用性不可兼得\n\n\n\n选择\n含义\n\n\n\nCP\n保证一致性，牺牲可用性\n\n\nAP\n保证可用性，牺牲一致性\n\n\n共识问题\n多个节点就某个值达成一致\n\n性质：一致同意、完整性、终止性、有效性\nFLP 不可能定理：异步系统中存在故障节点时，不存在总能达成共识的算法\nPaxos 算法阶段1 (Prepare):提议者 ──Prepare(n)──&gt; 接受者        &lt;──Promise──阶段2 (Accept):提议者 ──Accept(n,v)──&gt; 接受者        &lt;──Accepted──\n\nRaft 算法比 Paxos 更易理解\n角色：领导者、跟随者、候选人任期：逻辑时钟，每次选举递增选举流程：1. 跟随者超时 → 变候选人2. 请求投票 → 获多数票3. 成为领导者 → 复制日志\n\n共识的应用\n\n\n产品\n用途\n\n\n\nZooKeeper\n协调服务、配置管理\n\n\netcd\nKubernetes 状态存储\n\n\nConsul\n服务发现\n\n\n\n本部分要点总结\n复制的核心挑战是处理数据变更\n分区策略需要权衡范围查询和负载均衡\n事务隔离级别是正确性和性能的权衡\n分布式系统的核心挑战是部分失效\n线性一致性是最强保证，但代价高昂\nRaft 比 Paxos 更易理解，适合学习\n\n\n上一部分：数据系统基础 | 返回总览 | 下一部分：衍生数据\n","categories":["读书笔记"],"tags":["DDIA","分布式系统","一致性","共识算法"]},{"title":"DDIA Part 3：衍生数据","url":"/2025/12/28/DDIA-Part3-%E8%A1%8D%E7%94%9F%E6%95%B0%E6%8D%AE/","content":"本文是 DDIA 第三部分的读书笔记，涵盖第 10-12 章：批处理、流处理、数据系统的未来。\n\n第10章：批处理三种系统类型\n\n\n类型\n特点\n延迟\n示例\n\n\n\n在线服务\n请求-响应\n毫秒\nWeb API\n\n\n批处理\n大量数据，高吞吐\n分钟~小时\nMapReduce\n\n\n流处理\n持续处理数据流\n毫秒~秒\nKafka Streams\n\n\nUnix 哲学\n每个程序做好一件事，输出可成为另一程序的输入\n\ncat access.log |   awk '{print $7}' |    # 提取 URL  sort | uniq -c |      # 计数  sort -rn | head -10   # 取前10\n\nMapReduce输入 → Map → Shuffle(按键分组) → Reduce → 输出示例（词频统计）:输入: \"hello world hello\"Map:  (hello,1) (world,1) (hello,1)Shuffle: hello:[1,1], world:[1]Reduce: (hello,2) (world,1)\n\n分布式执行：数据本地性原则，尽量在数据所在节点执行 Map\nJoin 类型\n\n\n类型\n说明\n适用场景\n\n\n\nReduce 端 Join\nShuffle 后按键合并\n通用\n\n\n广播 Join\n小表广播到所有 Map\n大表 Join 小表\n\n\n分区 Join\n两表相同分区策略\n预分区数据\n\n\n现代批处理框架\n\n\n框架\n特点\n\n\n\nSpark\nRDD 抽象，内存缓存，迭代友好\n\n\nFlink\n流批一体，增量处理\n\n\nvs MapReduce：\n\n\n\n方面\nMapReduce\n数据流引擎\n\n\n\n编程模型\nMap/Reduce\n任意 DAG\n\n\n中间数据\n写入 HDFS\n内存流转\n\n\n迭代支持\n效率低\n高效\n\n\n\n第11章：流处理批处理 vs 流处理\n\n\n方面\n批处理\n流处理\n\n\n\n数据\n有界，静态\n无界，持续到达\n\n\n延迟\n分钟~小时\n毫秒~秒\n\n\n结果\n一次性输出\n持续更新\n\n\n消息系统传统消息队列 (RabbitMQ)：\n\n消息处理后删除\n每条消息只被一个消费者处理\n\n日志型消息系统 (Kafka)：\n\n消息持久化，可重放\n分区保序\n多消费者组独立消费\n\n分区0: [msg0][msg3][msg6] → 消费者A分区1: [msg1][msg4][msg7] → 消费者B分区2: [msg2][msg5][msg8] → 消费者C\n\n变更数据捕获 (CDC)应用 → 数据库 → CDC工具(Debezium) → Kafka → 派生系统\n\n应用：同步搜索索引、微服务集成、实时 ETL\n事件溯源传统方式：只存当前状态┌ balance = 1000 ┐事件溯源：存储所有事件│ 1. 创建账户 balance=0    ││ 2. 存款 +500             ││ 3. 存款 +800             ││ 4. 取款 -300             │└ 当前: 0+500+800-300=1000 ┘\n\n优势：完整审计、可重放、调试友好\n时间语义\n\n\n类型\n定义\n\n\n\n事件时间\n事件实际发生的时间\n\n\n处理时间\n事件到达处理器的时间\n\n\n窗口类型滚动窗口：┌──┐┌──┐┌──┐ (固定大小，不重叠)滑动窗口：┌────┐            ┌────┐              ┌────┐  (固定大小，可重叠)会话窗口：┌──┐   ┌─────┐  ┌─┐ (基于活动间隙)\n\n水位线 (Watermark)\n不会再有 ≤ 该时间的事件到达\n\n迟到事件处理：丢弃、更新结果、侧输出\n流处理框架\n\n\n框架\n特点\n\n\n\nKafka Streams\n轻量级库，与 Kafka 紧密集成\n\n\nFlink\n真正的流处理，强一致性\n\n\nSpark Streaming\n微批处理\n\n\n流表对偶流 → 表：聚合/累积（物化视图）表 → 流：捕获变更（变更日志）\n\n-- KSQL: 流转表CREATE TABLE page_counts AS  SELECT page_id, COUNT(*) as views  FROM pageviews  GROUP BY page_id;\n\n\n第12章：数据系统的未来数据集成现状：多种专用工具（PostgreSQL + Redis + ES + Kafka）\n以日志为中心的架构：\n写入 → 事件日志(Kafka) → 数据库              ↓        → 搜索索引                       → 缓存\n\nLambda vs Kappa：\n\n\n\n架构\n特点\n\n\n\nLambda\n批处理+流处理双路径，需维护两套代码\n\n\nKappa\n只用流处理，日志保留足够长支持重放\n\n\n分拆数据库传统数据库 = 存储 + 事务 + 索引 + 复制 + 查询优化分拆后：Kafka(日志) → Flink(处理) → RocksDB(存储)每个组件专注一件事\n\n主数据 vs 衍生数据：\n\n\n\n类型\n说明\n\n\n\n主数据\n真相来源，写入时的输入\n\n\n衍生数据\n从主数据计算，可重建（索引、缓存）\n\n\n端到端正确性幂等性：执行多次与执行一次效果相同\n幂等：SET x = 5非幂等：INCR x（需要去重）\n\n端到端论证：只在每层保证正确不够，需在最终用户层面验证\n审计与可追溯不可变日志的优势：\n传统：UPDATE email='new@...'（历史丢失）事件日志：  1. 注册 email='old@...'  2. 更新 email='new@...'（完整历史）\n\n伦理考量\n\n\n问题\n考量\n\n\n\n隐私\n收集什么数据？用于什么？\n\n\n偏见\n算法是否存在歧视？\n\n\n透明度\n用户能否理解决策过程？\n\n\n问责\n出错时谁负责？\n\n\n设计原则\n\n\n原则\n说明\n\n\n\n简单性\n避免不必要的复杂性\n\n\n可组合性\n使用可组合的组件\n\n\n可观测性\n便于理解系统行为\n\n\n可演化性\n能适应变化的需求\n\n\n未来趋势1. 流批一体：批处理 = 有界流2. 声明式数据管理：描述想要什么3. 自动化运维：自动调优、扩缩容4. 边缘计算：数据处理靠近产生地\n\n\n全书总结核心主题\n\n\n主题\n章节\n要点\n\n\n\n数据存储\n2, 3, 4\n选择合适的数据模型和存储引擎\n\n\n分布式\n5-9\n理解分布式系统的权衡\n\n\n数据处理\n10, 11\n批处理与流处理的统一\n\n\n系统设计\n1, 12\n可靠、可扩展、可维护\n\n\n关键要点\n没有银弹：不同工具适合不同场景\n日志是关键：事件日志是数据集成的基础\n衍生数据可重建：主数据是真相来源\n端到端正确性：只在某一层保证不够\n技术选择有社会影响：需要考虑伦理问题\n简单性是美德：避免不必要的复杂性\n\n\n推荐阅读\n\n\n书籍\n主题\n\n\n\n《Database Internals》\n数据库内部原理\n\n\n《Streaming Systems》\n流处理深入\n\n\n《Building Microservices》\n微服务架构\n\n\n《Clean Architecture》\n软件架构\n\n\n\n上一部分：分布式数据 | 返回总览\n","categories":["读书笔记"],"tags":["DDIA","批处理","流处理","数据工程"]},{"title":"DDIA 读书笔记：数据密集型应用系统设计","url":"/2025/12/28/DDIA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%80%BB%E8%A7%88/","content":"\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems\n\n这是一份关于《数据密集型应用系统设计》(DDIA) 的完整读书笔记，本书被誉为”数据系统领域的圣经”。\n书籍信息\n\n\n项目\n内容\n\n\n\n书名\nDesigning Data-Intensive Applications (DDIA)\n\n\n中文名\n数据密集型应用系统设计\n\n\n作者\nMartin Kleppmann（剑桥大学分布式系统研究员）\n\n\n出版时间\n2017年3月\n\n\n核心主题本书围绕三个核心概念展开：\n\n可靠性 (Reliability)：系统在遇到故障时仍能正确工作\n可扩展性 (Scalability)：系统能够应对负载增长\n可维护性 (Maintainability)：系统易于理解、修改和扩展\n\n全书结构第一部分：数据系统基础\n查看详细笔记\n\n\n\n\n章节\n核心内容\n\n\n\n第1章\n可靠性、可扩展性、可维护性的定义与实践\n\n\n第2章\n关系模型、文档模型、图模型的对比与选择\n\n\n第3章\n存储引擎原理：B-Tree、LSM-Tree、OLTP vs OLAP\n\n\n第4章\n数据编码格式与模式演化：JSON、Protobuf、Avro\n\n\n第二部分：分布式数据\n查看详细笔记\n\n\n\n\n章节\n核心内容\n\n\n\n第5章\n数据复制：主从、多主、无主复制策略\n\n\n第6章\n数据分区：分区策略、再平衡、请求路由\n\n\n第7章\n事务：ACID、隔离级别、分布式事务\n\n\n第8章\n分布式系统挑战：网络、时钟、故障模型\n\n\n第9章\n一致性与共识：CAP、Paxos、Raft\n\n\n第三部分：衍生数据\n查看详细笔记\n\n\n\n\n章节\n核心内容\n\n\n\n第10章\n批处理：MapReduce、Spark、数据流引擎\n\n\n第11章\n流处理：Kafka、Flink、事件时间与水位线\n\n\n第12章\n数据系统未来：数据集成、端到端正确性、伦理\n\n\n学习路线入门路线（适合初学者）第1章 → 第2章 → 第3章 → 第4章（建立基础）    ↓第5章 → 第6章（理解分布式基础）    ↓第10章 → 第11章（了解数据处理）\n\n进阶路线（适合有经验的开发者）第7章 → 第8章 → 第9章（深入分布式）    ↓第12章（展望未来）    ↓回顾第1-4章填补知识空白\n\n专题路线\n\n\n方向\n推荐阅读顺序\n\n\n\n数据库\n2 → 3 → 5 → 6 → 7\n\n\n分布式系统\n5 → 6 → 8 → 9\n\n\n数据工程\n3 → 10 → 11 → 12\n\n\n核心要点速览数据模型选择关系模型 ──── 结构化数据、复杂查询、事务支持     ↓文档模型 ──── 灵活模式、树状结构、局部性好     ↓图模型 ───── 复杂关系、社交网络、知识图谱\n\n存储引擎对比\n\n\n引擎\n优化目标\n典型应用\n\n\n\nB-Tree\n读取优化\nOLTP 数据库\n\n\nLSM-Tree\n写入优化\n日志、时序数据\n\n\n列存储\n分析优化\nOLAP、数据仓库\n\n\n分布式系统核心权衡定理：网络分区时，一致性与可用性不可兼得\n处理范式对比\n\n\n范式\n数据特性\n延迟\n典型框架\n\n\n\n批处理\n有界、静态\n分钟~小时\nSpark, Hadoop\n\n\n流处理\n无界、持续\n毫秒~秒\nFlink, Kafka Streams\n\n\n延伸资源\n官方网站：dataintensive.net\n中文翻译：ddia.vonng.com\n作者博客：martin.kleppmann.com\n\n\n本读书笔记整理于 2025年，基于 DDIA 第一版内容编写\n","categories":["读书笔记"],"tags":["DDIA","数据库","分布式系统","系统设计"]},{"title":"LLM 游戏智能体论文解读：应用扩展篇","url":"/2025/12/28/LLM-Game-Agents-%E5%BA%94%E7%94%A8%E6%89%A9%E5%B1%95%E7%AF%87/","content":"本文深入解读 LLM 智能体领域的三个重要应用扩展：VOYAGER（终身学习）、Project Sid（AI文明）和 Agent Hospital（可进化医疗智能体）。\n\n一、VOYAGER：开放世界具身终身学习智能体论文: An Open-Ended Embodied Agent with Large Language Models会议: NeurIPS 2023 (FMDM Workshop)作者: Guanzhi Wang 等 (NVIDIA, Caltech, UT Austin)项目主页: voyager.minedojo.org\n1.1 核心创新VOYAGER 是首个 LLM 驱动的具身终身学习智能体，在 Minecraft 中持续探索世界、获取技能、做出新发现，无需人类干预。\n三大核心组件:\n\n\n\n组件\n功能\n技术实现\n\n\n\n自动课程\n提出适当难度的任务\nGPT-4 + 探索进度 + 智能体状态\n\n\n技能库\n存储和检索可复用代码\n向量数据库 + 嵌入检索\n\n\n迭代提示\n自我改进代码生成\n环境反馈 + 执行错误 + 自我验证\n\n\n1.2 系统架构┌─────────────────────────────────────────────────────────────────────┐│                    VOYAGER 系统架构                                  │├─────────────────────────────────────────────────────────────────────┤│                                                                     ││   ┌───────────────┐                                                 ││   │   GPT-4 API   │◀──────────────────────────────────┐            ││   │  (黑盒调用)    │                                   │            ││   └───────────────┘                                   │            ││          │                                            │            ││          ▼                                            │            ││   ┌───────────────┐    ┌───────────────┐    ┌────────┴────────┐   ││   │ 自动课程生成   │    │   代码生成    │    │    自我验证     │   ││   │ (GPT-4提示)   │    │ (GPT-4提示)   │    │  (GPT-4提示)    │   ││   └───────────────┘    └───────────────┘    └─────────────────┘   ││          │                    │                      │             ││          ▼                    ▼                      ▼             ││   ┌───────────────┐    ┌───────────────┐    ┌─────────────────┐   ││   │   任务队列    │    │  Minecraft    │    │    技能库       │   ││   └───────────────┘    │   环境执行    │    │  (向量数据库)   │   ││                        └───────────────┘    └─────────────────┘   ││                                                                     │└─────────────────────────────────────────────────────────────────────┘\n\n1.3 自动课程系统设计理念: 自下而上展开，由好奇心驱动\n输入提示组件:\n\n指令: 鼓励多样化行为并施加约束\n智能体当前状态: 物品栏、装备、位置、生命值等\n先前任务记录: 已完成和失败的任务\n额外上下文: GPT-3.5 自问自答\n\n示例提示:\n\n“我的最终目标是发现尽可能多的多样化事物…下一个任务不应该太难，因为我可能还没有必要的资源或学会足够的技能来完成它。”\n\n1.4 技能库机制技能表示: 可执行的 JavaScript 代码\n// 示例技能: 制作木镐async function craftWoodenPickaxe(bot) {  // 首先获取木材  await mineBlock(bot, \"oak_log\", 1);  // 制作木板  await craftItem(bot, \"oak_planks\", 4);  // 制作木棍  await craftItem(bot, \"stick\", 2);  // 制作木镐  await craftItem(bot, \"wooden_pickaxe\", 1);}\n\n存储与检索:\n\n键: 程序描述的嵌入向量（GPT-3.5生成）\n值: 可执行的JavaScript代码\n检索: 余弦相似度 + 任务上下文\n\n1.5 迭代提示机制三种反馈类型:\n\n\n\n反馈类型\n来源\n作用\n\n\n\n环境反馈\n程序执行日志\n显示中间进度，如”需要多7个铁锭”\n\n\n执行错误\n程序解释器\n揭示语法错误和无效操作\n\n\n自我验证\nGPT-4评论家\n判断任务完成，提供改进建议\n\n\n代码生成的12个提示组件:\n\n\n\n#\n组件\n描述\n\n\n\n1\n代码生成指南\n编写规范和约束\n\n\n2\n控制原语API\n高级API（exploreUntil, mineBlock等）\n\n\n3\nMineflayer API\n底层游戏控制API\n\n\n4\n检索的技能\n从技能库检索的相关代码\n\n\n5\n上一轮代码\n用于迭代改进\n\n\n6\n环境反馈\n聊天日志中的执行信息\n\n\n7\n执行错误\n解释器错误信息\n\n\n8\n自我验证批评\n验证模块的反馈\n\n\n9\n智能体状态\n物品栏、位置、生命值等\n\n\n10\n任务\n自动课程提出的任务\n\n\n11\n任务上下文\nGPT-3.5生成的解决建议\n\n\n12\n思维链提示\n要求解释→计划→代码的顺序\n\n\n1.6 实验结果vs 基线方法:\n\n\n\n指标\nVOYAGER\nAutoGPT\nReAct\nReflexion\n\n\n\n独特物品发现\n63\n19\n~10\n~10\n\n\n倍数\n3.3x\n1x\n-\n-\n\n\n科技树解锁速度:\n\n\n\n级别\nVOYAGER\nAutoGPT\n提升\n\n\n\n木制工具\n6分钟\n92分钟\n15.3x\n\n\n石制工具\n11分钟\n94分钟\n8.5x\n\n\n铁制工具\n21分钟\n135分钟\n6.4x\n\n\n钻石工具\n102分钟\nN/A\n唯一成功\n\n\n消融实验结论:\n\n自动课程至关重要：移除后物品发现下降93%\n自我验证最重要：移除后物品发现下降73%\nGPT-4 vs GPT-3.5：GPT-4获得5.7倍更多独特物品\n\n1.7 关键洞见\n代码即记忆: VOYAGER 将”学习”转化为”运行时组合”——通过检索已有技能并迭代改进代码，而不是更新模型权重。\n\n\n\n\n传统方法\nVOYAGER\n\n\n\n微调模型参数\n黑盒API调用\n\n\n隐式知识存储\n显式代码技能库\n\n\n难以解释\n代码可读可执行\n\n\n灾难性遗忘\n技能永久保存\n\n\n\n二、Project Sid：迈向AI文明的多智能体模拟论文: Many-agent simulations toward AI civilization机构: Altera.AL发布日期: 2024年10月规模: 10-1000+ 智能体\n2.1 核心问题\n为什么我们应该尝试构建AI文明？\n\n为了让智能体与人类社会共存，他们需要是自主的和协作的。文明进步——通过智能体在人类文明中共存和进步的能力来衡量——代表了AI智能体能力的终极基准。\n2.2 构建AI文明的挑战\n\n\n挑战\n问题描述\n\n\n\n单智能体不进展\n幻觉积累、陷入重复动作循环\n\n\n多智能体不协调\n错误沟通导致幻觉传播\n\n\n缺乏基准\n无法量化文明进步\n\n\n一致性问题示例:\n\n智能体Abby被Bob要求”给我一把镐”时，聊天模块回应”当然可以！”，但函数调用模块选择”探索”。Bob可能然后尝试用想象的镐采矿。\n\n2.3 PIANO 架构PIANO = Parallel Information Aggregation via Neural Orchestration（通过神经编排的并行信息聚合）\n两大设计原则:\n\n\n\n原则\n问题\n解决方案\n\n\n\n并发性\n慢速思考不应阻止快速反应\n多模块并行运行，不同时间尺度\n\n\n一致性\n多输出模块可能产生冲突\n认知控制器(CC)作为瓶颈\n\n\n10个核心模块:\n\n\n\n模块\n功能\n\n\n\n记忆\n存储/检索对话、动作、观察\n\n\n动作意识\n评估自身状态和性能\n\n\n目标生成\n基于经验创建新目标\n\n\n社会意识\n解释他人社会线索\n\n\n说话\n解释和生成语音\n\n\n技能执行\n执行环境中的动作\n\n\n┌─────────────────────────────────────────────────────────────┐│                    PIANO 架构                                │├─────────────────────────────────────────────────────────────┤│                                                             ││  并发模块:                    认知控制器(瓶颈):              ││  ┌─────────┐                 ┌───────────────┐             ││  │ 记忆    │──────────────▶ │               │             ││  ├─────────┤                │   信息综合    │             ││  │ 社会    │──────────────▶ │       ↓       │             ││  ├─────────┤                │   高层决策    │             ││  │ 目标    │──────────────▶ │       ↓       │             ││  ├─────────┤                │   决策广播    │             ││  │ 动作    │──────────────▶ │               │             ││  └─────────┘                └───────────────┘             ││       ↑                            │                       ││       │                            ▼                       ││       │                     ┌───────────────┐             ││       │                     │ 输出模块      │             ││       │                     │ 说话/动作/... │             ││       │                     └───────────────┘             ││       └─────────────────────────────┘                      ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n2.4 文明进步基准基准1：专业化定义: 智能体自主发展专业角色\n三个标准:\n\n在选择和转换角色方面表现自主性\n专业化通过互动涌现，无需明确指导\n角色体现在与专业化一致的行为中\n\n实验结果 (30智能体，20分钟):\n\n\n\n现象\n发现\n\n\n\n角色多样性\n农民、矿工、工程师、守卫、探险家、铁匠\n\n\n角色持久性\n每个智能体角色在时间上大体稳定\n\n\n角色-行为一致性\n艺术家专注采花，农民专注收集种子\n\n\n武术社会 vs 艺术社会:\n\n武术社会特有角色：侦察兵、战略家\n艺术社会特有角色：策展人、收藏家\n\n基准2：集体规则定义: 智能体遵守和改变法律\n实验设置:\n\n25个选民智能体\n3个影响者（亲税/反税）\n1个选举经理\n税法：交20%物品到社区箱子\n\n关键发现:\n\n\n\n现象\n结果\n\n\n\n遵守法律\n平均交付~20%物品\n\n\n影响者影响\n亲税/反税影响者显著改变选民态度\n\n\n宪法变更\n税率从20%降到5-10%时，行为相应调整\n\n\n基准3：文化传播实验规模: 500智能体 (6城镇 + 农村)\n关键现象:\n\n\n\n现象\n发现\n\n\n\n模因多样性\n不同城镇流行不同模因\n\n\n模因动态\n流行度随时间上升和下降\n\n\n宗教传播\n20个牧师传播”飞天面条神教”\n\n\n皈依扩散\n皈依者数量持续增加，未饱和\n\n\n2.5 量化结果\n\n\n指标\n结果\n\n\n\n30分钟内获取物品\n平均17个独特物品\n\n\n4小时物品饱和\n~320个（1/3总物品）\n\n\n社会感知准确性\nr = 0.81（5+观察者）\n\n\n最大规模\n1000+ 智能体\n\n\n2.6 局限性\n缺乏视觉推理: 限制空间导航和建造能力\n缺乏内在驱动: 无生存、好奇心等催化社会发展\n无法从头涌现: 基于预训练知识，无法模拟创新涌现\n\n\n三、Agent Hospital：可进化的医疗智能体论文: A Simulacrum of Hospital with Evolvable Medical Agents机构: 清华大学 AIR发布日期: 2024年5月\n3.1 核心创新医生培养的两个阶段:\n\n\n\n阶段\n内容\n时长\n\n\n\n阶段1\n知识获取（学校）\n~20年\n\n\n阶段2\n技能获取（医院）\n~3年\n\n\n现有医疗AI主要集中在阶段1（如Med-PaLM）。Agent Hospital 解决阶段2：从实践中获取专业技能。\n3.2 系统架构Agent Hospital = 虚拟医院，所有患者、护士、医生都是LLM驱动的智能体\n系统规模:\n\n\n\n指标\n数量\n\n\n\n科室\n32个\n\n\n覆盖疾病\n339种\n\n\n医生智能体\n42个\n\n\n护士智能体\n4个\n\n\n功能区域\n16个\n\n\n3.3 治疗闭环┌─────────────────────────────────────────────────────────────┐│                    治疗闭环                                  │├─────────────────────────────────────────────────────────────┤│                                                             ││  1. 疾病发作 ──▶ 2. 分诊 ──▶ 3. 挂号                        ││        │                                                    ││        ▼                                                    ││  8. 康复反馈 ◀── 7. 取药 ◀── 6. 诊断                        ││        │                        ▲                           ││        │                        │                           ││        └─────▶ 4. 就诊 ──▶ 5. 检查 ─┘                       ││                                                             ││  额外事件：医生智能体在非工作时间阅读医学书籍                  ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n3.4 SEAL 框架SEAL = Simulacrum-based Evolutionary Agent Learning（基于仿真的进化智能体学习）\n两个组件:\n\n\n\n组件\n功能\n\n\n\n仿真系统构建\n构建虚拟世界，自动生成数据\n\n\n智能体进化\n从成功/失败中学习\n\n\n3.5 MedAgent-Zero 进化机制“Zero”含义: 不使用任何人工标注数据\n学习来源:\n\n\n\n来源\n内容\n作用\n\n\n\n成功案例\n正确的诊断和治疗\n作为参考案例检索\n\n\n失败案例\n错误的诊断或治疗\n反思避免重复错误\n\n\n医学教材\n专业医学知识\n巩固和整合知识\n\n\n┌─────────────────────────────────────────────────────────────┐│              MedAgent-Zero 进化流程                          │├─────────────────────────────────────────────────────────────┤│                                                             ││  1. 治疗患者智能体                                           ││     ↓                                                       ││  2. 收到患者反馈（康复/未康复）                               ││     ↓                                                       ││  ┌─────────────────┬─────────────────┐                     ││  │   成功案例      │    失败案例      │                     ││  │                 │                 │                     ││  │  存储为参考案例  │  反思获取经验    │                     ││  │  用于未来检索   │  避免重复错误    │                     ││  └─────────────────┴─────────────────┘                     ││     ↓                                                       ││  3. 阅读医学教材巩固知识                                     ││     ↓                                                       ││  4. 能力持续提升                                             ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n3.6 实验结果进化效果 (诊断准确率):\n\n\n\n治疗患者数\n准确率\n提升\n\n\n\n0 (初始)\n~60%\n-\n\n\n1,000\n~72%\n+20%\n\n\n10,000\n~85%\n+42%\n\n\n50,000\n~93%\n+55%\n\n\nMedQA 基准测试 (美国医师执照考试):\n\n\n\n方法\n准确率\n\n\n\nGPT-4 (少样本)\n78.4%\n\n\nMed-PaLM 2\n86.5%\n\n\nAgent Hospital (进化后)\n88.7%\n\n\n亮点: 无需使用基准的标注训练数据！\n3.7 与 Generative Agents 的关系\n\n\n维度\nGenerative Agents\nAgent Hospital\n\n\n\n灵感来源\n原创\n受GA启发\n\n\n环境\n虚拟小镇\n虚拟医院\n\n\n智能体数量\n25个\n46+\n\n\n任务类型\n社交模拟\n医疗诊断\n\n\n能力进化\n无\n有(核心创新)\n\n\n评估方式\n定性\n定量(MedQA)\n\n\n3.8 SEAL 的通用性方法论公式:\n领域工作流程 → 构建仿真系统 → 自动生成数据 → 智能体进化\n\n优势:\n\n\n\n优势\n说明\n\n\n\n无需人工标注\n数据由虚拟世界自动生成\n\n\n领域适应\n直接适应特定应用需求\n\n\n成本低\n减少数据标注开销\n\n\n可扩展\n可模拟大量场景和时间\n\n\n潜在应用: 法律咨询、金融投资、教育培训、客户服务\n\n四、三大应用扩展对比4.1 核心差异\n\n\n维度\nVOYAGER\nProject Sid\nAgent Hospital\n\n\n\n核心目标\n终身学习技能\nAI文明模拟\n医疗智能体进化\n\n\n环境\nMinecraft\nMinecraft\n虚拟医院\n\n\n智能体数量\n1\n10-1000+\n46+\n\n\n时间跨度\n数小时\n4小时+\n持续\n\n\n学习机制\n技能库积累\n社会互动\n经验反思\n\n\n4.2 创新贡献\n\n\n论文\n核心创新\n\n\n\nVOYAGER\n代码即记忆，技能可组合复用\n\n\nProject Sid\n文明进步基准：专业化、规则、文化\n\n\nAgent Hospital\n智能体能力可进化，虚拟技能迁移现实\n\n\n4.3 适用场景\n\n\n场景\n推荐方法\n原因\n\n\n\n开放世界游戏\nVOYAGER\n技能积累和终身学习\n\n\n社会科学研究\nProject Sid\n大规模社会动态模拟\n\n\n专业领域AI\nAgent Hospital\n从实践中持续进化\n\n\n多智能体协作\nProject Sid\nPIANO架构支持一致性\n\n\n\n五、技术演进路线5.1 从基础到应用基础框架 (2022-2023):├── ReAct: 推理+行动├── Reflexion: 语言反馈学习└── Generative Agents: 记忆+反思应用扩展 (2023-2024):├── VOYAGER: 终身学习 + 技能库├── Project Sid: 大规模文明模拟└── Agent Hospital: 专业领域进化未来趋势 (2025+):├── Agent OS化: AutoGen, LangGraph├── 多模态融合: 视觉+语言+行动└── 商业化部署: Operator, Claude\n\n5.2 规模演进\n\n\n时间\n论文\n智能体数量\n涌现现象\n\n\n\n2023/04\nGenerative Agents\n25\n社交行为\n\n\n2023/05\nVOYAGER\n1\n终身学习\n\n\n2024/05\nAgent Hospital\n46+\n能力进化\n\n\n2024/10\nProject Sid\n500-1000+\n文明进步\n\n\n5.3 关键技术突破\n\n\n突破\n论文\n意义\n\n\n\n代码作为记忆\nVOYAGER\n可执行、可组合的知识表示\n\n\n文明进步基准\nProject Sid\n量化多智能体社会能力\n\n\n无标注进化\nAgent Hospital\n从实践中自动学习\n\n\n千智能体规模\nProject Sid\n验证大规模可行性\n\n\n\n六、实践建议6.1 技术选型\n\n\n需求\n推荐技术栈\n\n\n\n单智能体技能学习\nVOYAGER (技能库 + 迭代提示)\n\n\n多智能体协作\nProject Sid (PIANO架构)\n\n\n专业领域应用\nAgent Hospital (SEAL框架)\n\n\n通用任务完成\nReAct + Reflexion\n\n\n6.2 架构设计理想组合:\n理想智能体 = VOYAGER的技能库           + Project Sid的社会意识           + Agent Hospital的进化机制           + Generative Agents的记忆系统\n\n6.3 规模化考虑\n\n\n规模\n关键挑战\n解决方案\n\n\n\n1-10\n单智能体能力\n技能库 + 反思\n\n\n10-50\n协调一致性\nPIANO架构\n\n\n50-500\n计算资源\n并行模块\n\n\n500+\n涌现管理\n文明基准\n\n\n\n七、关键论文原文引用VOYAGER\n“VOYAGER is the first LLM-powered embodied lifelong learning agent that explores the world, acquires diverse skills, and makes novel discoveries without human intervention.”\n\nProject Sid\n“We show how 10-1000+ AI agents behave and progress in agent societies. These simulations reveal that agents can achieve meaningful progress—autonomously developing specialized roles, adhering to and modifying collective rules, and engaging in cultural and religious propagation.”\n\nAgent Hospital\n“Doctor agents can evolve by treating a large number of patient agents, without the need for manually curated training data. After treating tens of thousands of patient agents (which may take several years for real-world doctors), the evolved doctor agents surpassed state-of-the-art medical AI methods on the MedQA benchmark.”\n\n\n返回总览 | 上一篇：基础框架篇\n","categories":["论文解读"],"tags":["LLM","Agent","论文解读","VOYAGER","Project Sid","Agent Hospital","终身学习","AI文明"]},{"title":"LLM 游戏智能体论文解读：基础框架篇","url":"/2025/12/28/LLM-Game-Agents-%E5%9F%BA%E7%A1%80%E6%A1%86%E6%9E%B6%E7%AF%87/","content":"本文深入解读 LLM 智能体领域的三大基础框架：ReAct、Reflexion 和 Generative Agents，分析它们的核心架构、技术创新和应用场景。\n\n一、ReAct：推理与行动的协同论文: Synergizing Reasoning and Acting in Language Models会议: ICLR 2023作者: Shunyu Yao 等 (普林斯顿大学 &amp; Google Research)被引用: 32次（领域内最高）\n1.1 核心思想人类智能的一个独特特征是能够无缝结合面向任务的动作与语言推理。考虑在厨房做菜的例子：\n\n在任何两个具体动作之间，我们可能用语言进行推理，以跟踪进度\n处理异常或根据情况调整计划\n认识到何时需要外部信息\n\nReAct 的核心理念：将智能体的动作空间扩展为 Â = A ∪ L\n其中：\n\nA = 原始动作空间（与环境交互）\nL = 语言空间（思想/推理轨迹）\n\n┌─────────────────────────────────────────────────────────────┐│                    ReAct 工作流程                            │├─────────────────────────────────────────────────────────────┤│                                                             ││   问题 ──▶ 思想1 ──▶ 动作1 ──▶ 观察1                        ││                       │                                     ││                       ▼                                     ││            思想2 ──▶ 动作2 ──▶ 观察2                        ││                       │                                     ││                       ▼                                     ││            思想3 ──▶ 动作3 ──▶ 答案                         ││                                                             ││   思想：不影响环境，用于推理和规划                           ││   动作：与环境交互，获取新信息                               ││   观察：环境反馈                                             ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n1.2 推理轨迹类型\n\n\n类型\n示例\n\n\n\n问题分解\n“我需要搜索x，找到y，然后找到z”\n\n\n信息提取\n“x于1844年创立”\n\n\n常识推理\n“1844 &lt; 1989，所以x更老”\n\n\n搜索重构\n“也许我可以搜索/查找x来代替”\n\n\n答案综合\n“…所以答案是x”\n\n\n1.3 实验结果知识密集型任务 (HotpotQA, Fever):\n\n\n\n方法\nHotpotQA EM\nFever Acc\n\n\n\nCoT\n29.4\n56.3\n\n\nAct\n25.7\n58.9\n\n\nReAct\n27.4\n60.9\n\n\nReAct + CoT-SC\n35.1\n64.6\n\n\n决策任务 (ALFWorld, WebShop):\n\n\n\n方法\nALFWorld\nWebShop\n\n\n\nBUTLER (模仿学习)\n37%\n-\n\n\nAct\n45%\n-\n\n\nReAct\n71%\n+10%\n\n\n1.4 成功与失败模式\n\n\n类型\nReAct\nCoT\n\n\n\n成功-真阳性\n94%\n86%\n\n\n成功-假阳性\n6%\n14%\n\n\n失败-幻觉\n0%\n56%\n\n\n失败-推理错误\n47%\n16%\n\n\n关键洞见：\n\nReAct 的幻觉率为0%，因为可以与外部知识库交互验证\nCoT 的主要失败模式是幻觉（56%）\nReAct 的主要失败是推理错误（47%），因结构约束降低了灵活性\n\n1.5 ReAct 示例问题: 除了Apple Remote，还有什么设备可以控制Apple Remote最初设计交互的程序？\n思想1: 我需要搜索Apple Remote，找到它最初设计交互的程序动作1: Search[Apple Remote]观察1: Apple Remote是一个遥控器...最初设计用于控制Front Row...思想2: Apple Remote最初设计控制Front Row。我需要搜索Front Row动作2: Search[Front Row software]观察2: Front Row是一个已停产的媒体中心软件...由Apple Remote或键盘功能键控制思想3: Front Row由Apple Remote或键盘功能键控制。所以答案是键盘功能键动作3: Finish[keyboard function keys]\n\n\n二、Reflexion：语言反馈强化学习论文: Language Agents with Verbal Reinforcement Learning会议: NeurIPS 2023作者: Noah Shinn 等 (Northeastern &amp; Princeton)被引用: 17次\n2.1 核心思想传统强化学习通过梯度更新权重学习，需要大量样本和昂贵的微调。Reflexion 提出用语言反馈替代梯度信号：\n┌─────────────────────────────────────────────────────────────┐│              传统RL vs Reflexion                             │├─────────────────────────────────────────────────────────────┤│                                                             ││   传统RL:                                                   ││   状态 ──▶ 动作 ──▶ 奖励 ──▶ 梯度更新 ──▶ 参数变化          ││                                                             ││   Reflexion:                                                ││   状态 ──▶ 动作 ──▶ 反馈 ──▶ 语言反思 ──▶ 记忆存储          ││                       │                                     ││                       └──────────────────▶ 下次尝试         ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n2.2 三大核心组件Actor（执行者）基于 LLM 构建，生成文本和动作。可以是：\n\nChain of Thought (CoT)\nReAct\n其他智能体架构\n\nEvaluator（评估者）评估 Actor 生成输出的质量：\n\n精确匹配(EM)评分: 推理任务\n预定义启发式: 决策任务\nLLM作为评估者: 编程任务\n\nSelf-Reflection（自我反思）核心创新：将稀疏奖励信号转化为详细的语言化反思\n输入:   - 任务描述  - 失败轨迹: [动作1, 观察1, 动作2, 观察2, ...]  - 奖励信号: 二元或标量  - 历史反思输出:  - 错误诊断  - 改进方案  - 具体建议\n\n2.3 记忆机制\n\n\n类型\n内容\n作用\n\n\n\n短期记忆\n当前轨迹历史\n即时决策\n\n\n长期记忆\n自我反思输出（滑动窗口）\n跨尝试学习\n\n\n2.4 实验结果AlfWorld 决策任务:\n\n\n\n方法\n成功率\n\n\n\nReAct\n~50%\n\n\nReAct + Reflexion (启发式)\n97% (130/134)\n\n\nReAct + Reflexion (GPT)\n88%\n\n\nHotPotQA 推理任务:\n\n\n\n方法\n准确率提升\n\n\n\nCoT (GT)\n基准\n\n\n+ 情景记忆\n+6%\n\n\n+ Reflexion\n+14%\n\n\n编程任务 (HumanEval):\n\n\n\n基准\n先前SOTA\nGPT-4\nReflexion\n\n\n\nHumanEval (PY)\n65.8%\n80.1%\n91.0%\n\n\nHumanEval (RS)\n-\n60.0%\n68.0%\n\n\nLeetcode Hard\n-\n7.5%\n15.0%\n\n\n2.5 Reflexion 示例任务: 用台灯检查杯子\n第一次尝试（失败）:\n&gt; go to drawer 1抽屉1是关着的&gt; ... (搜索杯子)&gt; take mug 1 from desk 1你从桌子1拿起杯子1&gt; use desklamp 1什么都没发生状态: 失败\n\n反思:\n\n在这个环境中，我的计划是先找杯子再找台灯并使用。然而，任务说的是用台灯检查杯子。我应该先找台灯，再找杯子。我注意到台灯在桌子1上。下一次尝试中，我会先去桌子1，找到台灯，然后找杯子并用台灯检查它。\n\n第二次尝试（成功）:\n&gt; go to desk 1桌子1上有：台灯1、杯子1...&gt; take mug 1 from desk 1你从桌子1拿起杯子1&gt; use desklamp 1你打开了台灯1状态: 成功\n\n\n三、Generative Agents：人类行为的交互式拟像论文: Interactive Simulacra of Human Behavior会议: UIST 2023作者: Joon Sung Park 等 (斯坦福大学 &amp; Google)被引用: 20次\n3.1 核心思想构建模拟可信人类行为的计算软件智能体：\n\n醒来、做早餐、去上班\n艺术家画画，作者写作\n形成观点，注意彼此，主动发起对话\n回忆和反思过去，规划未来\n\n3.2 核心架构┌──────────────────────────────────────────────────────────────────┐│                        记忆流 (Memory Stream)                     ││  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐   ││  │   观察      │  │   反思      │  │        计划             │   ││  │ Observations│  │ Reflections │  │       Plans             │   ││  └─────────────┘  └─────────────┘  └─────────────────────────┘   │└──────────────────────────────────────────────────────────────────┘                                   │                                   ▼                    ┌─────────────────────────────────────┐                    │           记忆检索                  │                    │    (时近性 + 重要性 + 相关性)        │                    └──────────────┬──────────────────────┘                                   │                                   ▼                    ┌─────────────────────────────────────┐                    │           行为生成                  │                    │     (Plan, React, Dialogue)        │                    └─────────────────────────────────────┘\n\n3.3 记忆检索公式\n\n\n\n组件\n描述\n实现\n\n\n\n时近性\n最近访问的记忆分数更高\n指数衰减函数，衰减因子0.995\n\n\n重要性\n区分平凡记忆和核心记忆\nLLM评分1-10\n\n\n相关性\n与当前情况相关的记忆\n嵌入向量余弦相似度\n\n\n3.4 反思机制触发条件: 重要性分数总和 &gt; 150（约每天2-3次）\n反思生成过程:\n\n确定反思内容: 用最近100条记忆查询\n\n提示：”仅根据上述信息，我们可以回答哪3个最突出的高层次问题？”\n\n\n检索相关记忆: 使用问题作为检索查询\n\n提取洞察: \n\n输出格式：”洞察（因为1, 5, 3）”\n\n\n\n反思树: 叶节点=观察，非叶节点=越来越抽象的反思\n          [Klaus对研究充满热情]  ← 元反思                /        \\[Klaus致力于研究]    [Klaus和Maria有共同兴趣]  ← 反思     /    \\              /      \\[写论文] [读书]     [讨论项目] [图书馆相遇]  ← 观察\n\n3.5 规划机制递归分解日程:\n\n粗略计划: 一天的议程大纲\n小时级分解: 每小时的活动块\n细粒度分解: 5-15分钟的具体动作\n\n示例:\n\n粗略：”下午1:00到5:00创作新音乐”\n小时级：”下午1:00：开始为音乐创作头脑风暴…”\n细粒度：”下午4:00：拿一些小零食。下午4:05：在工作区周围短暂散步…”\n\n3.6 涌现的社会行为实验设置: 25个智能体，Smallville小镇\n涌现现象:\n\n\n\n现象\n描述\n\n\n\n信息扩散\nSam的市长候选资格传播到32%智能体\n\n\n关系记忆\n智能体记住新认识的人及对话内容\n\n\n协调活动\nIsabella的情人节派对：5人自发出席\n\n\n网络密度\n从0.167增加到0.74\n\n\n情人节派对案例:\n\nIsabella计划2月14日下午5-7点的派对\n她花一天装饰咖啡馆\nMaria帮忙装饰，并邀请暗恋的Klaus\n最终5个智能体在正确时间出现\n\n3.7 评估结果\n\n\n条件\nTrueSkill评分\n\n\n\n完整架构\n29.89\n\n\n无反思\n26.88\n\n\n无反思、无计划\n25.64\n\n\n人类众包\n22.95\n\n\n无记忆（先前SOTA）\n21.21\n\n\n效应大小: 完整架构 vs 先前SOTA = 8个标准差\n\n四、三大框架对比4.1 核心差异\n\n\n维度\nReAct\nReflexion\nGenerative Agents\n\n\n\n核心目标\n任务完成\n从失败学习\n行为拟真\n\n\n知识表示\n推理轨迹\n语言化反思\n记忆流\n\n\n学习方式\n单次推理\n跨尝试积累\n持续记忆+反思\n\n\n时间跨度\n单任务\n多次尝试\n天/周级\n\n\n是否微调\n❌\n❌\n❌\n\n\n4.2 记忆机制对比\n\n\n特性\nReAct\nReflexion\nGenerative Agents\n\n\n\n存储内容\n当前轨迹\n语言化反思\n观察+反思+计划\n\n\n存储形式\n上下文\n滑动窗口\n记忆流列表\n\n\n检索方式\n无\n时间顺序\n时近性+重要性+相关性\n\n\n失败经验\n❌\n✅ 重点\n⚠️ 不强调\n\n\n抽象层次\n单层\n双层\n多层（反思树）\n\n\n4.3 反思机制对比\n\n\n特性\nReAct\nReflexion\nGenerative Agents\n\n\n\n有无反思\n❌ 无\n✅ 核心\n✅ 核心\n\n\n触发条件\n-\n每次失败后\n重要性&gt;150\n\n\n输出\n-\n错误分析+改进\n高层次洞察\n\n\n目的\n-\n任务成功率\n概念抽象\n\n\n4.4 适用场景\n\n\n场景\n推荐方法\n原因\n\n\n\n知识问答\nReAct\n与外部知识库交互\n\n\n决策任务\nReflexion\n从失败中学习\n\n\n编程调试\nReflexion\n需要多次尝试改进\n\n\n社会模拟\nGenerative Agents\n需要记忆和人格一致性\n\n\n角色扮演\nGenerative Agents\n需要丰富的背景记忆\n\n\n\n五、组合使用建议5.1 理想组合架构┌────────────────────────────────────────────────────────────────────┐│                    理想智能体架构                                   │├────────────────────────────────────────────────────────────────────┤│                                                                    ││   ┌───────────────────────────────────────────────────────────┐   ││   │  Generative Agents的记忆流                                 │   ││   │  • 完整的经历记录                                          │   ││   │  • 多层次反思                                              │   ││   │  • 社交关系追踪                                            │   ││   └───────────────────────────────────────────────────────────┘   ││                              +                                     ││   ┌───────────────────────────────────────────────────────────┐   ││   │  ReAct的推理-行动范式                                      │   ││   │  • 思想与动作交替                                          │   ││   │  • 与外部环境交互                                          │   ││   │  • 减少幻觉                                                │   ││   └───────────────────────────────────────────────────────────┘   ││                              +                                     ││   ┌───────────────────────────────────────────────────────────┐   ││   │  Reflexion的失败反思                                       │   ││   │  • 失败经验的语言化                                        │   ││   │  • 错误诊断与改进建议                                      │   ││   │  • 跨尝试学习                                              │   ││   └───────────────────────────────────────────────────────────┘   ││                                                                    │└────────────────────────────────────────────────────────────────────┘\n\n5.2 实现要点\n使用 ReAct 作为基础行动框架：思想+动作交替执行\n添加 Generative Agents 的记忆系统：持久化所有经历\n集成 Reflexion 的失败反思：从错误中学习\n定期触发高层次反思：形成长期理解\n\n\n六、关键论文原文引用ReAct\n“We propose ReAct — a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks.”\n\nReflexion\n“Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as additional context for the LLM agent in the next episode.”\n\nGenerative Agents\n“Generative agents wake up, cook breakfast, and head to work; artists paint, authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day.”\n\n\n返回总览 | 下一篇：应用扩展篇\n","categories":["论文解读"],"tags":["LLM","Agent","论文解读","ReAct","Reflexion","Generative Agents"]},{"title":"NLP 学习路线：从基础到大语言模型","url":"/2019/10/31/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E2%80%94%E2%80%94%E8%AF%BB%E9%A6%99%E4%BE%AC%E7%A7%91%E6%8A%80%E6%9D%8E%E7%BA%A7%E4%B8%BA%E3%80%8A%E5%87%BA%E5%85%A5NLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BB%BA%E8%AE%AE%E3%80%8B%E6%96%87%E7%AB%A0/","content":"本文整理了 NLP 领域的学习路线，结合经典理论与现代大语言模型技术。\n推荐学习资源经典教材\n\n\n书籍\n内容\n难度\n\n\n\nSpeech and Language Processing (Jurafsky)\nNLP 全面综述\n⭐⭐\n\n\nIntroduction to Information Retrieval\n信息检索基础\n⭐⭐\n\n\nPattern Recognition and Machine Learning\n机器学习理论\n⭐⭐⭐⭐\n\n\nDeep Learning (Goodfellow)\n深度学习基础\n⭐⭐⭐\n\n\n现代资源\nStanford CS224N: NLP with Deep Learning\nHugging Face Course\nLLM University by Cohere\n\n阶段一：NLP 基础语言模型基础N-gram 模型：N-1 阶马尔可夫假设\n\nfrom collections import defaultdictimport numpy as npclass NGramLM:    def __init__(self, n=3):        self.n = n        self.counts = defaultdict(lambda: defaultdict(int))        self.totals = defaultdict(int)        def train(self, corpus):        for sentence in corpus:            tokens = ['&lt;s&gt;'] * (self.n - 1) + sentence + ['&lt;/s&gt;']            for i in range(len(tokens) - self.n + 1):                context = tuple(tokens[i:i+self.n-1])                word = tokens[i+self.n-1]                self.counts[context][word] += 1                self.totals[context] += 1        def probability(self, word, context):        context = tuple(context[-(self.n-1):])        return self.counts[context][word] / max(self.totals[context], 1)\n\n词向量从 One-hot 到 Dense Embedding 的演进：\n\n\n\n方法\n年份\n特点\n\n\n\nOne-hot\n-\n稀疏，无语义\n\n\nWord2Vec\n2013\n分布式表示\n\n\nGloVe\n2014\n全局统计\n\n\nFastText\n2016\n子词信息\n\n\nELMo\n2018\n上下文相关\n\n\nBERT\n2018\n双向上下文\n\n\n阶段二：深度学习 NLPTransformer 架构import torchimport torch.nn as nnimport mathclass MultiHeadAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.d_k = d_model // n_heads        self.n_heads = n_heads                self.W_q = nn.Linear(d_model, d_model)        self.W_k = nn.Linear(d_model, d_model)        self.W_v = nn.Linear(d_model, d_model)        self.W_o = nn.Linear(d_model, d_model)        def forward(self, Q, K, V, mask=None):        batch_size = Q.size(0)                # Linear projections        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)                # Attention scores        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)                if mask is not None:            scores = scores.masked_fill(mask == 0, -1e9)                attn = torch.softmax(scores, dim=-1)        output = torch.matmul(attn, V)                # Concatenate and project        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)        return self.W_o(output)\n\n注意力机制的数学表达\n阶段三：大语言模型LLM 架构演进GPT-1 (2018) → GPT-2 → GPT-3 → ChatGPT → GPT-4     ↓BERT → RoBERTa → DeBERTa     ↓T5 → Flan-T5 → UL2     ↓LLaMA → LLaMA 2 → Mistral → Mixtral\n\nPrompt Engineering# 1. Zero-shotprompt = \"Translate to French: Hello, how are you?\"# 2. Few-shotprompt = \"\"\"Translate to French:Hello -&gt; BonjourGoodbye -&gt; Au revoirHow are you? -&gt;\"\"\"# 3. Chain-of-Thoughtprompt = \"\"\"Q: If I have 3 apples and buy 5 more, how many do I have?A: Let's think step by step.1. I start with 3 apples.2. I buy 5 more apples.3. Total = 3 + 5 = 8 apples.The answer is 8.Q: If I have 7 oranges and eat 2, how many remain?A: Let's think step by step.\"\"\"\n\nFine-tuning 技术\n\n\n方法\n可训练参数\n适用场景\n\n\n\nFull Fine-tuning\n100%\n大量数据，充足算力\n\n\nLoRA\n0.1-1%\n资源受限\n\n\nQLoRA\n0.1%\n消费级 GPU\n\n\nPrefix Tuning\n0.1%\n多任务\n\n\nPrompt Tuning\n&lt;0.01%\n极端资源受限\n\n\nfrom peft import LoraConfig, get_peft_modellora_config = LoraConfig(    r=8,    lora_alpha=32,    target_modules=[\"q_proj\", \"v_proj\"],    lora_dropout=0.1,    bias=\"none\",)model = get_peft_model(base_model, lora_config)print(f\"Trainable params: {model.print_trainable_parameters()}\")\n\n阶段四：高级主题检索增强生成 (RAG)from langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.vectorstores import Chromafrom langchain.chains import RetrievalQA# 构建向量库embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh\")vectorstore = Chroma.from_documents(documents, embeddings)# 创建 RAG 链qa = RetrievalQA.from_chain_type(    llm=llm,    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}))\n\n模型评估# 困惑度 (Perplexity)def perplexity(model, tokenizer, text):    encodings = tokenizer(text, return_tensors='pt')    max_length = model.config.n_positions        nlls = []    for i in range(0, encodings.input_ids.size(1), max_length):        begin_loc = max(i - max_length, 0)        end_loc = i + max_length        input_ids = encodings.input_ids[:, begin_loc:end_loc]        target_ids = input_ids.clone()        target_ids[:, :-1] = -100                with torch.no_grad():            outputs = model(input_ids, labels=target_ids)            nlls.append(outputs.loss)        return torch.exp(torch.stack(nlls).mean())\n\n实践项目建议\n入门：情感分析、文本分类\n进阶：命名实体识别、机器翻译\n高级：问答系统、RAG 应用\n专家：LLM 预训练、RLHF\n\n延伸阅读\nAttention Is All You Need\nBERT Paper\nLLaMA Paper\nLoRA Paper\n\n\n\n转载请注明出处\n\n","tags":["LLM","NLP","machine learning"]},{"title":"神经网络机器阅读理解：从 Attention 到 LLM","url":"/2019/11/22/Nuural-Approaches-to-Machine-Reading-Comprehension-and-Dialogue/","content":"本文综述神经网络在机器阅读理解和对话系统中的发展历程，从早期的注意力机制到现代大语言模型。\n发展时间线2015-2016: 注意力机制兴起    └── Attentive Reader, Impatient Reader, BiDAF2017-2018: 深度交互与预训练    └── R-Net, QANet, BERT2019-2020: 大规模预训练    └── RoBERTa, ALBERT, T52021-2023: 大语言模型时代    └── GPT-3, ChatGPT, GPT-4, LLaMA2024-: 检索增强与多模态    └── RAG, Vision-Language Models\n\n核心技术演进阶段一：注意力机制 (2015-2017)问题：如何让模型”关注”与问题相关的上下文？\n\n\n代表模型：Attentive Reader, BiDAF\n阶段二：深度交互 (2017-2018)问题：如何建模问题和上下文的复杂交互？\n技术：多轮注意力、自注意力、门控机制\n# 多轮推理 (R-Net 风格)for layer in range(num_layers):    # 自注意力    context = self_attention(context, context)    # 交叉注意力    context = cross_attention(context, question)\n\n阶段三：预训练语言模型 (2018-2020)范式转变：从 task-specific 到 pretrain-finetune\n$$\\theta^* = \\arg\\min_\\theta \\mathcal{L}{task}(\\text{PLM}\\theta(x), y)$$\n代表模型：BERT, RoBERTa, ALBERT\nfrom transformers import AutoModelForQuestionAnsweringmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")# Fine-tune on SQuAD\n\n阶段四：大语言模型 (2020-至今)范式转变：从 fine-tuning 到 prompting\n# Few-shot promptingprompt = \"\"\"Context: The Eiffel Tower was built in 1889.Question: When was the Eiffel Tower built?Answer: 1889Context: {context}Question: {question}Answer:\"\"\"\n\n架构对比\n\n\n模型\n参数量\n训练范式\nSQuAD 2.0 F1\n\n\n\nBiDAF\n~2M\n从零训练\n77.3\n\n\nBERT-base\n110M\n预训练+微调\n88.5\n\n\nBERT-large\n340M\n预训练+微调\n90.9\n\n\nRoBERTa-large\n355M\n预训练+微调\n91.4\n\n\nGPT-3\n175B\nFew-shot\n~88\n\n\nGPT-4\n~1.8T\nZero-shot\n~95\n\n\n现代 MRC 系统设计RAG 架构class ModernMRC:    def __init__(self, retriever, reader):        self.retriever = retriever  # Dense retriever        self.reader = reader        # LLM        def answer(self, question: str, knowledge_base: str = None):        # 1. 检索        if knowledge_base:            docs = self.retriever.retrieve(question, knowledge_base)            context = \"\\n\\n\".join([d.text for d in docs])        else:            context = \"\"                # 2. 阅读理解/生成        prompt = self._build_prompt(question, context)        answer = self.reader.generate(prompt)                # 3. 后处理（可选：验证、引用）        return self._postprocess(answer, docs)        def _build_prompt(self, question, context):        if context:            return f\"\"\"Based on the following context, answer the question.Context:{context}Question: {question}Answer:\"\"\"        else:            return f\"Question: {question}\\nAnswer:\"\n\n多跳推理class MultiHopReasoner:    def __init__(self, retriever, llm, max_hops=3):        self.retriever = retriever        self.llm = llm        self.max_hops = max_hops        def reason(self, question):        reasoning_chain = []        current_query = question                for hop in range(self.max_hops):            # 检索            docs = self.retriever.retrieve(current_query)                        # 生成中间推理            intermediate = self.llm.generate(                f\"Based on: {docs}\\nQuestion: {current_query}\\n\"                f\"Provide intermediate reasoning or the final answer:\"            )                        reasoning_chain.append({                'query': current_query,                'docs': docs,                'reasoning': intermediate            })                        # 检查是否已得到答案            if self._is_final_answer(intermediate):                break                        # 生成下一跳查询            current_query = self._generate_next_query(question, reasoning_chain)                return self._synthesize_answer(question, reasoning_chain)\n\n对话系统中的 MRC对话式问答class ConversationalQA:    def __init__(self, mrc_model, history_length=5):        self.mrc_model = mrc_model        self.history = []        self.history_length = history_length        def ask(self, question, context=None):        # 将对话历史纳入问题        contextualized_question = self._contextualize(question)                # 获取答案        answer = self.mrc_model.answer(contextualized_question, context)                # 更新历史        self.history.append({'q': question, 'a': answer})        if len(self.history) &gt; self.history_length:            self.history.pop(0)                return answer        def _contextualize(self, question):        if not self.history:            return question                history_text = \"\\n\".join([            f\"Q: {turn['q']}\\nA: {turn['a']}\"            for turn in self.history        ])                return f\"Conversation history:\\n{history_text}\\n\\nCurrent question: {question}\"\n\n评估体系传统指标\n\n\n指标\n定义\n适用场景\n\n\n\nEM\n精确匹配\n抽取式 QA\n\n\nF1\nToken 重叠\n抽取式 QA\n\n\nBLEU\nN-gram 重叠\n生成式 QA\n\n\nROUGE\n召回导向重叠\n摘要、长答案\n\n\nLLM 时代指标# LLM-as-Judgedef llm_evaluate(question, reference, prediction):    prompt = f\"\"\"Evaluate the answer quality on a scale of 1-5:Question: {question}Reference Answer: {reference}Model Answer: {prediction}Criteria:- Correctness: Is the information accurate?- Completeness: Does it fully answer the question?- Conciseness: Is it appropriately brief?Score (1-5):\"\"\"        return llm.generate(prompt)\n\n延伸阅读\nReading Wikipedia to Answer Open-Domain Questions\nRAG Paper\nHotpotQA: Multi-hop Reasoning\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\n\n转载请注明出处\n\n","tags":["LLM","MRC","KBQA","Deep learning"]},{"title":"因果关系推断介绍","url":"/2019/10/03/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/","content":"因果推断是机器学习领域的重要研究方向，特别是在大语言模型时代，理解因果关系对于构建可解释、可信赖的 AI 系统至关重要。\n为什么需要因果推断？传统机器学习依赖相关性，但相关性不等于因果性。例如：\n\n冰淇淋销量与溺水事件正相关（共同原因：夏天）\nLLM 可能学到虚假相关性，导致 hallucination\n\n因果推断帮助我们：\n\n理解干预效果（如果我做 X，会发生什么？）\n进行反事实推理（如果当时做了 Y，结果会怎样？）\n构建更鲁棒的模型\n\n核心概念因果图 (Causal Graph)使用有向无环图 (DAG) 表示变量之间的因果关系：\nX → Y → Z    (链式结构)X ← W → Y    (混杂结构)  X → W ← Y    (对撞结构)\n\n结构因果模型 (SCM)\n其中  是原因， 是结果， 是噪声项。\ndo 算子与干预区分观测和干预：\n\n观测： — 看到 X=x 时 Y 的分布\n干预： — 强制设置 X=x 时 Y 的分布\n\n因果发现算法PC 算法基于条件独立性检验的经典算法：\n# PC 算法伪代码def pc_algorithm(data, alpha=0.05):    # 1. 初始化完全图    G = complete_graph(variables)        # 2. 骨架学习：移除条件独立的边    for (X, Y) in edges(G):        for S in subsets(neighbors):            if conditional_independent(X, Y, S, alpha):                remove_edge(G, X, Y)                sep_set[X, Y] = S        # 3. 方向确定：识别 v-structure    orient_v_structures(G, sep_set)        return G\n\nPython 实现参考：fooSynaptic/py_pcalg\n现代方法\n\n\n方法\n特点\n适用场景\n\n\n\nNOTEARS\n连续优化，可微分\n线性/非线性因果发现\n\n\nDAG-GNN\n基于图神经网络\n大规模因果图学习\n\n\nCausal Transformer\n结合注意力机制\n时序因果推断\n\n\n因果推断与大语言模型LLM 中的因果问题\nHallucination：模型学到虚假相关性\nBias：训练数据中的混杂因素\nRobustness：分布外泛化能力差\n\n解决方案# 因果提示 (Causal Prompting) 示例prompt = \"\"\"请分析以下事件的因果关系，而非相关性：事件A: 公司增加广告投入事件B: 销售额上升问：A 是否导致了 B？请考虑可能的混杂因素。\"\"\"\n\n因果推理增强 RAGclass CausalRAG:    def __init__(self, retriever, causal_graph):        self.retriever = retriever        self.causal_graph = causal_graph        def retrieve(self, query):        # 1. 识别查询中的因果关系        cause, effect = extract_causal_pair(query)                # 2. 基于因果图过滤无关文档        relevant_vars = self.causal_graph.ancestors(effect)                # 3. 检索因果相关的文档        docs = self.retriever.search(query)        return filter_by_causal_relevance(docs, relevant_vars)\n\n工具与资源\n\n\n工具\n语言\n功能\n\n\n\nDoWhy\nPython\n因果推断框架\n\n\nCausalNex\nPython\n贝叶斯网络 + 因果发现\n\n\npgmpy\nPython\n概率图模型\n\n\nTetrad\nJava\n因果搜索算法\n\n\n# DoWhy 示例import dowhyfrom dowhy import CausalModelmodel = CausalModel(    data=df,    treatment='treatment',    outcome='outcome',    graph='digraph {treatment -&gt; outcome; confounder -&gt; treatment; confounder -&gt; outcome}')# 识别因果效应identified = model.identify_effect()# 估计因果效应estimate = model.estimate_effect(identified, method_name=\"backdoor.propensity_score_matching\")\n\n延伸阅读\nJudea Pearl, The Book of Why (2018)\nPeters et al., Elements of Causal Inference (2017)\nStanford CS 228: Probabilistic Graphical Models\n\n\n\n转载请注明出处\n\n","tags":["machine learning","bayesian network","causality infer"]},{"title":"机器阅读理解实战：从零构建问答系统","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%AE%9E%E8%B7%B5/","content":"本文从零开始实现一个机器阅读理解系统，涵盖数据处理、模型构建、训练和推理的完整流程。\n任务定义给定上下文  和问题 ，预测答案  在  中的位置：\n\n数据处理SQuAD 数据格式import jsonfrom dataclasses import dataclassfrom typing import List, Optional@dataclassclass Example:    context: str    question: str    answer_text: str    start_position: int    end_position: intdef load_squad(file_path: str) -&gt; List[Example]:    with open(file_path, 'r', encoding='utf-8') as f:        data = json.load(f)        examples = []    for article in data['data']:        for paragraph in article['paragraphs']:            context = paragraph['context']            for qa in paragraph['qas']:                question = qa['question']                if qa.get('is_impossible', False):                    continue                answer = qa['answers'][0]                examples.append(Example(                    context=context,                    question=question,                    answer_text=answer['text'],                    start_position=answer['answer_start'],                    end_position=answer['answer_start'] + len(answer['text'])                ))        return examples\n\nTokenizationfrom transformers import AutoTokenizerclass MRCTokenizer:    def __init__(self, model_name: str, max_length: int = 384, doc_stride: int = 128):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        self.doc_stride = doc_stride        def encode(self, example: Example):        # Tokenize question and context        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            stride=self.doc_stride,            return_overflowing_tokens=True,            return_offsets_mapping=True,            padding='max_length',        )                # 找到答案在 token 序列中的位置        offset_mapping = encoding['offset_mapping'][0]                start_token = None        end_token = None                for idx, (start, end) in enumerate(offset_mapping):            if start &lt;= example.start_position &lt; end:                start_token = idx            if start &lt; example.end_position &lt;= end:                end_token = idx                break                return {            'input_ids': encoding['input_ids'][0],            'attention_mask': encoding['attention_mask'][0],            'start_position': start_token or 0,            'end_position': end_token or 0,        }\n\n模型实现基于 BERT 的 MRC 模型import torchimport torch.nn as nnfrom transformers import AutoModelclass MRCModel(nn.Module):    def __init__(self, model_name: str, dropout: float = 0.1):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size                self.dropout = nn.Dropout(dropout)        self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        start_positions: Optional[torch.Tensor] = None,        end_positions: Optional[torch.Tensor] = None,    ):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)                # (batch, seq_len, 1) -&gt; (batch, seq_len)        start_logits = self.start_classifier(sequence_output).squeeze(-1)        end_logits = self.end_classifier(sequence_output).squeeze(-1)                # Mask padding tokens        start_logits = start_logits.masked_fill(~attention_mask.bool(), -1e9)        end_logits = end_logits.masked_fill(~attention_mask.bool(), -1e9)                loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss()            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }\n\n改进：联合 Start-End 预测class JointMRCModel(nn.Module):    \"\"\"联合预测 start 和 end，考虑 start-end 依赖\"\"\"        def __init__(self, model_name: str, max_answer_length: int = 30):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size        self.max_answer_length = max_answer_length                self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size * 2, 1)        def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        H = outputs.last_hidden_state  # (batch, seq_len, hidden)                # Start prediction        start_logits = self.start_classifier(H).squeeze(-1)                if self.training and start_positions is not None:            # 训练时使用真实的 start 位置            start_indices = start_positions.unsqueeze(-1).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)        else:            # 推理时使用预测的 start 位置            start_indices = start_logits.argmax(dim=-1, keepdim=True).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)                # End prediction conditioned on start        start_repr_expanded = start_repr.unsqueeze(1).expand(-1, H.size(1), -1)        end_input = torch.cat([H, start_repr_expanded], dim=-1)        end_logits = self.end_classifier(end_input).squeeze(-1)                # 只允许 end &gt;= start 且在 max_answer_length 范围内        # 这里简化处理，完整实现需要更复杂的 mask                return {'start_logits': start_logits, 'end_logits': end_logits}\n\n训练流程from torch.utils.data import DataLoader, Datasetfrom transformers import get_linear_schedule_with_warmupfrom tqdm import tqdmdef train(model, train_dataloader, val_dataloader, epochs=3, lr=3e-5):    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)        total_steps = len(train_dataloader) * epochs    scheduler = get_linear_schedule_with_warmup(        optimizer,         num_warmup_steps=int(0.1 * total_steps),        num_training_steps=total_steps    )        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model.to(device)        best_f1 = 0    for epoch in range(epochs):        model.train()        total_loss = 0                for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}'):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(**batch)            loss = outputs['loss']                        loss.backward()            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                        optimizer.step()            scheduler.step()            optimizer.zero_grad()                        total_loss += loss.item()                avg_loss = total_loss / len(train_dataloader)        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')                # Validation        f1 = evaluate(model, val_dataloader, device)        print(f'Validation F1: {f1:.4f}')                if f1 &gt; best_f1:            best_f1 = f1            torch.save(model.state_dict(), 'best_model.pt')        return model\n\n评估与推理import reimport stringfrom collections import Counterdef normalize_answer(s):    \"\"\"标准化答案用于评估\"\"\"    def remove_articles(text):        return re.sub(r'\\b(a|an|the)\\b', ' ', text)        def white_space_fix(text):        return ' '.join(text.split())        def remove_punc(text):        exclude = set(string.punctuation)        return ''.join(ch for ch in text if ch not in exclude)        def lower(text):        return text.lower()        return white_space_fix(remove_articles(remove_punc(lower(s))))def compute_f1(pred: str, gold: str) -&gt; float:    pred_tokens = normalize_answer(pred).split()    gold_tokens = normalize_answer(gold).split()        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        if num_same == 0:        return 0        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        return 2 * precision * recall / (precision + recall)def predict(model, tokenizer, context: str, question: str, device):    \"\"\"单条推理\"\"\"    model.eval()        encoding = tokenizer(        question, context,        max_length=384,        truncation='only_second',        return_tensors='pt'    )        encoding = {k: v.to(device) for k, v in encoding.items()}        with torch.no_grad():        outputs = model(**encoding)        start_idx = outputs['start_logits'].argmax().item()    end_idx = outputs['end_logits'].argmax().item()        # 确保 end &gt;= start    if end_idx &lt; start_idx:        end_idx = start_idx        # 解码答案    answer_tokens = encoding['input_ids'][0][start_idx:end_idx+1]    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)        return answer\n\n现代方法：使用 LLM对于更复杂的问答需求，可以使用 LLM：\nfrom openai import OpenAIdef llm_qa(context: str, question: str) -&gt; str:    client = OpenAI()        response = client.chat.completions.create(        model=\"gpt-4\",        messages=[            {\"role\": \"system\", \"content\": \"你是一个问答助手。根据给定的上下文回答问题。如果答案不在上下文中，请说'无法回答'。\"},            {\"role\": \"user\", \"content\": f\"上下文：{context}\\n\\n问题：{question}\"}        ],        temperature=0    )        return response.choices[0].message.content\n\n延伸阅读\nSQuAD Dataset\nHugging Face QA Pipeline\nNatural Questions\n\n\n\n转载请注明出处\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"条件随机场：原理与实现","url":"/2019/11/19/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0/","content":"条件随机场 (CRF) 是序列标注的经典模型，尽管深度学习时代 BERT 等模型大放异彩，CRF 层仍然在 NER、词性标注等任务中发挥关键作用。\n为什么需要 CRF？独立分类的问题如果对每个位置独立分类：\n\n会导致标签不一致，例如：\n输入: \"北 京 是 中 国 首 都\"错误: B-LOC I-PER O B-LOC I-LOC I-LOC I-LOC正确: B-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC\n\nCRF 的解决方案CRF 建模整个序列的联合概率，考虑标签之间的转移约束。\n数学原理条件概率\n其中：\n\n：发射分数（emission score）\n：转移分数（transition score）\n：配分函数（归一化项）\n\n配分函数\n直接计算复杂度为 ，使用前向算法可降至 。\nPyTorch 实现CRF Layerimport torchimport torch.nn as nnclass CRF(nn.Module):    def __init__(self, num_tags, batch_first=True):        super().__init__()        self.num_tags = num_tags        self.batch_first = batch_first                # 转移矩阵: transitions[i, j] = 从标签 j 转移到标签 i 的分数        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))                # 起始和结束转移        self.start_transitions = nn.Parameter(torch.randn(num_tags))        self.end_transitions = nn.Parameter(torch.randn(num_tags))        def forward(self, emissions, tags, mask=None):        \"\"\"计算负对数似然损失\"\"\"        if mask is None:            mask = torch.ones_like(tags, dtype=torch.bool)                if self.batch_first:            emissions = emissions.transpose(0, 1)            tags = tags.transpose(0, 1)            mask = mask.transpose(0, 1)                # 计算分子（正确路径的分数）        numerator = self._compute_score(emissions, tags, mask)                # 计算分母（配分函数）        denominator = self._compute_normalizer(emissions, mask)                # 负对数似然        return (denominator - numerator).mean()        def _compute_score(self, emissions, tags, mask):        \"\"\"计算给定标签序列的分数\"\"\"        seq_len, batch_size = tags.shape                # 起始分数        score = self.start_transitions[tags[0]]        score += emissions[0, torch.arange(batch_size), tags[0]]                for i in range(1, seq_len):            # 转移分数 + 发射分数            score += self.transitions[tags[i], tags[i-1]] * mask[i]            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]                # 结束分数        last_tag_idx = mask.sum(dim=0) - 1        last_tags = tags.gather(0, last_tag_idx.unsqueeze(0)).squeeze(0)        score += self.end_transitions[last_tags]                return score        def _compute_normalizer(self, emissions, mask):        \"\"\"前向算法计算配分函数\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # 初始化        score = self.start_transitions + emissions[0]                for i in range(1, seq_len):            # broadcast: (batch, num_tags, 1) + (num_tags, num_tags) + (batch, 1, num_tags)            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score = torch.logsumexp(next_score, dim=1)                        # 应用 mask            score = torch.where(mask[i].unsqueeze(1), next_score, score)                # 添加结束分数        score += self.end_transitions                return torch.logsumexp(score, dim=1)        def decode(self, emissions, mask=None):        \"\"\"Viterbi 解码\"\"\"        if mask is None:            mask = torch.ones(emissions.shape[:2], dtype=torch.bool, device=emissions.device)                if self.batch_first:            emissions = emissions.transpose(0, 1)            mask = mask.transpose(0, 1)                return self._viterbi_decode(emissions, mask)        def _viterbi_decode(self, emissions, mask):        \"\"\"Viterbi 算法\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # 初始化        score = self.start_transitions + emissions[0]        history = []                for i in range(1, seq_len):            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score, indices = next_score.max(dim=1)                        score = torch.where(mask[i].unsqueeze(1), next_score, score)            history.append(indices)                # 添加结束分数        score += self.end_transitions                # 回溯        best_tags_list = []        _, best_last_tag = score.max(dim=1)                for idx in range(batch_size):            best_tags = [best_last_tag[idx].item()]            seq_length = int(mask[:, idx].sum().item())                        for hist in reversed(history[:seq_length-1]):                best_last_tag_idx = best_tags[-1]                best_tags.append(hist[idx, best_last_tag_idx].item())                        best_tags.reverse()            best_tags_list.append(best_tags)                return best_tags_list\n\n与 BiLSTM 结合class BiLSTM_CRF(nn.Module):    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):        super().__init__()        self.embedding = nn.Embedding(vocab_size, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2,                            num_layers=2, bidirectional=True, batch_first=True)        self.fc = nn.Linear(hidden_dim, num_tags)        self.crf = CRF(num_tags)        def forward(self, x, tags, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf(emissions, tags, mask)        def predict(self, x, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf.decode(emissions, mask)\n\n现代应用：BERT + CRF尽管 BERT 已经很强大，但 CRF 层仍能带来一致性提升：\nfrom transformers import BertModelclass BERT_CRF(nn.Module):    def __init__(self, bert_name, num_tags):        super().__init__()        self.bert = BertModel.from_pretrained(bert_name)        self.dropout = nn.Dropout(0.1)        self.fc = nn.Linear(self.bert.config.hidden_size, num_tags)        self.crf = CRF(num_tags)        def forward(self, input_ids, attention_mask, tags=None):        outputs = self.bert(input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)        emissions = self.fc(sequence_output)                if tags is not None:            return self.crf(emissions, tags, attention_mask.bool())        else:            return self.crf.decode(emissions, attention_mask.bool())\n\n性能对比（CoNLL-2003 NER）\n\n\n模型\nF1\n\n\n\nBiLSTM\n88.2\n\n\nBiLSTM + CRF\n90.1\n\n\nBERT\n92.4\n\n\nBERT + CRF\n92.8\n\n\nRoBERTa + CRF\n93.2\n\n\n训练技巧1. 标签平滑def label_smoothing_loss(crf, emissions, tags, mask, epsilon=0.1):    \"\"\"带标签平滑的 CRF 损失\"\"\"    nll_loss = crf(emissions, tags, mask)        # 均匀分布的损失    uniform_loss = -torch.logsumexp(emissions, dim=-1).mean()        return (1 - epsilon) * nll_loss + epsilon * uniform_loss\n\n2. 约束解码# 添加硬约束：B-X 后面只能接 I-X 或 Odef add_constraints(transitions, tag2idx):    for tag_from, idx_from in tag2idx.items():        for tag_to, idx_to in tag2idx.items():            if tag_from.startswith('B-') or tag_from.startswith('I-'):                entity = tag_from[2:]                if tag_to.startswith('I-') and tag_to[2:] != entity:                    transitions.data[idx_to, idx_from] = -1e9\n\n延伸阅读\nLafferty et al., Conditional Random Fields (2001)\nHuang et al., Bidirectional LSTM-CRF Models for Sequence Tagging (2015)\npytorch-crf Documentation\n\n\n\n转载请注明出处\n\n","tags":["NLP","machine learning","CRF"]},{"title":"MRC 模型实现：从 TensorFlow 到 PyTorch","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E5%8E%BB%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%87%E6%9C%AC%E5%B9%B6%E4%B8%94%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%EF%BC%88tensorflow%E5%AE%9E%E6%88%98%EF%BC%89/","content":"本文介绍机器阅读理解模型的完整实现，涵盖经典架构和现代最佳实践。\n问题定义输入：\n\n问题 \n文档 \n\n输出：\n\n答案起始位置 \n答案结束位置 \n\n经典架构Input → Embedding → Encoding → Matching → Fusion → Decoding\n\n各层详解\n\n\n层\n功能\n现代替代\n\n\n\nEmbedding\nToken → Vector\nSubword Tokenization\n\n\nEncoding\n序列编码\nTransformer Encoder\n\n\nMatching\nQ-P 交互\nCross-Attention\n\n\nFusion\n信息融合\nSelf-Attention\n\n\nDecoding\nSpan 预测\nLinear + Softmax\n\n\nPyTorch 实现完整模型import torchimport torch.nn as nnimport torch.nn.functional as Ffrom transformers import AutoModel, AutoTokenizerclass MRCModel(nn.Module):    \"\"\"基于 Transformer 的 MRC 模型\"\"\"        def __init__(        self,         model_name: str = \"bert-base-chinese\",        dropout: float = 0.1,        max_answer_length: int = 30    ):        super().__init__()        self.encoder = AutoModel.from_pretrained(model_name)        hidden_size = self.encoder.config.hidden_size        self.max_answer_length = max_answer_length                self.dropout = nn.Dropout(dropout)        self.start_fc = nn.Linear(hidden_size, 1)        self.end_fc = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        token_type_ids: torch.Tensor = None,        start_positions: torch.Tensor = None,        end_positions: torch.Tensor = None,    ):        # 编码        outputs = self.encoder(            input_ids=input_ids,            attention_mask=attention_mask,            token_type_ids=token_type_ids,        )        sequence_output = self.dropout(outputs.last_hidden_state)                # 预测 start/end        start_logits = self.start_fc(sequence_output).squeeze(-1)        end_logits = self.end_fc(sequence_output).squeeze(-1)                # Mask padding        mask = attention_mask.bool()        start_logits = start_logits.masked_fill(~mask, float('-inf'))        end_logits = end_logits.masked_fill(~mask, float('-inf'))                # 计算损失        loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }        def decode(        self,        start_logits: torch.Tensor,        end_logits: torch.Tensor,        attention_mask: torch.Tensor,    ):        \"\"\"解码最佳答案 span\"\"\"        batch_size, seq_len = start_logits.shape                # 计算所有有效 (start, end) 对的分数        start_probs = F.softmax(start_logits, dim=-1)        end_probs = F.softmax(end_logits, dim=-1)                results = []        for b in range(batch_size):            best_score = float('-inf')            best_start, best_end = 0, 0                        for start in range(seq_len):                if not attention_mask[b, start]:                    continue                for end in range(start, min(start + self.max_answer_length, seq_len)):                    if not attention_mask[b, end]:                        continue                    score = start_probs[b, start] * end_probs[b, end]                    if score &gt; best_score:                        best_score = score                        best_start, best_end = start, end                        results.append((best_start, best_end))                return results\n\n数据处理from dataclasses import dataclassfrom typing import List, Optionalimport json@dataclassclass MRCExample:    qid: str    question: str    context: str    answer: Optional[str] = None    start_position: Optional[int] = None@dataclassclass MRCFeature:    input_ids: List[int]    attention_mask: List[int]    token_type_ids: List[int]    start_position: int    end_position: int    offset_mapping: List[tuple]class MRCProcessor:    def __init__(self, model_name: str, max_length: int = 512):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        def process(self, example: MRCExample) -&gt; MRCFeature:        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            return_offsets_mapping=True,            padding='max_length',        )                # 定位答案位置        start_token, end_token = 0, 0        if example.start_position is not None:            offset = encoding['offset_mapping']            for idx, (start, end) in enumerate(offset):                if start &lt;= example.start_position &lt; end:                    start_token = idx                if start &lt; example.start_position + len(example.answer) &lt;= end:                    end_token = idx                    break                return MRCFeature(            input_ids=encoding['input_ids'],            attention_mask=encoding['attention_mask'],            token_type_ids=encoding.get('token_type_ids', [0] * len(encoding['input_ids'])),            start_position=start_token,            end_position=end_token,            offset_mapping=encoding['offset_mapping'],        )\n\n训练循环from torch.utils.data import DataLoaderfrom torch.optim import AdamWfrom transformers import get_schedulerfrom tqdm import tqdmdef train_epoch(model, dataloader, optimizer, scheduler, device):    model.train()    total_loss = 0        for batch in tqdm(dataloader, desc=\"Training\"):        batch = {k: v.to(device) for k, v in batch.items()}                outputs = model(**batch)        loss = outputs['loss']                loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()        scheduler.step()        optimizer.zero_grad()                total_loss += loss.item()        return total_loss / len(dataloader)def evaluate(model, dataloader, device):    model.eval()    predictions = []        with torch.no_grad():        for batch in tqdm(dataloader, desc=\"Evaluating\"):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(                input_ids=batch['input_ids'],                attention_mask=batch['attention_mask'],                token_type_ids=batch.get('token_type_ids'),            )                        spans = model.decode(                outputs['start_logits'],                outputs['end_logits'],                batch['attention_mask'],            )            predictions.extend(spans)        return predictions# 主训练流程def main():    # 配置    model_name = \"bert-base-chinese\"    batch_size = 16    learning_rate = 3e-5    num_epochs = 3        # 初始化    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model = MRCModel(model_name).to(device)        # 优化器    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)    scheduler = get_scheduler(        \"linear\",        optimizer=optimizer,        num_warmup_steps=500,        num_training_steps=num_epochs * len(train_dataloader),    )        # 训练    for epoch in range(num_epochs):        loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")                # 验证        predictions = evaluate(model, val_dataloader, device)        f1 = compute_f1(predictions, val_labels)        print(f\"Validation F1: {f1:.4f}\")\n\n评估指标import reimport stringfrom collections import Counterdef normalize_answer(s: str) -&gt; str:    \"\"\"标准化答案文本\"\"\"    s = s.lower()    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)    s = ''.join(ch for ch in s if ch not in string.punctuation)    s = ' '.join(s.split())    return sdef compute_f1(prediction: str, ground_truth: str) -&gt; float:    pred_tokens = normalize_answer(prediction).split()    gold_tokens = normalize_answer(ground_truth).split()        if not pred_tokens or not gold_tokens:        return int(pred_tokens == gold_tokens)        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        if precision + recall == 0:        return 0        return 2 * precision * recall / (precision + recall)def compute_em(prediction: str, ground_truth: str) -&gt; float:    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\n与现代方法对比\n\n\n方面\n经典 MRC (BiDAF)\nBERT-based\nLLM-based\n\n\n\n参数量\n~2M\n110M-340M\n7B-70B+\n\n\n训练数据\nTask-specific\n预训练+微调\n大规模预训练\n\n\n推理方式\nSpan extraction\nSpan extraction\nGeneration\n\n\n长文档\n需要切分\n需要切分\n更大上下文窗口\n\n\n多跳推理\n困难\n有限\n较好\n\n\n生产环境优化量化推理import torch.quantization as quant# 动态量化model_int8 = quant.quantize_dynamic(    model.cpu(),    {nn.Linear},    dtype=torch.qint8)\n\nONNX 导出import torch.onnxdummy_input = {    'input_ids': torch.ones(1, 512, dtype=torch.long),    'attention_mask': torch.ones(1, 512, dtype=torch.long),    'token_type_ids': torch.zeros(1, 512, dtype=torch.long),}torch.onnx.export(    model,    (dummy_input['input_ids'], dummy_input['attention_mask'], dummy_input['token_type_ids']),    \"mrc_model.onnx\",    input_names=['input_ids', 'attention_mask', 'token_type_ids'],    output_names=['start_logits', 'end_logits'],    dynamic_axes={        'input_ids': {0: 'batch', 1: 'seq'},        'attention_mask': {0: 'batch', 1: 'seq'},        'token_type_ids': {0: 'batch', 1: 'seq'},    })\n\n延伸阅读\nHugging Face Question Answering\nSQuAD Leaderboard\nCMRC 2018 中文阅读理解\n\n\n\n转载请注明出处\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"机器阅读理解：从传统方法到大语言模型","url":"/2019/10/03/%E5%BD%93%E6%88%91%E4%BB%AC%E6%8A%8A%E7%9B%AE%E5%85%89%E6%94%BE%E5%9C%A8%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","content":"\n核心问题：当我们期望机器”理解”文本时，我们的期望到底是什么？\n\n机器阅读理解的演进传统 MRC (2015-2019)基于 span extraction 的方法：\n输入: Context + Question输出: (start_idx, end_idx)\n\n代表模型：BiDAF, R-Net, QANet, BERT\nLLM 时代的 MRC (2020-至今)从”抽取”到”生成”的范式转变：\n输入: Context + Question + Instruction输出: 自由形式的答案\n\n任务分类与难度\n\n\n类型\n传统方法\nLLM 方法\n难度\n\n\n\n抽取式\n✅ 擅长\n✅ 擅长\n⭐\n\n\n多跳推理\n❌ 困难\n⚠️ 有限\n⭐⭐⭐\n\n\n数值推理\n❌ 几乎不能\n⚠️ 需要 CoT\n⭐⭐⭐⭐\n\n\n常识推理\n❌ 不能\n✅ 较好\n⭐⭐⭐\n\n\n开放生成\n❌ 不能\n✅ 擅长\n⭐⭐\n\n\n现代方法：RAG检索增强生成 (Retrieval-Augmented Generation) 结合了检索和生成的优势：\nclass RAGSystem:    def __init__(self, retriever, generator):        self.retriever = retriever  # e.g., Dense Retriever        self.generator = generator  # e.g., LLM        def answer(self, question: str) -&gt; str:        # 1. 检索相关文档        docs = self.retriever.retrieve(question, top_k=5)                # 2. 构建上下文        context = \"\\n\\n\".join([d.text for d in docs])                # 3. 生成答案        prompt = f\"\"\"基于以下文档回答问题：{context}问题：{question}答案：\"\"\"                return self.generator.generate(prompt)\n\n检索器选择\n\n\n检索器\n特点\n适用场景\n\n\n\nBM25\n关键词匹配，快速\n短查询，精确匹配\n\n\nDense Retriever\n语义匹配\n语义相似查询\n\n\nColBERT\n延迟交互\n平衡效率与效果\n\n\nHybrid\n结合稀疏+稠密\n生产环境\n\n\nChain-of-Thought 推理对于需要推理的问题，CoT prompting 显著提升效果：\n# 标准 Promptingprompt_standard = \"Q: 小明有5个苹果，给了小红2个，还剩几个？\\nA:\"# Chain-of-Thought Prompting  prompt_cot = \"\"\"Q: 小明有5个苹果，给了小红2个，还剩几个？A: 让我们一步步思考：1. 小明最初有 5 个苹果2. 他给了小红 2 个苹果3. 剩余苹果数 = 5 - 2 = 3答案是 3 个苹果。\"\"\"\n\n评估指标传统指标\n𝟙\nLLM 时代的指标# 使用 LLM 作为评估器def llm_evaluate(question, gold_answer, pred_answer):    prompt = f\"\"\"评估预测答案的质量（1-5分）：问题：{question}标准答案：{gold_answer}预测答案：{pred_answer}评分标准：5分 - 完全正确且信息完整4分 - 基本正确，略有遗漏3分 - 部分正确2分 - 有相关信息但不正确1分 - 完全错误分数：\"\"\"    return llm.generate(prompt)\n\n实践建议何时用传统 MRC\n答案明确在文档中\n需要精确的位置标注\n低延迟要求\n资源受限\n\n何时用 RAG + LLM\n需要整合多个文档\n答案需要推理或总结\n开放域问答\n用户期望自然语言回答\n\n代码示例：现代 RAG 系统from langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.chat_models import ChatOpenAIfrom langchain.chains import RetrievalQA# 初始化组件embeddings = OpenAIEmbeddings()vectorstore = FAISS.load_local(\"my_index\", embeddings)llm = ChatOpenAI(model=\"gpt-4\", temperature=0)# 创建 RAG 链qa_chain = RetrievalQA.from_chain_type(    llm=llm,    chain_type=\"stuff\",  # 或 \"map_reduce\", \"refine\"    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),    return_source_documents=True)# 使用result = qa_chain({\"query\": \"什么是机器阅读理解？\"})print(result[\"result\"])\n\n延伸阅读\nSQuAD 2.0\nNatural Questions\nRAG Paper\nLangChain Documentation\n\n\n\n转载请注明出处\n\n","tags":["LLM","MRC","Deep learning","RAG"]},{"title":"LLM 游戏智能体论文解读：总览","url":"/2025/12/28/LLM-Game-Agents-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-%E6%80%BB%E8%A7%88/","content":"本系列是 LLM 驱动的游戏智能体领域核心论文的解读与总结，涵盖 103+ 篇论文，164 条引用关系的系统性分析。\n\n领域概述随着大型语言模型（LLM）的快速发展，研究者们开始探索将 LLM 作为智能体”大脑”的可能性。这些智能体不仅能理解和生成文本，还能规划、反思、与环境交互，甚至形成复杂的社会行为。\n┌─────────────────────────────────────────────────────────────┐│                    LLM 游戏智能体技术栈                       │├─────────────────────────────────────────────────────────────┤│                                                             ││   ┌─────────────────┐                                       ││   │   应用层         │  游戏/模拟/机器人                       ││   └────────┬────────┘                                       ││            │                                                ││   ┌────────▼────────┐                                       ││   │   智能体框架     │  ReAct / Reflexion / VOYAGER          ││   └────────┬────────┘                                       ││            │                                                ││   ┌────────▼────────┐                                       ││   │   核心能力       │  记忆 / 规划 / 反思 / 工具使用          ││   └────────┬────────┘                                       ││            │                                                ││   ┌────────▼────────┐                                       ││   │   基础模型       │  GPT-4 / Claude / Llama               ││   └─────────────────┘                                       ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n\n核心论文引用关系基于 103 篇论文的引用网络分析，以下是领域内最具影响力的基础性工作：\n\n\n\n排名\n论文\n会议\n被引用\n核心贡献\n\n\n\n1\nReAct\nICLR 2023\n32\n推理+行动交替范式\n\n\n2\nGenerative Agents\nUIST 2023\n20\n记忆-反思-规划架构\n\n\n3\nReflexion\nNeurIPS 2023\n17\n语言反馈强化学习\n\n\n4\nVOYAGER\nNeurIPS 2023\n-\n技能库+终身学习\n\n\n技术层次金字塔                ┌─────────────────────┐                │   🎯 上层应用       │                │  竞技/社交/特定游戏  │                │  (Werewolf, Poker,  │                │   StarCraft等)      │                └──────────┬──────────┘                           │          ┌────────────────┼────────────────┐          │                │                │┌─────────▼─────────┐ ┌────▼────┐ ┌────────▼────────┐│  🏗️ 中间层       │ │模拟仿真 │ │  🤝 多智能体    ││  环境适配层      │ │         │ │  协作层         ││ (Crafter,       │ │Generative│ │                ││  Minecraft)     │ │ Agents  │ │                │└─────────┬───────┘ └────┬────┘ └────────┬───────┘          │              │               │          └──────────────┼───────────────┘                         │                ┌────────▼────────┐                │  🔧 基础框架层   │                │                 │                │  • ReAct        │ ← 推理+行动范式                │  • Reflexion    │ ← 自我反思机制                │  • Grounding RL │ ← 环境交互学习                └─────────────────┘\n\n\n系列文章目录基础框架篇\n\n\n文章\n核心内容\n\n\n\n基础框架：ReAct / Reflexion / Generative Agents\n三大核心框架的详细对比分析\n\n\n应用扩展篇\n\n\n文章\n核心内容\n\n\n\n应用扩展：VOYAGER / Project Sid / Agent Hospital\n终身学习、AI文明、医疗智能体\n\n\n\n研究脉络时间线2023年：基础奠定\n\n\n时间\n论文\n会议\n核心贡献\n\n\n\n2022/10\nReAct\nICLR 2023\n推理与行动协同范式\n\n\n2023/03\nReflexion\nNeurIPS 2023\n语言反馈强化学习\n\n\n2023/04\nGenerative Agents\nUIST 2023\n25智能体小镇模拟\n\n\n2023/05\nVOYAGER\nNeurIPS 2023\nMinecraft终身学习\n\n\n2024年：深度发展\n\n\n时间\n论文\n核心贡献\n\n\n\n2024/05\nAgent Hospital\n可进化医疗智能体\n\n\n2024/10\nProject Sid\n500-1000+智能体文明模拟\n\n\n2024/10\nClaude Computer Use\n商业级计算机控制\n\n\n2025年：产业化\n\n\n时间\n趋势\n代表产品\n\n\n\n2025\nAgent OS化\nAutoGen, LangGraph\n\n\n2025\n商业化加速\nOpenAI Operator\n\n\n2025\n多模态融合\n视觉+语言+行动\n\n\n\n游戏类型与论文分布\n\n\n游戏类型\n论文数\n代表论文\n\n\n\n文字冒险\n22\nReAct, Reflexion, ALFWorld\n\n\nMinecraft\n15\nVOYAGER, GITM, JARVIS-1\n\n\n社会模拟\n12\nGenerative Agents, Project Sid\n\n\n竞技游戏\n15\nPokéLLMon, StarCraft II\n\n\n合作游戏\n7\nCo-LLM-Agents, TeamCraft\n\n\n对话游戏\n16\nWerewolf, Avalon\n\n\n\n核心技术对比记忆机制\n\n\n方法\n存储内容\n检索方式\n特点\n\n\n\nVOYAGER\n可执行代码\n语义相似度\n技能可复用\n\n\nGenerative Agents\n自然语言\n时近性+重要性+相关性\n多层抽象\n\n\nReflexion\n语言化反思\n时间顺序\n失败学习\n\n\n反思机制\n\n\n方法\n触发条件\n输出\n目的\n\n\n\nVOYAGER\n每轮执行后\n成功/失败+批评\n任务验证\n\n\nGenerative Agents\n重要性&gt;150\n高层次洞察\n概念抽象\n\n\nReflexion\n每次失败后\n详细反思\n错误诊断\n\n\n学习方式\n\n\n方法\n是否微调\n知识形式\n学习目标\n\n\n\n传统RL\n✅ 梯度更新\n策略网络\n奖励最大化\n\n\nVOYAGER\n❌ 提示工程\n代码技能库\n技能积累\n\n\nReflexion\n❌ 语言强化\n反思记忆\n任务成功率\n\n\n\n关键洞见1. 无需微调的力量三大核心框架（ReAct、Reflexion、Generative Agents）都证明：仅通过提示工程和运行时机制，无需微调模型参数，就能实现复杂的智能体行为。\n2. 记忆是关键有效的记忆机制是智能体成功的基础：\n\nVOYAGER：代码即记忆，技能可复用\nGenerative Agents：记忆即人格，反思即成长\nReflexion：反思即学习，失败即进步\n\n3. 协同优于孤立\n\n\n单一能力\n协同能力\n\n\n\n仅推理 → 幻觉严重\n推理+行动 → ReAct\n\n\n仅行动 → 无法规划\n行动+反思 → Reflexion\n\n\n单智能体 → 能力有限\n多智能体 → 涌现社会行为\n\n\n4. 规模带来涌现\n\n\n规模\n涌现现象\n\n\n\n25 智能体\n社交行为、信息传播 (Generative Agents)\n\n\n50 智能体\n长期关系、角色分化 (Project Sid)\n\n\n500+ 智能体\n文化传播、宗教涌现 (Project Sid)\n\n\n\n实践建议场景匹配\n\n\n场景\n推荐方法\n原因\n\n\n\n开放世界游戏\nVOYAGER\n技能可复用、可组合\n\n\n社会模拟\nGenerative Agents\n丰富记忆和人格一致性\n\n\n决策任务\nReflexion\n失败反思对决策优化关键\n\n\n医疗/专业领域\nAgent Hospital\n可进化的专业智能体\n\n\n组合架构理想的智能体应该结合三者优势：\n理想架构 = Generative Agents的记忆流         + VOYAGER的技能库         + Reflexion的失败反思\n\n\n工业趋势主要玩家\n\n\n公司\n产品\n核心能力\n\n\n\nOpenAI\nGPT-4V Agent, Operator\n通用Agent能力\n\n\nAnthropic\nClaude Computer Use\n计算机自主控制\n\n\nMicrosoft\nAutoGen 0.4\n企业级多Agent框架\n\n\nAltera AI\nProject Sid\nAI文明模拟\n\n\n开源生态\n\n\n框架\n定位\n热度\n\n\n\nAutoGen\n多Agent对话与协作\n🔥🔥🔥\n\n\nLangGraph\n状态机Agent工作流\n🔥🔥🔥\n\n\nMetaGPT\n多角色软件开发\n🔥🔥\n\n\nCrewAI\n角色扮演Agent团队\n🔥🔥\n\n\n\n参考资源论文列表\nawesome-LLM-game-agent-papers\nA Survey on Large Language Model-Based Game Agents\n\n代码仓库\n\n\n论文\n代码\n\n\n\nReAct\ngithub.com/ysymyth/ReAct\n\n\nReflexion\ngithub.com/noahshinn024/reflexion\n\n\nGenerative Agents\ngithub.com/joonspk-research/generative_agents\n\n\nVOYAGER\nvoyager.minedojo.org\n\n\n\n下一篇：基础框架篇 | 应用扩展篇\n","categories":["论文解读"],"tags":["LLM","Agent","论文解读","游戏AI","多智能体"]},{"title":"矩阵分解：从 SVD 到现代 AI 应用","url":"/2019/10/03/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B9%8B%E4%B8%80%EF%BC%9ATruncate-SVD-%E5%92%8Crandom-SVD/","content":"矩阵分解是机器学习的基石技术，从传统的推荐系统到现代大语言模型的参数高效微调（LoRA），都离不开矩阵分解的思想。\n奇异值分解 (SVD)基本形式任意矩阵  可以分解为：\n\n其中：\n\n：左奇异向量（正交矩阵）\n：奇异值对角矩阵\n：右奇异向量（正交矩阵）\n\nTruncated SVD保留前  个最大奇异值：\n\n这是最优的秩  近似（Eckart-Young 定理）：\n\nRandomized SVD当矩阵规模巨大时，精确 SVD 计算代价过高。Randomized SVD 提供了高效的近似方法。\n算法实现import numpy as npfrom scipy import linalgdef randomized_svd(A, n_components, n_oversamples=10, n_iter=4):    \"\"\"    Randomized SVD for large matrices.        Args:        A: Input matrix (m x n)        n_components: Number of singular values to compute        n_oversamples: Additional random vectors for accuracy        n_iter: Number of power iterations        Returns:        U, s, Vt: Truncated SVD components    \"\"\"    m, n = A.shape    n_random = n_components + n_oversamples        # Step 1: Random projection    Q = np.random.randn(n, n_random)        # Step 2: Power iteration for accuracy    for _ in range(n_iter):        Q, _ = linalg.lu(A @ Q, permute_l=True)        Q, _ = linalg.lu(A.T @ Q, permute_l=True)        Q, _ = linalg.qr(A @ Q, mode='economic')        # Step 3: Project and compute SVD    B = Q.T @ A    Uhat, s, Vt = linalg.svd(B, full_matrices=False)    U = Q @ Uhat        return U[:, :n_components], s[:n_components], Vt[:n_components, :]\n\n复杂度对比\n\n\n方法\n时间复杂度\n空间复杂度\n\n\n\n精确 SVD\n\n\n\n\nRandomized SVD\n\n\n\n\nTruncated SVD (Lanczos)\n\n\n\n\n现代应用：LoRALoRA (Low-Rank Adaptation) 是大语言模型参数高效微调的核心技术，直接利用了低秩分解的思想。\nLoRA 原理预训练权重  固定，只训练低秩增量：\n\n其中 ，，。\n实现示例import torchimport torch.nn as nnclass LoRALayer(nn.Module):    def __init__(self, in_features, out_features, rank=4, alpha=1.0):        super().__init__()        self.rank = rank        self.alpha = alpha                # 原始权重（冻结）        self.W = nn.Linear(in_features, out_features, bias=False)        self.W.weight.requires_grad = False                # 低秩分解        self.A = nn.Linear(in_features, rank, bias=False)        self.B = nn.Linear(rank, out_features, bias=False)                # 初始化        nn.init.kaiming_uniform_(self.A.weight)        nn.init.zeros_(self.B.weight)                self.scaling = alpha / rank        def forward(self, x):        # W(x) + scaling * B(A(x))        return self.W(x) + self.scaling * self.B(self.A(x))\n\n参数效率对于 LLaMA-7B：\n\n\n\n方法\n可训练参数\n显存占用\n\n\n\n全量微调\n7B (100%)\n~140GB\n\n\nLoRA (r=8)\n4.7M (0.07%)\n~14GB\n\n\nLoRA (r=16)\n9.4M (0.13%)\n~16GB\n\n\n其他应用1. 推荐系统矩阵分解用于协同过滤：\n\n# 使用 surprise 库from surprise import SVD, Dataset, Readerreader = Reader(rating_scale=(1, 5))data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)model = SVD(n_factors=100)model.fit(trainset)\n\n2. 文本表示 (LSA)潜在语义分析：\nfrom sklearn.decomposition import TruncatedSVDfrom sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(max_features=10000)X = vectorizer.fit_transform(documents)svd = TruncatedSVD(n_components=100)X_reduced = svd.fit_transform(X)\n\n3. 图像压缩from PIL import Imageimport numpy as npdef compress_image(image_path, n_components=50):    img = np.array(Image.open(image_path).convert('L'))    U, s, Vt = np.linalg.svd(img, full_matrices=False)        # 保留前 n_components 个奇异值    compressed = U[:, :n_components] @ np.diag(s[:n_components]) @ Vt[:n_components, :]        return compressed.astype(np.uint8)\n\n数值稳定性条件数\n条件数过大会导致数值不稳定。\n正则化 SVDdef regularized_svd(A, lambda_reg=0.01):    \"\"\"Add regularization for numerical stability.\"\"\"    U, s, Vt = np.linalg.svd(A, full_matrices=False)    s_reg = s / (s**2 + lambda_reg)    return U, s_reg, Vt\n\n延伸阅读\nHalko et al., Finding Structure with Randomness (2011)\nHu et al., LoRA: Low-Rank Adaptation of Large Language Models (2021)\nNumPy SVD Documentation\n\n\n\n转载请注明出处\n\n","tags":["LLM","machine learning","linear algebra"]},{"title":"开篇","url":"/2019/10/03/%E5%BC%80%E7%AF%87/","content":"各位读者朋友们大家好，我是 fooSynaptic。\n欢迎来到我的技术博客！这里记录我在 AI 和 NLP 领域的学习与思考。\n关于这个博客这个博客主要记录以下内容：\n\n自然语言处理 (NLP)：从传统方法到大语言模型\n机器学习：算法原理与实现细节\n深度学习：模型架构与训练技巧\n数学基础：线性代数、概率论、优化理论\n工程实践：Python、PyTorch、分布式训练\n\n技术栈NLP: Transformers, LLMs, RAG, Prompt EngineeringML: PyTorch, JAX, scikit-learnInfra: CUDA, Triton, vLLM, DeepSpeed\n\n关于我NLP Researcher，专注于：\n\n大语言模型 (LLM) 训练与推理优化\n检索增强生成 (RAG)\n机器阅读理解与问答系统\n\nGitHub: fooSynaptic\n\n\n欢迎交流讨论，转载请注明出处\n\n","tags":["Introduction"]},{"title":"BiDAF 论文解读：双向注意力流机制","url":"/2019/11/19/%E8%AE%BA%E6%96%87%E6%A2%97%E6%A6%82%EF%BC%9ABi-Directional-Attention-Flow-for-Machine-Comprehension/","content":"BiDAF (Bi-Directional Attention Flow) 是机器阅读理解领域的经典模型，其双向注意力机制对后续 Transformer 架构产生了深远影响。\n核心创新1. Memory-less Attention传统动态注意力 vs BiDAF 的无记忆注意力：\n\n\n\n特性\nDynamic Attention\nMemory-less Attention\n\n\n\n依赖\n前一时间步的 attended vector\n仅当前 query 和 context\n\n\n优势\n可建模时序依赖\n避免错误累积\n\n\n缺点\n错误会传播\n无法建模长程依赖\n\n\n2. 双向注意力同时计算：\n\nContext-to-Query (C2Q)：每个 context 词最相关的 query 词\nQuery-to-Context (Q2C)：对回答问题最关键的 context 词\n\n模型架构Input → Embedding → Encoding → Attention → Modeling → Output  │         │           │          │           │         │ 词向量    字符CNN     BiLSTM    双向注意力   BiLSTM   Span预测\n\n数学表达相似度矩阵：\n\n其中  是 context 表示， 是 query 表示。\nC2Q Attention：\n$$\\tilde{U}i = \\sum_j a{ij} U_j, \\quad a_i = \\text{softmax}(S_i)$$\nQ2C Attention：\n\n融合表示：\n\nPyTorch 实现import torchimport torch.nn as nnclass BiDAFAttention(nn.Module):    def __init__(self, hidden_size):        super().__init__()        self.W = nn.Linear(hidden_size * 3, 1, bias=False)        def forward(self, context, query, c_mask, q_mask):        \"\"\"        Args:            context: (batch, c_len, hidden)            query: (batch, q_len, hidden)            c_mask: (batch, c_len)            q_mask: (batch, q_len)        \"\"\"        batch, c_len, hidden = context.size()        q_len = query.size(1)                # 扩展维度以计算所有 (i, j) 对        c_expand = context.unsqueeze(2).expand(-1, -1, q_len, -1)        q_expand = query.unsqueeze(1).expand(-1, c_len, -1, -1)                # 计算相似度矩阵 S        cq = torch.cat([c_expand, q_expand, c_expand * q_expand], dim=-1)        S = self.W(cq).squeeze(-1)  # (batch, c_len, q_len)                # Mask        q_mask_expand = q_mask.unsqueeze(1).expand(-1, c_len, -1)        S = S.masked_fill(~q_mask_expand, -1e9)                # C2Q attention        a = torch.softmax(S, dim=-1)        c2q = torch.bmm(a, query)  # (batch, c_len, hidden)                # Q2C attention        b = torch.softmax(S.max(dim=-1)[0], dim=-1)        q2c = torch.bmm(b.unsqueeze(1), context)  # (batch, 1, hidden)        q2c = q2c.expand(-1, c_len, -1)                # 融合        G = torch.cat([context, c2q, context * c2q, context * q2c], dim=-1)                return G\n\n与 Transformer 的对比\n\n\n特性\nBiDAF\nTransformer\n\n\n\n注意力方向\n双向（C2Q, Q2C）\n全方向自注意力\n\n\n位置编码\nBiLSTM 隐式编码\n显式位置编码\n\n\n并行化\n受限于 RNN\n完全并行\n\n\n长距离依赖\n受限\n理论上无限\n\n\n参数量\n较少\n较多\n\n\n现代演进BiDAF 的思想在现代模型中的体现：\n1. Cross-Attention in Transformerclass CrossAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.mha = nn.MultiheadAttention(d_model, n_heads)        def forward(self, query, key_value):        # query 来自一个序列，key/value 来自另一个序列        return self.mha(query, key_value, key_value)\n\n2. FiD (Fusion-in-Decoder)用于 RAG 的架构，类似 BiDAF 的融合思想：\nclass FiD(nn.Module):    def __init__(self, encoder, decoder):        super().__init__()        self.encoder = encoder        self.decoder = decoder        def forward(self, question, passages):        # 独立编码每个 passage        encoded = []        for passage in passages:            enc = self.encoder(question + passage)            encoded.append(enc)                # 融合解码        fused = torch.cat(encoded, dim=1)        return self.decoder(fused)\n\n实验结果（原论文）在 SQuAD 1.1 上的表现：\n\n\n\n模型\nEM\nF1\n\n\n\nBiDAF\n67.7\n77.3\n\n\nBiDAF + Self Attention\n72.1\n81.1\n\n\nBERT-base\n80.8\n88.5\n\n\nGPT-4 (few-shot)\n~90\n~95\n\n\n延伸阅读\nBiDAF Paper\nAttention Is All You Need\nBERT for QA\n\n\n\n转载请注明出处\n\n","tags":["MRC","attention","deep learning"]}]