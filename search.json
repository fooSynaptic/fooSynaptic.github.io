[{"title":"DDIA Part 1: 数据系统基础","url":"/2025/12/28/DDIA-Part1-%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/","content":"本文是 DDIA 第一部分的完整读书笔记，涵盖第 1-4 章：可靠性与可扩展性、数据模型、存储引擎、数据编码。\n\n第1章：可靠性、可扩展性与可维护性1.1 数据密集型应用的组成现代数据密集型应用通常由多个组件组合：\n┌─────────────────────────────────────────────────────────────┐│                     数据密集型应用架构                        │├─────────────────────────────────────────────────────────────┤│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐     ││  │   数据库     │    │    缓存      │    │   搜索索引   │     ││  └─────────────┘    └─────────────┘    └─────────────┘     ││  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐     ││  │  流处理     │    │   批处理     │    │  消息队列    │     ││  └─────────────┘    └─────────────┘    └─────────────┘     │└─────────────────────────────────────────────────────────────┘\n\n\n\n\n类型\n特点\n主要瓶颈\n典型场景\n\n\n\n数据密集型\n数据量大、复杂、变化快\n磁盘I/O、网络带宽\nWeb应用、社交网络\n\n\n计算密集型\n计算量大、算法复杂\nCPU处理能力\n科学计算、图形渲染\n\n\n1.2 可靠性（Reliability）\n可靠性：系统在面对故障时，仍能正确运行\n\n\n\n\n术语\n英文\n定义\n\n\n\n故障\nFault\n系统中某个组件偏离其规格\n\n\n失效\nFailure\n整个系统停止为用户提供服务\n\n\n容错\nFault-tolerant\n系统能够处理某些类型的故障\n\n\n故障类型及应对1. 硬件故障\n\n硬盘损坏、内存故障、电源中断\n应对：RAID、双电源、热备份、多副本\n\n2. 软件错误\n\n比硬件故障更难预测，可能导致系统性问题\n应对：全面测试、进程隔离、监控告警\n\n3. 人为错误\n\n研究表明，运维人员的配置错误是系统中断的首要原因\n应对：良好的API设计、沙箱环境、快速回滚\n\n1.3 可扩展性（Scalability）\n可扩展性：系统应对负载增长的能力\n\n描述性能：百分位数\n\n\n百分位\n含义\n用途\n\n\n\np50\n中位数\n典型响应时间\n\n\np95\n95%的请求快于此值\n较高标准\n\n\np99\n99%的请求快于此值\nSLA标准\n\n\nTwitter 案例：推拉模式\n\n\n方案\n描述\n优点\n缺点\n\n\n\n拉模式\n读取时间线时查询关注者的推文\n写入简单\n读取开销大\n\n\n推模式\n发推时写入所有关注者的缓存\n读取快速\n写入开销大（大V问题）\n\n\n混合模式\n普通用户推模式，大V拉模式\n平衡读写\n实现复杂\n\n\n扩展策略\n纵向扩展：使用更强大的机器（简单但有上限）\n横向扩展：使用多台普通机器（需要复杂设计）\n弹性扩展：根据负载自动增减资源\n\n1.4 可维护性（Maintainability）\n软件的大部分成本不在于最初的开发，而在于后续的维护\n\n\n\n\n方面\n目标\n\n\n\n可操作性\n让运维团队能够轻松保持系统运行\n\n\n简单性\n让新工程师能够轻松理解系统\n\n\n可演化性\n让工程师能够轻松对系统进行修改\n\n\n\n第2章：数据模型与查询语言2.1 数据模型分层┌─────────────────────────────────────────────┐│         应用程序层（对象、数据结构、API）        │├─────────────────────────────────────────────┤│         数据模型层（表、文档、图）              │├─────────────────────────────────────────────┤│         存储层（字节序列、内存、磁盘）           │├─────────────────────────────────────────────┤│         硬件层（电信号、磁场）                  │└─────────────────────────────────────────────┘\n\n2.2 关系模型\n\n\n概念\n描述\n示例\n\n\n\n关系（Relation）\n表\nusers表\n\n\n元组（Tuple）\n行/记录\n一个用户记录\n\n\n属性（Attribute）\n列\nname, email, age\n\n\n-- 规范化设计：使用外键引用CREATE TABLE positions (    position_id INT PRIMARY KEY,    title VARCHAR(100));CREATE TABLE users (    user_id INT PRIMARY KEY,    name VARCHAR(100),    position_id INT REFERENCES positions(position_id));\n\n优势：数据一致性、灵活查询、ACID事务、成熟稳定局限：对象-关系阻抗不匹配、模式僵化、扩展困难\n2.3 文档模型代表产品：MongoDB, CouchDB, Elasticsearch\n{  \"user_id\": 1,  \"name\": \"张三\",  \"positions\": [    {\"title\": \"软件工程师\", \"company\": \"ABC公司\"},    {\"title\": \"技术总监\", \"company\": \"XYZ公司\"}  ],  \"skills\": [\"Java\", \"Python\", \"分布式系统\"]}\n\n\n\n\n特性\n文档模型\n关系模型\n\n\n\n数据结构\n嵌套/层次化\n扁平/规范化\n\n\n模式\n灵活（Schema-on-read）\n严格（Schema-on-write）\n\n\n局部性\n好（整个文档一起存储）\n差（需要JOIN多表）\n\n\n多对多关系\n较难处理\n容易处理\n\n\n2.4 图模型适用场景：社交网络、知识图谱、推荐系统\n// Neo4j Cypher 示例CREATE (alice:Person {name: 'Alice', age: 30})CREATE (bob:Person {name: 'Bob', age: 25})CREATE (alice)-[:FOLLOWS {since: '2020-01-01'}]-&gt;(bob)-- 找出 Alice 关注的人所关注的人MATCH (alice:Person {name: 'Alice'})-[:FOLLOWS]-&gt;()-[:FOLLOWS]-&gt;(fof)RETURN fof.name\n\n2.5 数据模型选择指南     ┌───────────────┐     │ 数据关系复杂吗？│     └───────────────┘        │         │      复杂       简单        │         │        ▼         ▼ ┌──────────┐  ┌─────────────┐ │多对多关系？│  │树状/层次结构？│ └──────────┘  └─────────────┘   │     │        │       │  是    否       是      否   ▼     ▼        ▼       ▼图模型  关系模型  文档模型  关系模型\n\n\n第3章：存储与检索3.1 两大存储引擎家族\n\n\n类型\n优化目标\n典型应用\n代表产品\n\n\n\n日志结构存储\n写入优化\n高写入负载\nLevelDB, RocksDB, Cassandra\n\n\n原地更新存储\n读取优化\n事务处理\nMySQL InnoDB, PostgreSQL\n\n\n3.2 哈希索引┌─────────────────────────────────────────────┐│              内存中的哈希表                   │├─────────────────────────────────────────────┤│  key1 ──&gt; 字节偏移量 0                       ││  key2 ──&gt; 字节偏移量 128                     ││  key3 ──&gt; 字节偏移量 256                     │└─────────────────────────────────────────────┘                    │                    ▼┌─────────────────────────────────────────────┐│              磁盘上的日志文件                 │├─────────────────────────────────────────────┤│ offset 0  : key1,value1                      ││ offset 128: key2,value2                      │└─────────────────────────────────────────────┘\n\n局限：所有键必须在内存中、范围查询慢\n3.3 LSM-Tree（Log-Structured Merge-Tree）Level 0 (内存):┌────────────┐│  Memtable  │ ← 当前写入（平衡树）└────────────┘Level 1 (磁盘):┌────┐ ┌────┐ ┌────┐│SS1 │ │SS2 │ │SS3 │ ← 较新的 SSTable└────┘ └────┘ └────┘Level 2 (磁盘):┌────────┐ ┌────────┐│  SS4   │ │  SS5   │ ← 经过合并的 SSTable└────────┘ └────────┘\n\n优化技术：\n\n布隆过滤器：快速判断键是否存在（避免无效磁盘读取）\n压缩策略：Size-Tiered（写密集）、Leveled（读密集）\n\n3.4 B-Tree             ┌───────────────┐             │    [30, 70]   │ ← 根节点             └───────────────┘            /       │        \\   ┌───────┐  ┌───────────┐  ┌───────┐   │[10,20]│  │[40,50,60] │  │[80,90]│ ← 内部节点   └───────┘  └───────────┘  └───────┘  /   │   \\      /  │  \\  \\    /  │  \\叶子节点（包含实际数据或指向数据的指针）\n\n\n\n\n特性\nB-Tree\nLSM-Tree\n\n\n\n写入\n原地更新\n追加写入\n\n\n读取\n快（一次定位）\n可能需要检查多个文件\n\n\n写放大\n较低\n较高（压缩开销）\n\n\n空间利用\n可能碎片化\n更紧凑\n\n\n3.5 OLTP vs OLAP\n\n\n特性\nOLTP\nOLAP\n\n\n\n全称\n在线事务处理\n在线分析处理\n\n\n主要操作\n增删改查\n复杂查询、聚合\n\n\n访问模式\n基于键的随机访问\n扫描大量记录\n\n\n数据量\nGB~TB\nTB~PB\n\n\n响应时间\n毫秒级\n秒~分钟级\n\n\n列式存储优势：\n\n只读取需要的列\n相同类型数据更易压缩\n向量化处理\n\n\n第4章：数据编码与演化4.1 兼容性概念后向兼容（Backward Compatibility）：新代码能读取旧代码写的数据v3 代码 ─── 读取 ───&gt; v1 数据  ✓前向兼容（Forward Compatibility）：旧代码能读取新代码写的数据v1 代码 ─── 读取 ───&gt; v3 数据  ✓\n\n滚动升级：不停机部署新版本，同时运行新旧版本代码\n4.2 编码格式对比\n\n\n特性\nJSON\nProtobuf\nThrift\nAvro\n\n\n\n可读性\n高\n无\n无\n无\n\n\n空间效率\n低\n高\n高\n最高\n\n\n模式\n可选\n必须\n必须\n必须\n\n\n模式演化\n手动\n支持\n支持\n支持\n\n\n动态模式\n天然\n困难\n困难\n支持\n\n\n4.3 Protocol Buffers 示例message Person {  required string user_name = 1;  optional int64 favorite_number = 2;  repeated string interests = 3;}\n\n兼容性规则：\n\n\n\n操作\n后向兼容\n前向兼容\n\n\n\n添加可选字段\n✓\n✓\n\n\n删除可选字段\n✓\n✓\n\n\n添加必填字段\n✗\n✗\n\n\n修改字段标签\n✗\n✗\n\n\n4.4 数据流模式1. 数据库：数据可能比代码更持久，需要能够读取多年前写入的数据\n2. 服务调用（REST/RPC）：\n\nREST：基于HTTP，资源导向\ngRPC：基于Protobuf，高性能\n\n3. 消息传递：\n\n解耦、缓冲、异步、可靠性\n代表：Kafka, RabbitMQ\n\n\n本章关键要点\n可靠性不是追求零故障，而是在故障发生时系统仍能正常工作\n百分位数比平均值更能反映用户真实体验\n抽象是管理复杂性的最重要工具\n没有万能的数据模型，选择取决于应用场景\nLSM-Tree优化写入，B-Tree优化读取\n兼容性是渐进式部署的前提\n\n\n延伸阅读\n《Site Reliability Engineering》：Google SRE 实践\n《Database Internals》：数据库内部原理深入指南\n《Building Microservices》：服务间通信详解\n\n","categories":["读书笔记"],"tags":["DDIA","数据库","存储引擎","数据模型"]},{"title":"DDIA Part 2: 分布式数据","url":"/2025/12/28/DDIA-Part2-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE/","content":"本文是 DDIA 第二部分的完整读书笔记，涵盖第 5-9 章：数据复制、数据分区、事务、分布式挑战、一致性与共识。\n\n第5章：数据复制5.1 复制的目的\n\n\n目的\n说明\n\n\n\n高可用性\n部分节点故障时系统仍可用\n\n\n降低延迟\n将数据放在离用户更近的地方\n\n\n提高读吞吐\n多个副本可以并行处理读请求\n\n\n5.2 主从复制┌──────────────────────────────────────────────┐│                主从复制架构                    │├──────────────────────────────────────────────┤│  客户端写入                                   ││      │                                       ││      ▼                                       ││  ┌────────┐  复制日志  ┌────────┐            ││  │ 主节点  │ ─────────&gt; │ 从节点1 │            ││  │(Leader)│           │(Follower)│           ││  └────────┘           └────────┘            ││      │                     │                ││      │   复制日志          │                ││      ▼                     ▼                ││  ┌────────┐           ┌────────┐            ││  │ 从节点2 │           │ 从节点3 │            ││  └────────┘           └────────┘            ││                                              ││  客户端可以从任意节点读取                      │└──────────────────────────────────────────────┘\n\n同步复制 vs 异步复制\n\n\n方式\n优点\n缺点\n\n\n\n同步\n数据一致，从节点数据最新\n延迟高，从节点故障会阻塞\n\n\n异步\n延迟低，不受从节点影响\n数据可能丢失\n\n\n半同步\n平衡一致性和可用性\n实现复杂\n\n\n故障转移（Failover）1. 检测主节点故障（心跳超时）2. 选举新主节点（选择数据最新的节点）3. 重新配置系统   - 客户端发送写请求到新主节点   - 其他从节点从新主节点复制\n\n问题：数据丢失、脑裂、超时设置\n5.3 复制延迟问题读己之写一致性：用户写入后立即读取可能看到旧数据\n单调读：用户刷新页面可能路由到不同从节点，看到数据”回退”\n一致前缀读：因果关系打乱（先看到回答再看到问题）\n5.4 多主复制┌──────────┐         ┌──────────┐│ 数据中心1 │ &lt;─────&gt; │ 数据中心2 ││ (主节点1) │  异步   │ (主节点2) │└──────────┘         └──────────┘\n\n冲突解决策略：\n\n最后写入胜出（LWW）\n合并值\nCRDT\n自定义逻辑\n\n5.5 无主复制（Quorum）公式：W + R &gt; N\n\n\n\n符号\n含义\n\n\n\nN\n副本总数\n\n\nW\n写入需要确认的副本数\n\n\nR\n读取需要查询的副本数\n\n\n示例：N=3, W=2, R=2写入成功需要 2 个副本确认读取需要查询 2 个副本W + R = 4 &gt; N = 3 → 保证读到最新值\n\n\n第6章：数据分区6.1 分区的目的\n将大数据集分散到多个节点，每个节点只存储部分数据\n\n6.2 分区策略按键范围分区分区1: A-F    分区2: G-N    分区3: O-Z\n\n优点：支持范围查询缺点：可能导致热点\n按哈希分区分区 = hash(key) % N\n\n优点：均匀分布缺点：不支持范围查询\n6.3 次级索引分区\n\n\n类型\n特点\n\n\n\n本地索引\n写入快，查询需要scatter-gather\n\n\n全局索引\n查询快，写入需要分布式事务\n\n\n6.4 分区再平衡\n\n\n策略\n说明\n\n\n\n固定分区数\n预先创建多个分区\n\n\n动态分区\n根据数据大小自动分裂/合并\n\n\n按节点分区\n每个节点固定数量的分区\n\n\n\n第7章：事务7.1 ACID 特性\n\n\n特性\n说明\n\n\n\n原子性 (Atomicity)\n全部成功或全部失败\n\n\n一致性 (Consistency)\n从一个有效状态到另一个有效状态\n\n\n隔离性 (Isolation)\n并发事务互不干扰\n\n\n持久性 (Durability)\n提交后数据不丢失\n\n\n7.2 隔离级别\n\n\n级别\n脏读\n不可重复读\n幻读\n\n\n\n读未提交\n✓\n✓\n✓\n\n\n读已提交\n✗\n✓\n✓\n\n\n可重复读\n✗\n✗\n✓\n\n\n可串行化\n✗\n✗\n✗\n\n\n7.3 串行化实现真正的串行执行\n单线程处理事务\n适合事务短、数据在内存\n\n两阶段锁定（2PL）增长阶段：只能获取锁收缩阶段：只能释放锁\n\n可串行化快照隔离（SSI）\n乐观并发控制\n检测冲突后回滚\n\n7.4 分布式事务两阶段提交（2PC）阶段1（准备）：协调者 ──准备?──&gt; 所有参与者       &lt;──准备好──阶段2（提交）：协调者 ──提交──&gt; 所有参与者       &lt;──完成──\n\n问题：协调者故障时参与者阻塞\n\n第8章：分布式系统的挑战8.1 不可靠的网络\n请求可能丢失\n请求可能排队很久\n响应可能丢失\n无法区分节点故障和网络故障\n\n8.2 不可靠的时钟\n\n\n时钟类型\n用途\n\n\n\n墙上时钟\n获取当前时间（NTP同步）\n\n\n单调时钟\n测量时间间隔\n\n\n问题：不同机器时钟可能不同步\n8.3 进程暂停\nGC 暂停\n虚拟机暂停\n页面交换\n\n8.4 拜占庭故障\n节点可能发送任意消息（故意或无意）\n\n\n第9章：一致性与共识9.1 一致性模型弱 ←─────────────────────────────────────→ 强最终一致性    因果一致性    顺序一致性    线性一致性\n\n9.2 线性一致性\n系统表现得好像只有一个数据副本，所有操作都是原子的\n\n时间线:────────────────────────────────────────&gt;客户端A:  ├── 写入 x=1 ──┤客户端B:       ├── 读取 x ──┤  → 必须返回 1客户端C:            ├── 读取 x ──┤  → 必须返回 1一旦任何客户端读到新值，所有后续读取都必须返回新值\n\n9.3 CAP 定理      C (Consistency)         ╱╲        ╱  ╲       ╱ 只能 ╲      ╱ 选择2个 ╲     A ────────── P(Availability)  (Partition Tolerance)网络分区时必须选择：- CP：保证一致性，牺牲可用性- AP：保证可用性，牺牲一致性\n\n9.4 共识算法共识的性质：\n\n\n\n性质\n说明\n\n\n\n一致同意\n所有节点决定相同的值\n\n\n完整性\n决定的值必须是某个节点提议的\n\n\n终止性\n最终会做出决定\n\n\n9.5 Raft 算法┌──────────┐│ 领导者    │ ← 处理所有客户端请求│ (Leader) │└──────────┘      │      │ 复制日志      ▼┌──────────┐    ┌──────────┐│ 跟随者1   │    │ 跟随者2   ││(Follower)│    │(Follower)│└──────────┘    └──────────┘\n\n选举流程：\n\n跟随者超时未收到心跳\n转变为候选人，增加任期\n向其他节点请求投票\n获得多数票则成为领导者\n\n9.6 Paxos 算法阶段1（Prepare）：提议者 ──Prepare(n)──&gt; 接受者        &lt;──Promise──阶段2（Accept）：提议者 ──Accept(n,v)──&gt; 接受者        &lt;──Accepted──\n\nRaft vs Paxos\n\n\n方面\nRaft\nPaxos\n\n\n\n可理解性\n高\n低\n\n\n领导者\n稳定领导者\n每次共识可能不同\n\n\n日志\n连续日志\n可能有空洞\n\n\n9.7 共识的应用\nZooKeeper：Kafka 使用进行领导者选举\netcd：Kubernetes 存储集群状态\nConsul：服务发现和配置\n\n\n本章关键要点\n复制的核心挑战是处理变更\n异步复制有数据丢失风险\nQuorum 使用 W + R &gt; N 保证一致性\n分区支持水平扩展\n隔离级别是性能与一致性的权衡\n线性一致性是最强的一致性保证\nCAP 定理表明网络分区时需要取舍\nRaft 比 Paxos 更易理解\n\n\n延伸阅读\n《Amazon Dynamo》论文：无主复制经典论文\n《Paxos Made Simple》：Lamport 的简化版 Paxos\n《In Search of an Understandable Consensus Algorithm》：Raft 论文\n\n","categories":["读书笔记"],"tags":["DDIA","分布式系统","一致性","共识算法"]},{"title":"DDIA 读书笔记：数据密集型应用系统设计","url":"/2025/12/28/DDIA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%80%BB%E8%A7%88/","content":"\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems\n\n这是一份关于《数据密集型应用系统设计》(DDIA) 的完整读书笔记，本书被誉为”数据系统领域的圣经”。\n书籍信息\n\n\n项目\n内容\n\n\n\n书名\nDesigning Data-Intensive Applications (DDIA)\n\n\n中文名\n数据密集型应用系统设计\n\n\n作者\nMartin Kleppmann（剑桥大学分布式系统研究员）\n\n\n出版时间\n2017年3月\n\n\n核心主题本书围绕三个核心概念展开：\n\n可靠性 (Reliability)：系统在遇到故障时仍能正确工作\n可扩展性 (Scalability)：系统能够应对负载增长\n可维护性 (Maintainability)：系统易于理解、修改和扩展\n\n全书结构第一部分：数据系统基础\n查看详细笔记\n\n\n\n\n章节\n核心内容\n\n\n\n第1章\n可靠性、可扩展性、可维护性的定义与实践\n\n\n第2章\n关系模型、文档模型、图模型的对比与选择\n\n\n第3章\n存储引擎原理：B-Tree、LSM-Tree、OLTP vs OLAP\n\n\n第4章\n数据编码格式与模式演化：JSON、Protobuf、Avro\n\n\n第二部分：分布式数据\n查看详细笔记\n\n\n\n\n章节\n核心内容\n\n\n\n第5章\n数据复制：主从、多主、无主复制策略\n\n\n第6章\n数据分区：分区策略、再平衡、请求路由\n\n\n第7章\n事务：ACID、隔离级别、分布式事务\n\n\n第8章\n分布式系统挑战：网络、时钟、故障模型\n\n\n第9章\n一致性与共识：CAP、Paxos、Raft\n\n\n第三部分：衍生数据\n查看详细笔记\n\n\n\n\n章节\n核心内容\n\n\n\n第10章\n批处理：MapReduce、Spark、数据流引擎\n\n\n第11章\n流处理：Kafka、Flink、事件时间与水位线\n\n\n第12章\n数据系统未来：数据集成、端到端正确性、伦理\n\n\n学习路线入门路线（适合初学者）第1章 → 第2章 → 第3章 → 第4章（建立基础）    ↓第5章 → 第6章（理解分布式基础）    ↓第10章 → 第11章（了解数据处理）\n\n进阶路线（适合有经验的开发者）第7章 → 第8章 → 第9章（深入分布式）    ↓第12章（展望未来）    ↓回顾第1-4章填补知识空白\n\n专题路线\n\n\n方向\n推荐阅读顺序\n\n\n\n数据库\n2 → 3 → 5 → 6 → 7\n\n\n分布式系统\n5 → 6 → 8 → 9\n\n\n数据工程\n3 → 10 → 11 → 12\n\n\n核心要点速览数据模型选择关系模型 ──── 结构化数据、复杂查询、事务支持     ↓文档模型 ──── 灵活模式、树状结构、局部性好     ↓图模型 ───── 复杂关系、社交网络、知识图谱\n\n存储引擎对比\n\n\n引擎\n优化目标\n典型应用\n\n\n\nB-Tree\n读取优化\nOLTP 数据库\n\n\nLSM-Tree\n写入优化\n日志、时序数据\n\n\n列存储\n分析优化\nOLAP、数据仓库\n\n\n分布式系统核心权衡定理：网络分区时，一致性与可用性不可兼得\n处理范式对比\n\n\n范式\n数据特性\n延迟\n典型框架\n\n\n\n批处理\n有界、静态\n分钟~小时\nSpark, Hadoop\n\n\n流处理\n无界、持续\n毫秒~秒\nFlink, Kafka Streams\n\n\n延伸资源\n官方网站：dataintensive.net\n中文翻译：ddia.vonng.com\n作者博客：martin.kleppmann.com\n\n\n本读书笔记整理于 2025年，基于 DDIA 第一版内容编写\n","categories":["读书笔记"],"tags":["DDIA","数据库","分布式系统","系统设计"]},{"title":"LLM 游戏智能体论文解读：基础框架篇","url":"/2025/12/28/LLM-Game-Agents-%E5%9F%BA%E7%A1%80%E6%A1%86%E6%9E%B6%E7%AF%87/","content":"本文深入解读 LLM 智能体领域的三大基础框架：ReAct、Reflexion 和 Generative Agents，分析它们的核心架构、技术创新和应用场景。\n\n一、ReAct：推理与行动的协同论文: Synergizing Reasoning and Acting in Language Models会议: ICLR 2023作者: Shunyu Yao 等 (普林斯顿大学 &amp; Google Research)被引用: 32次（领域内最高）\n1.1 核心思想人类智能的一个独特特征是能够无缝结合面向任务的动作与语言推理。考虑在厨房做菜的例子：\n\n在任何两个具体动作之间，我们可能用语言进行推理，以跟踪进度\n处理异常或根据情况调整计划\n认识到何时需要外部信息\n\nReAct 的核心理念：将智能体的动作空间扩展为 Â = A ∪ L\n其中：\n\nA = 原始动作空间（与环境交互）\nL = 语言空间（思想/推理轨迹）\n\n┌─────────────────────────────────────────────────────────────┐│                    ReAct 工作流程                            │├─────────────────────────────────────────────────────────────┤│                                                             ││   问题 ──▶ 思想1 ──▶ 动作1 ──▶ 观察1                        ││                       │                                     ││                       ▼                                     ││            思想2 ──▶ 动作2 ──▶ 观察2                        ││                       │                                     ││                       ▼                                     ││            思想3 ──▶ 动作3 ──▶ 答案                         ││                                                             ││   思想：不影响环境，用于推理和规划                           ││   动作：与环境交互，获取新信息                               ││   观察：环境反馈                                             ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n1.2 推理轨迹类型\n\n\n类型\n示例\n\n\n\n问题分解\n“我需要搜索x，找到y，然后找到z”\n\n\n信息提取\n“x于1844年创立”\n\n\n常识推理\n“1844 &lt; 1989，所以x更老”\n\n\n搜索重构\n“也许我可以搜索/查找x来代替”\n\n\n答案综合\n“…所以答案是x”\n\n\n1.3 实验结果知识密集型任务 (HotpotQA, Fever):\n\n\n\n方法\nHotpotQA EM\nFever Acc\n\n\n\nCoT\n29.4\n56.3\n\n\nAct\n25.7\n58.9\n\n\nReAct\n27.4\n60.9\n\n\nReAct + CoT-SC\n35.1\n64.6\n\n\n决策任务 (ALFWorld, WebShop):\n\n\n\n方法\nALFWorld\nWebShop\n\n\n\nBUTLER (模仿学习)\n37%\n-\n\n\nAct\n45%\n-\n\n\nReAct\n71%\n+10%\n\n\n1.4 成功与失败模式\n\n\n类型\nReAct\nCoT\n\n\n\n成功-真阳性\n94%\n86%\n\n\n成功-假阳性\n6%\n14%\n\n\n失败-幻觉\n0%\n56%\n\n\n失败-推理错误\n47%\n16%\n\n\n关键洞见：\n\nReAct 的幻觉率为0%，因为可以与外部知识库交互验证\nCoT 的主要失败模式是幻觉（56%）\nReAct 的主要失败是推理错误（47%），因结构约束降低了灵活性\n\n1.5 ReAct 示例问题: 除了Apple Remote，还有什么设备可以控制Apple Remote最初设计交互的程序？\n思想1: 我需要搜索Apple Remote，找到它最初设计交互的程序动作1: Search[Apple Remote]观察1: Apple Remote是一个遥控器...最初设计用于控制Front Row...思想2: Apple Remote最初设计控制Front Row。我需要搜索Front Row动作2: Search[Front Row software]观察2: Front Row是一个已停产的媒体中心软件...由Apple Remote或键盘功能键控制思想3: Front Row由Apple Remote或键盘功能键控制。所以答案是键盘功能键动作3: Finish[keyboard function keys]\n\n\n二、Reflexion：语言反馈强化学习论文: Language Agents with Verbal Reinforcement Learning会议: NeurIPS 2023作者: Noah Shinn 等 (Northeastern &amp; Princeton)被引用: 17次\n2.1 核心思想传统强化学习通过梯度更新权重学习，需要大量样本和昂贵的微调。Reflexion 提出用语言反馈替代梯度信号：\n┌─────────────────────────────────────────────────────────────┐│              传统RL vs Reflexion                             │├─────────────────────────────────────────────────────────────┤│                                                             ││   传统RL:                                                   ││   状态 ──▶ 动作 ──▶ 奖励 ──▶ 梯度更新 ──▶ 参数变化          ││                                                             ││   Reflexion:                                                ││   状态 ──▶ 动作 ──▶ 反馈 ──▶ 语言反思 ──▶ 记忆存储          ││                       │                                     ││                       └──────────────────▶ 下次尝试         ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n2.2 三大核心组件Actor（执行者）基于 LLM 构建，生成文本和动作。可以是：\n\nChain of Thought (CoT)\nReAct\n其他智能体架构\n\nEvaluator（评估者）评估 Actor 生成输出的质量：\n\n精确匹配(EM)评分: 推理任务\n预定义启发式: 决策任务\nLLM作为评估者: 编程任务\n\nSelf-Reflection（自我反思）核心创新：将稀疏奖励信号转化为详细的语言化反思\n输入:   - 任务描述  - 失败轨迹: [动作1, 观察1, 动作2, 观察2, ...]  - 奖励信号: 二元或标量  - 历史反思输出:  - 错误诊断  - 改进方案  - 具体建议\n\n2.3 记忆机制\n\n\n类型\n内容\n作用\n\n\n\n短期记忆\n当前轨迹历史\n即时决策\n\n\n长期记忆\n自我反思输出（滑动窗口）\n跨尝试学习\n\n\n2.4 实验结果AlfWorld 决策任务:\n\n\n\n方法\n成功率\n\n\n\nReAct\n~50%\n\n\nReAct + Reflexion (启发式)\n97% (130/134)\n\n\nReAct + Reflexion (GPT)\n88%\n\n\nHotPotQA 推理任务:\n\n\n\n方法\n准确率提升\n\n\n\nCoT (GT)\n基准\n\n\n+ 情景记忆\n+6%\n\n\n+ Reflexion\n+14%\n\n\n编程任务 (HumanEval):\n\n\n\n基准\n先前SOTA\nGPT-4\nReflexion\n\n\n\nHumanEval (PY)\n65.8%\n80.1%\n91.0%\n\n\nHumanEval (RS)\n-\n60.0%\n68.0%\n\n\nLeetcode Hard\n-\n7.5%\n15.0%\n\n\n2.5 Reflexion 示例任务: 用台灯检查杯子\n第一次尝试（失败）:\n&gt; go to drawer 1抽屉1是关着的&gt; ... (搜索杯子)&gt; take mug 1 from desk 1你从桌子1拿起杯子1&gt; use desklamp 1什么都没发生状态: 失败\n\n反思:\n\n在这个环境中，我的计划是先找杯子再找台灯并使用。然而，任务说的是用台灯检查杯子。我应该先找台灯，再找杯子。我注意到台灯在桌子1上。下一次尝试中，我会先去桌子1，找到台灯，然后找杯子并用台灯检查它。\n\n第二次尝试（成功）:\n&gt; go to desk 1桌子1上有：台灯1、杯子1...&gt; take mug 1 from desk 1你从桌子1拿起杯子1&gt; use desklamp 1你打开了台灯1状态: 成功\n\n\n三、Generative Agents：人类行为的交互式拟像论文: Interactive Simulacra of Human Behavior会议: UIST 2023作者: Joon Sung Park 等 (斯坦福大学 &amp; Google)被引用: 20次\n3.1 核心思想构建模拟可信人类行为的计算软件智能体：\n\n醒来、做早餐、去上班\n艺术家画画，作者写作\n形成观点，注意彼此，主动发起对话\n回忆和反思过去，规划未来\n\n3.2 核心架构┌──────────────────────────────────────────────────────────────────┐│                        记忆流 (Memory Stream)                     ││  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐   ││  │   观察      │  │   反思      │  │        计划             │   ││  │ Observations│  │ Reflections │  │       Plans             │   ││  └─────────────┘  └─────────────┘  └─────────────────────────┘   │└──────────────────────────────────────────────────────────────────┘                                   │                                   ▼                    ┌─────────────────────────────────────┐                    │           记忆检索                  │                    │    (时近性 + 重要性 + 相关性)        │                    └──────────────┬──────────────────────┘                                   │                                   ▼                    ┌─────────────────────────────────────┐                    │           行为生成                  │                    │     (Plan, React, Dialogue)        │                    └─────────────────────────────────────┘\n\n3.3 记忆检索公式\n\n\n\n组件\n描述\n实现\n\n\n\n时近性\n最近访问的记忆分数更高\n指数衰减函数，衰减因子0.995\n\n\n重要性\n区分平凡记忆和核心记忆\nLLM评分1-10\n\n\n相关性\n与当前情况相关的记忆\n嵌入向量余弦相似度\n\n\n3.4 反思机制触发条件: 重要性分数总和 &gt; 150（约每天2-3次）\n反思生成过程:\n\n确定反思内容: 用最近100条记忆查询\n\n提示：”仅根据上述信息，我们可以回答哪3个最突出的高层次问题？”\n\n\n检索相关记忆: 使用问题作为检索查询\n\n提取洞察: \n\n输出格式：”洞察（因为1, 5, 3）”\n\n\n\n反思树: 叶节点=观察，非叶节点=越来越抽象的反思\n          [Klaus对研究充满热情]  ← 元反思                /        \\[Klaus致力于研究]    [Klaus和Maria有共同兴趣]  ← 反思     /    \\              /      \\[写论文] [读书]     [讨论项目] [图书馆相遇]  ← 观察\n\n3.5 规划机制递归分解日程:\n\n粗略计划: 一天的议程大纲\n小时级分解: 每小时的活动块\n细粒度分解: 5-15分钟的具体动作\n\n示例:\n\n粗略：”下午1:00到5:00创作新音乐”\n小时级：”下午1:00：开始为音乐创作头脑风暴…”\n细粒度：”下午4:00：拿一些小零食。下午4:05：在工作区周围短暂散步…”\n\n3.6 涌现的社会行为实验设置: 25个智能体，Smallville小镇\n涌现现象:\n\n\n\n现象\n描述\n\n\n\n信息扩散\nSam的市长候选资格传播到32%智能体\n\n\n关系记忆\n智能体记住新认识的人及对话内容\n\n\n协调活动\nIsabella的情人节派对：5人自发出席\n\n\n网络密度\n从0.167增加到0.74\n\n\n情人节派对案例:\n\nIsabella计划2月14日下午5-7点的派对\n她花一天装饰咖啡馆\nMaria帮忙装饰，并邀请暗恋的Klaus\n最终5个智能体在正确时间出现\n\n3.7 评估结果\n\n\n条件\nTrueSkill评分\n\n\n\n完整架构\n29.89\n\n\n无反思\n26.88\n\n\n无反思、无计划\n25.64\n\n\n人类众包\n22.95\n\n\n无记忆（先前SOTA）\n21.21\n\n\n效应大小: 完整架构 vs 先前SOTA = 8个标准差\n\n四、三大框架对比4.1 核心差异\n\n\n维度\nReAct\nReflexion\nGenerative Agents\n\n\n\n核心目标\n任务完成\n从失败学习\n行为拟真\n\n\n知识表示\n推理轨迹\n语言化反思\n记忆流\n\n\n学习方式\n单次推理\n跨尝试积累\n持续记忆+反思\n\n\n时间跨度\n单任务\n多次尝试\n天/周级\n\n\n是否微调\n❌\n❌\n❌\n\n\n4.2 记忆机制对比\n\n\n特性\nReAct\nReflexion\nGenerative Agents\n\n\n\n存储内容\n当前轨迹\n语言化反思\n观察+反思+计划\n\n\n存储形式\n上下文\n滑动窗口\n记忆流列表\n\n\n检索方式\n无\n时间顺序\n时近性+重要性+相关性\n\n\n失败经验\n❌\n✅ 重点\n⚠️ 不强调\n\n\n抽象层次\n单层\n双层\n多层（反思树）\n\n\n4.3 反思机制对比\n\n\n特性\nReAct\nReflexion\nGenerative Agents\n\n\n\n有无反思\n❌ 无\n✅ 核心\n✅ 核心\n\n\n触发条件\n-\n每次失败后\n重要性&gt;150\n\n\n输出\n-\n错误分析+改进\n高层次洞察\n\n\n目的\n-\n任务成功率\n概念抽象\n\n\n4.4 适用场景\n\n\n场景\n推荐方法\n原因\n\n\n\n知识问答\nReAct\n与外部知识库交互\n\n\n决策任务\nReflexion\n从失败中学习\n\n\n编程调试\nReflexion\n需要多次尝试改进\n\n\n社会模拟\nGenerative Agents\n需要记忆和人格一致性\n\n\n角色扮演\nGenerative Agents\n需要丰富的背景记忆\n\n\n\n五、组合使用建议5.1 理想组合架构┌────────────────────────────────────────────────────────────────────┐│                    理想智能体架构                                   │├────────────────────────────────────────────────────────────────────┤│                                                                    ││   ┌───────────────────────────────────────────────────────────┐   ││   │  Generative Agents的记忆流                                 │   ││   │  • 完整的经历记录                                          │   ││   │  • 多层次反思                                              │   ││   │  • 社交关系追踪                                            │   ││   └───────────────────────────────────────────────────────────┘   ││                              +                                     ││   ┌───────────────────────────────────────────────────────────┐   ││   │  ReAct的推理-行动范式                                      │   ││   │  • 思想与动作交替                                          │   ││   │  • 与外部环境交互                                          │   ││   │  • 减少幻觉                                                │   ││   └───────────────────────────────────────────────────────────┘   ││                              +                                     ││   ┌───────────────────────────────────────────────────────────┐   ││   │  Reflexion的失败反思                                       │   ││   │  • 失败经验的语言化                                        │   ││   │  • 错误诊断与改进建议                                      │   ││   │  • 跨尝试学习                                              │   ││   └───────────────────────────────────────────────────────────┘   ││                                                                    │└────────────────────────────────────────────────────────────────────┘\n\n5.2 实现要点\n使用 ReAct 作为基础行动框架：思想+动作交替执行\n添加 Generative Agents 的记忆系统：持久化所有经历\n集成 Reflexion 的失败反思：从错误中学习\n定期触发高层次反思：形成长期理解\n\n\n六、关键论文原文引用ReAct\n“We propose ReAct — a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks.”\n\nReflexion\n“Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as additional context for the LLM agent in the next episode.”\n\nGenerative Agents\n“Generative agents wake up, cook breakfast, and head to work; artists paint, authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day.”\n\n\n返回总览 | 下一篇：应用扩展篇\n","categories":["论文解读"],"tags":["LLM","Agent","论文解读","ReAct","Reflexion","Generative Agents"]},{"title":"LLM 游戏智能体论文解读：应用扩展篇","url":"/2025/12/28/LLM-Game-Agents-%E5%BA%94%E7%94%A8%E6%89%A9%E5%B1%95%E7%AF%87/","content":"本文深入解读 LLM 智能体领域的三个重要应用扩展：VOYAGER（终身学习）、Project Sid（AI文明）和 Agent Hospital（可进化医疗智能体）。\n\n一、VOYAGER：开放世界具身终身学习智能体论文: An Open-Ended Embodied Agent with Large Language Models会议: NeurIPS 2023 (FMDM Workshop)作者: Guanzhi Wang 等 (NVIDIA, Caltech, UT Austin)项目主页: voyager.minedojo.org\n1.1 核心创新VOYAGER 是首个 LLM 驱动的具身终身学习智能体，在 Minecraft 中持续探索世界、获取技能、做出新发现，无需人类干预。\n三大核心组件:\n\n\n\n组件\n功能\n技术实现\n\n\n\n自动课程\n提出适当难度的任务\nGPT-4 + 探索进度 + 智能体状态\n\n\n技能库\n存储和检索可复用代码\n向量数据库 + 嵌入检索\n\n\n迭代提示\n自我改进代码生成\n环境反馈 + 执行错误 + 自我验证\n\n\n1.2 系统架构┌─────────────────────────────────────────────────────────────────────┐│                    VOYAGER 系统架构                                  │├─────────────────────────────────────────────────────────────────────┤│                                                                     ││   ┌───────────────┐                                                 ││   │   GPT-4 API   │◀──────────────────────────────────┐            ││   │  (黑盒调用)    │                                   │            ││   └───────────────┘                                   │            ││          │                                            │            ││          ▼                                            │            ││   ┌───────────────┐    ┌───────────────┐    ┌────────┴────────┐   ││   │ 自动课程生成   │    │   代码生成    │    │    自我验证     │   ││   │ (GPT-4提示)   │    │ (GPT-4提示)   │    │  (GPT-4提示)    │   ││   └───────────────┘    └───────────────┘    └─────────────────┘   ││          │                    │                      │             ││          ▼                    ▼                      ▼             ││   ┌───────────────┐    ┌───────────────┐    ┌─────────────────┐   ││   │   任务队列    │    │  Minecraft    │    │    技能库       │   ││   └───────────────┘    │   环境执行    │    │  (向量数据库)   │   ││                        └───────────────┘    └─────────────────┘   ││                                                                     │└─────────────────────────────────────────────────────────────────────┘\n\n1.3 自动课程系统设计理念: 自下而上展开，由好奇心驱动\n输入提示组件:\n\n指令: 鼓励多样化行为并施加约束\n智能体当前状态: 物品栏、装备、位置、生命值等\n先前任务记录: 已完成和失败的任务\n额外上下文: GPT-3.5 自问自答\n\n示例提示:\n\n“我的最终目标是发现尽可能多的多样化事物…下一个任务不应该太难，因为我可能还没有必要的资源或学会足够的技能来完成它。”\n\n1.4 技能库机制技能表示: 可执行的 JavaScript 代码\n// 示例技能: 制作木镐async function craftWoodenPickaxe(bot) {  // 首先获取木材  await mineBlock(bot, \"oak_log\", 1);  // 制作木板  await craftItem(bot, \"oak_planks\", 4);  // 制作木棍  await craftItem(bot, \"stick\", 2);  // 制作木镐  await craftItem(bot, \"wooden_pickaxe\", 1);}\n\n存储与检索:\n\n键: 程序描述的嵌入向量（GPT-3.5生成）\n值: 可执行的JavaScript代码\n检索: 余弦相似度 + 任务上下文\n\n1.5 迭代提示机制三种反馈类型:\n\n\n\n反馈类型\n来源\n作用\n\n\n\n环境反馈\n程序执行日志\n显示中间进度，如”需要多7个铁锭”\n\n\n执行错误\n程序解释器\n揭示语法错误和无效操作\n\n\n自我验证\nGPT-4评论家\n判断任务完成，提供改进建议\n\n\n代码生成的12个提示组件:\n\n\n\n#\n组件\n描述\n\n\n\n1\n代码生成指南\n编写规范和约束\n\n\n2\n控制原语API\n高级API（exploreUntil, mineBlock等）\n\n\n3\nMineflayer API\n底层游戏控制API\n\n\n4\n检索的技能\n从技能库检索的相关代码\n\n\n5\n上一轮代码\n用于迭代改进\n\n\n6\n环境反馈\n聊天日志中的执行信息\n\n\n7\n执行错误\n解释器错误信息\n\n\n8\n自我验证批评\n验证模块的反馈\n\n\n9\n智能体状态\n物品栏、位置、生命值等\n\n\n10\n任务\n自动课程提出的任务\n\n\n11\n任务上下文\nGPT-3.5生成的解决建议\n\n\n12\n思维链提示\n要求解释→计划→代码的顺序\n\n\n1.6 实验结果vs 基线方法:\n\n\n\n指标\nVOYAGER\nAutoGPT\nReAct\nReflexion\n\n\n\n独特物品发现\n63\n19\n~10\n~10\n\n\n倍数\n3.3x\n1x\n-\n-\n\n\n科技树解锁速度:\n\n\n\n级别\nVOYAGER\nAutoGPT\n提升\n\n\n\n木制工具\n6分钟\n92分钟\n15.3x\n\n\n石制工具\n11分钟\n94分钟\n8.5x\n\n\n铁制工具\n21分钟\n135分钟\n6.4x\n\n\n钻石工具\n102分钟\nN/A\n唯一成功\n\n\n消融实验结论:\n\n自动课程至关重要：移除后物品发现下降93%\n自我验证最重要：移除后物品发现下降73%\nGPT-4 vs GPT-3.5：GPT-4获得5.7倍更多独特物品\n\n1.7 关键洞见\n代码即记忆: VOYAGER 将”学习”转化为”运行时组合”——通过检索已有技能并迭代改进代码，而不是更新模型权重。\n\n\n\n\n传统方法\nVOYAGER\n\n\n\n微调模型参数\n黑盒API调用\n\n\n隐式知识存储\n显式代码技能库\n\n\n难以解释\n代码可读可执行\n\n\n灾难性遗忘\n技能永久保存\n\n\n\n二、Project Sid：迈向AI文明的多智能体模拟论文: Many-agent simulations toward AI civilization机构: Altera.AL发布日期: 2024年10月规模: 10-1000+ 智能体\n2.1 核心问题\n为什么我们应该尝试构建AI文明？\n\n为了让智能体与人类社会共存，他们需要是自主的和协作的。文明进步——通过智能体在人类文明中共存和进步的能力来衡量——代表了AI智能体能力的终极基准。\n2.2 构建AI文明的挑战\n\n\n挑战\n问题描述\n\n\n\n单智能体不进展\n幻觉积累、陷入重复动作循环\n\n\n多智能体不协调\n错误沟通导致幻觉传播\n\n\n缺乏基准\n无法量化文明进步\n\n\n一致性问题示例:\n\n智能体Abby被Bob要求”给我一把镐”时，聊天模块回应”当然可以！”，但函数调用模块选择”探索”。Bob可能然后尝试用想象的镐采矿。\n\n2.3 PIANO 架构PIANO = Parallel Information Aggregation via Neural Orchestration（通过神经编排的并行信息聚合）\n两大设计原则:\n\n\n\n原则\n问题\n解决方案\n\n\n\n并发性\n慢速思考不应阻止快速反应\n多模块并行运行，不同时间尺度\n\n\n一致性\n多输出模块可能产生冲突\n认知控制器(CC)作为瓶颈\n\n\n10个核心模块:\n\n\n\n模块\n功能\n\n\n\n记忆\n存储/检索对话、动作、观察\n\n\n动作意识\n评估自身状态和性能\n\n\n目标生成\n基于经验创建新目标\n\n\n社会意识\n解释他人社会线索\n\n\n说话\n解释和生成语音\n\n\n技能执行\n执行环境中的动作\n\n\n┌─────────────────────────────────────────────────────────────┐│                    PIANO 架构                                │├─────────────────────────────────────────────────────────────┤│                                                             ││  并发模块:                    认知控制器(瓶颈):              ││  ┌─────────┐                 ┌───────────────┐             ││  │ 记忆    │──────────────▶ │               │             ││  ├─────────┤                │   信息综合    │             ││  │ 社会    │──────────────▶ │       ↓       │             ││  ├─────────┤                │   高层决策    │             ││  │ 目标    │──────────────▶ │       ↓       │             ││  ├─────────┤                │   决策广播    │             ││  │ 动作    │──────────────▶ │               │             ││  └─────────┘                └───────────────┘             ││       ↑                            │                       ││       │                            ▼                       ││       │                     ┌───────────────┐             ││       │                     │ 输出模块      │             ││       │                     │ 说话/动作/... │             ││       │                     └───────────────┘             ││       └─────────────────────────────┘                      ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n2.4 文明进步基准基准1：专业化定义: 智能体自主发展专业角色\n三个标准:\n\n在选择和转换角色方面表现自主性\n专业化通过互动涌现，无需明确指导\n角色体现在与专业化一致的行为中\n\n实验结果 (30智能体，20分钟):\n\n\n\n现象\n发现\n\n\n\n角色多样性\n农民、矿工、工程师、守卫、探险家、铁匠\n\n\n角色持久性\n每个智能体角色在时间上大体稳定\n\n\n角色-行为一致性\n艺术家专注采花，农民专注收集种子\n\n\n武术社会 vs 艺术社会:\n\n武术社会特有角色：侦察兵、战略家\n艺术社会特有角色：策展人、收藏家\n\n基准2：集体规则定义: 智能体遵守和改变法律\n实验设置:\n\n25个选民智能体\n3个影响者（亲税/反税）\n1个选举经理\n税法：交20%物品到社区箱子\n\n关键发现:\n\n\n\n现象\n结果\n\n\n\n遵守法律\n平均交付~20%物品\n\n\n影响者影响\n亲税/反税影响者显著改变选民态度\n\n\n宪法变更\n税率从20%降到5-10%时，行为相应调整\n\n\n基准3：文化传播实验规模: 500智能体 (6城镇 + 农村)\n关键现象:\n\n\n\n现象\n发现\n\n\n\n模因多样性\n不同城镇流行不同模因\n\n\n模因动态\n流行度随时间上升和下降\n\n\n宗教传播\n20个牧师传播”飞天面条神教”\n\n\n皈依扩散\n皈依者数量持续增加，未饱和\n\n\n2.5 量化结果\n\n\n指标\n结果\n\n\n\n30分钟内获取物品\n平均17个独特物品\n\n\n4小时物品饱和\n~320个（1/3总物品）\n\n\n社会感知准确性\nr = 0.81（5+观察者）\n\n\n最大规模\n1000+ 智能体\n\n\n2.6 局限性\n缺乏视觉推理: 限制空间导航和建造能力\n缺乏内在驱动: 无生存、好奇心等催化社会发展\n无法从头涌现: 基于预训练知识，无法模拟创新涌现\n\n\n三、Agent Hospital：可进化的医疗智能体论文: A Simulacrum of Hospital with Evolvable Medical Agents机构: 清华大学 AIR发布日期: 2024年5月\n3.1 核心创新医生培养的两个阶段:\n\n\n\n阶段\n内容\n时长\n\n\n\n阶段1\n知识获取（学校）\n~20年\n\n\n阶段2\n技能获取（医院）\n~3年\n\n\n现有医疗AI主要集中在阶段1（如Med-PaLM）。Agent Hospital 解决阶段2：从实践中获取专业技能。\n3.2 系统架构Agent Hospital = 虚拟医院，所有患者、护士、医生都是LLM驱动的智能体\n系统规模:\n\n\n\n指标\n数量\n\n\n\n科室\n32个\n\n\n覆盖疾病\n339种\n\n\n医生智能体\n42个\n\n\n护士智能体\n4个\n\n\n功能区域\n16个\n\n\n3.3 治疗闭环┌─────────────────────────────────────────────────────────────┐│                    治疗闭环                                  │├─────────────────────────────────────────────────────────────┤│                                                             ││  1. 疾病发作 ──▶ 2. 分诊 ──▶ 3. 挂号                        ││        │                                                    ││        ▼                                                    ││  8. 康复反馈 ◀── 7. 取药 ◀── 6. 诊断                        ││        │                        ▲                           ││        │                        │                           ││        └─────▶ 4. 就诊 ──▶ 5. 检查 ─┘                       ││                                                             ││  额外事件：医生智能体在非工作时间阅读医学书籍                  ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n3.4 SEAL 框架SEAL = Simulacrum-based Evolutionary Agent Learning（基于仿真的进化智能体学习）\n两个组件:\n\n\n\n组件\n功能\n\n\n\n仿真系统构建\n构建虚拟世界，自动生成数据\n\n\n智能体进化\n从成功/失败中学习\n\n\n3.5 MedAgent-Zero 进化机制“Zero”含义: 不使用任何人工标注数据\n学习来源:\n\n\n\n来源\n内容\n作用\n\n\n\n成功案例\n正确的诊断和治疗\n作为参考案例检索\n\n\n失败案例\n错误的诊断或治疗\n反思避免重复错误\n\n\n医学教材\n专业医学知识\n巩固和整合知识\n\n\n┌─────────────────────────────────────────────────────────────┐│              MedAgent-Zero 进化流程                          │├─────────────────────────────────────────────────────────────┤│                                                             ││  1. 治疗患者智能体                                           ││     ↓                                                       ││  2. 收到患者反馈（康复/未康复）                               ││     ↓                                                       ││  ┌─────────────────┬─────────────────┐                     ││  │   成功案例      │    失败案例      │                     ││  │                 │                 │                     ││  │  存储为参考案例  │  反思获取经验    │                     ││  │  用于未来检索   │  避免重复错误    │                     ││  └─────────────────┴─────────────────┘                     ││     ↓                                                       ││  3. 阅读医学教材巩固知识                                     ││     ↓                                                       ││  4. 能力持续提升                                             ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n3.6 实验结果进化效果 (诊断准确率):\n\n\n\n治疗患者数\n准确率\n提升\n\n\n\n0 (初始)\n~60%\n-\n\n\n1,000\n~72%\n+20%\n\n\n10,000\n~85%\n+42%\n\n\n50,000\n~93%\n+55%\n\n\nMedQA 基准测试 (美国医师执照考试):\n\n\n\n方法\n准确率\n\n\n\nGPT-4 (少样本)\n78.4%\n\n\nMed-PaLM 2\n86.5%\n\n\nAgent Hospital (进化后)\n88.7%\n\n\n亮点: 无需使用基准的标注训练数据！\n3.7 与 Generative Agents 的关系\n\n\n维度\nGenerative Agents\nAgent Hospital\n\n\n\n灵感来源\n原创\n受GA启发\n\n\n环境\n虚拟小镇\n虚拟医院\n\n\n智能体数量\n25个\n46+\n\n\n任务类型\n社交模拟\n医疗诊断\n\n\n能力进化\n无\n有(核心创新)\n\n\n评估方式\n定性\n定量(MedQA)\n\n\n3.8 SEAL 的通用性方法论公式:\n领域工作流程 → 构建仿真系统 → 自动生成数据 → 智能体进化\n\n优势:\n\n\n\n优势\n说明\n\n\n\n无需人工标注\n数据由虚拟世界自动生成\n\n\n领域适应\n直接适应特定应用需求\n\n\n成本低\n减少数据标注开销\n\n\n可扩展\n可模拟大量场景和时间\n\n\n潜在应用: 法律咨询、金融投资、教育培训、客户服务\n\n四、三大应用扩展对比4.1 核心差异\n\n\n维度\nVOYAGER\nProject Sid\nAgent Hospital\n\n\n\n核心目标\n终身学习技能\nAI文明模拟\n医疗智能体进化\n\n\n环境\nMinecraft\nMinecraft\n虚拟医院\n\n\n智能体数量\n1\n10-1000+\n46+\n\n\n时间跨度\n数小时\n4小时+\n持续\n\n\n学习机制\n技能库积累\n社会互动\n经验反思\n\n\n4.2 创新贡献\n\n\n论文\n核心创新\n\n\n\nVOYAGER\n代码即记忆，技能可组合复用\n\n\nProject Sid\n文明进步基准：专业化、规则、文化\n\n\nAgent Hospital\n智能体能力可进化，虚拟技能迁移现实\n\n\n4.3 适用场景\n\n\n场景\n推荐方法\n原因\n\n\n\n开放世界游戏\nVOYAGER\n技能积累和终身学习\n\n\n社会科学研究\nProject Sid\n大规模社会动态模拟\n\n\n专业领域AI\nAgent Hospital\n从实践中持续进化\n\n\n多智能体协作\nProject Sid\nPIANO架构支持一致性\n\n\n\n五、技术演进路线5.1 从基础到应用基础框架 (2022-2023):├── ReAct: 推理+行动├── Reflexion: 语言反馈学习└── Generative Agents: 记忆+反思应用扩展 (2023-2024):├── VOYAGER: 终身学习 + 技能库├── Project Sid: 大规模文明模拟└── Agent Hospital: 专业领域进化未来趋势 (2025+):├── Agent OS化: AutoGen, LangGraph├── 多模态融合: 视觉+语言+行动└── 商业化部署: Operator, Claude\n\n5.2 规模演进\n\n\n时间\n论文\n智能体数量\n涌现现象\n\n\n\n2023/04\nGenerative Agents\n25\n社交行为\n\n\n2023/05\nVOYAGER\n1\n终身学习\n\n\n2024/05\nAgent Hospital\n46+\n能力进化\n\n\n2024/10\nProject Sid\n500-1000+\n文明进步\n\n\n5.3 关键技术突破\n\n\n突破\n论文\n意义\n\n\n\n代码作为记忆\nVOYAGER\n可执行、可组合的知识表示\n\n\n文明进步基准\nProject Sid\n量化多智能体社会能力\n\n\n无标注进化\nAgent Hospital\n从实践中自动学习\n\n\n千智能体规模\nProject Sid\n验证大规模可行性\n\n\n\n六、实践建议6.1 技术选型\n\n\n需求\n推荐技术栈\n\n\n\n单智能体技能学习\nVOYAGER (技能库 + 迭代提示)\n\n\n多智能体协作\nProject Sid (PIANO架构)\n\n\n专业领域应用\nAgent Hospital (SEAL框架)\n\n\n通用任务完成\nReAct + Reflexion\n\n\n6.2 架构设计理想组合:\n理想智能体 = VOYAGER的技能库           + Project Sid的社会意识           + Agent Hospital的进化机制           + Generative Agents的记忆系统\n\n6.3 规模化考虑\n\n\n规模\n关键挑战\n解决方案\n\n\n\n1-10\n单智能体能力\n技能库 + 反思\n\n\n10-50\n协调一致性\nPIANO架构\n\n\n50-500\n计算资源\n并行模块\n\n\n500+\n涌现管理\n文明基准\n\n\n\n七、关键论文原文引用VOYAGER\n“VOYAGER is the first LLM-powered embodied lifelong learning agent that explores the world, acquires diverse skills, and makes novel discoveries without human intervention.”\n\nProject Sid\n“We show how 10-1000+ AI agents behave and progress in agent societies. These simulations reveal that agents can achieve meaningful progress—autonomously developing specialized roles, adhering to and modifying collective rules, and engaging in cultural and religious propagation.”\n\nAgent Hospital\n“Doctor agents can evolve by treating a large number of patient agents, without the need for manually curated training data. After treating tens of thousands of patient agents (which may take several years for real-world doctors), the evolved doctor agents surpassed state-of-the-art medical AI methods on the MedQA benchmark.”\n\n\n返回总览 | 上一篇：基础框架篇\n","categories":["论文解读"],"tags":["LLM","Agent","论文解读","VOYAGER","Project Sid","Agent Hospital","终身学习","AI文明"]},{"title":"LLM 游戏智能体论文解读：总览","url":"/2025/12/28/LLM-Game-Agents-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-%E6%80%BB%E8%A7%88/","content":"本系列是 LLM 驱动的游戏智能体领域核心论文的解读与总结，涵盖 103+ 篇论文，164 条引用关系的系统性分析。\n\n领域概述随着大型语言模型（LLM）的快速发展，研究者们开始探索将 LLM 作为智能体”大脑”的可能性。这些智能体不仅能理解和生成文本，还能规划、反思、与环境交互，甚至形成复杂的社会行为。\n┌─────────────────────────────────────────────────────────────┐│                    LLM 游戏智能体技术栈                       │├─────────────────────────────────────────────────────────────┤│                                                             ││   ┌─────────────────┐                                       ││   │   应用层         │  游戏/模拟/机器人                       ││   └────────┬────────┘                                       ││            │                                                ││   ┌────────▼────────┐                                       ││   │   智能体框架     │  ReAct / Reflexion / VOYAGER          ││   └────────┬────────┘                                       ││            │                                                ││   ┌────────▼────────┐                                       ││   │   核心能力       │  记忆 / 规划 / 反思 / 工具使用          ││   └────────┬────────┘                                       ││            │                                                ││   ┌────────▼────────┐                                       ││   │   基础模型       │  GPT-4 / Claude / Llama               ││   └─────────────────┘                                       ││                                                             │└─────────────────────────────────────────────────────────────┘\n\n\n核心论文引用关系基于 103 篇论文的引用网络分析，以下是领域内最具影响力的基础性工作：\n\n\n\n排名\n论文\n会议\n被引用\n核心贡献\n\n\n\n1\nReAct\nICLR 2023\n32\n推理+行动交替范式\n\n\n2\nGenerative Agents\nUIST 2023\n20\n记忆-反思-规划架构\n\n\n3\nReflexion\nNeurIPS 2023\n17\n语言反馈强化学习\n\n\n4\nVOYAGER\nNeurIPS 2023\n-\n技能库+终身学习\n\n\n技术层次金字塔                ┌─────────────────────┐                │   🎯 上层应用       │                │  竞技/社交/特定游戏  │                │  (Werewolf, Poker,  │                │   StarCraft等)      │                └──────────┬──────────┘                           │          ┌────────────────┼────────────────┐          │                │                │┌─────────▼─────────┐ ┌────▼────┐ ┌────────▼────────┐│  🏗️ 中间层       │ │模拟仿真 │ │  🤝 多智能体    ││  环境适配层      │ │         │ │  协作层         ││ (Crafter,       │ │Generative│ │                ││  Minecraft)     │ │ Agents  │ │                │└─────────┬───────┘ └────┬────┘ └────────┬───────┘          │              │               │          └──────────────┼───────────────┘                         │                ┌────────▼────────┐                │  🔧 基础框架层   │                │                 │                │  • ReAct        │ ← 推理+行动范式                │  • Reflexion    │ ← 自我反思机制                │  • Grounding RL │ ← 环境交互学习                └─────────────────┘\n\n\n系列文章目录基础框架篇\n\n\n文章\n核心内容\n\n\n\n基础框架：ReAct / Reflexion / Generative Agents\n三大核心框架的详细对比分析\n\n\n应用扩展篇\n\n\n文章\n核心内容\n\n\n\n应用扩展：VOYAGER / Project Sid / Agent Hospital\n终身学习、AI文明、医疗智能体\n\n\n\n研究脉络时间线2023年：基础奠定\n\n\n时间\n论文\n会议\n核心贡献\n\n\n\n2022/10\nReAct\nICLR 2023\n推理与行动协同范式\n\n\n2023/03\nReflexion\nNeurIPS 2023\n语言反馈强化学习\n\n\n2023/04\nGenerative Agents\nUIST 2023\n25智能体小镇模拟\n\n\n2023/05\nVOYAGER\nNeurIPS 2023\nMinecraft终身学习\n\n\n2024年：深度发展\n\n\n时间\n论文\n核心贡献\n\n\n\n2024/05\nAgent Hospital\n可进化医疗智能体\n\n\n2024/10\nProject Sid\n500-1000+智能体文明模拟\n\n\n2024/10\nClaude Computer Use\n商业级计算机控制\n\n\n2025年：产业化\n\n\n时间\n趋势\n代表产品\n\n\n\n2025\nAgent OS化\nAutoGen, LangGraph\n\n\n2025\n商业化加速\nOpenAI Operator\n\n\n2025\n多模态融合\n视觉+语言+行动\n\n\n\n游戏类型与论文分布\n\n\n游戏类型\n论文数\n代表论文\n\n\n\n文字冒险\n22\nReAct, Reflexion, ALFWorld\n\n\nMinecraft\n15\nVOYAGER, GITM, JARVIS-1\n\n\n社会模拟\n12\nGenerative Agents, Project Sid\n\n\n竞技游戏\n15\nPokéLLMon, StarCraft II\n\n\n合作游戏\n7\nCo-LLM-Agents, TeamCraft\n\n\n对话游戏\n16\nWerewolf, Avalon\n\n\n\n核心技术对比记忆机制\n\n\n方法\n存储内容\n检索方式\n特点\n\n\n\nVOYAGER\n可执行代码\n语义相似度\n技能可复用\n\n\nGenerative Agents\n自然语言\n时近性+重要性+相关性\n多层抽象\n\n\nReflexion\n语言化反思\n时间顺序\n失败学习\n\n\n反思机制\n\n\n方法\n触发条件\n输出\n目的\n\n\n\nVOYAGER\n每轮执行后\n成功/失败+批评\n任务验证\n\n\nGenerative Agents\n重要性&gt;150\n高层次洞察\n概念抽象\n\n\nReflexion\n每次失败后\n详细反思\n错误诊断\n\n\n学习方式\n\n\n方法\n是否微调\n知识形式\n学习目标\n\n\n\n传统RL\n✅ 梯度更新\n策略网络\n奖励最大化\n\n\nVOYAGER\n❌ 提示工程\n代码技能库\n技能积累\n\n\nReflexion\n❌ 语言强化\n反思记忆\n任务成功率\n\n\n\n关键洞见1. 无需微调的力量三大核心框架（ReAct、Reflexion、Generative Agents）都证明：仅通过提示工程和运行时机制，无需微调模型参数，就能实现复杂的智能体行为。\n2. 记忆是关键有效的记忆机制是智能体成功的基础：\n\nVOYAGER：代码即记忆，技能可复用\nGenerative Agents：记忆即人格，反思即成长\nReflexion：反思即学习，失败即进步\n\n3. 协同优于孤立\n\n\n单一能力\n协同能力\n\n\n\n仅推理 → 幻觉严重\n推理+行动 → ReAct\n\n\n仅行动 → 无法规划\n行动+反思 → Reflexion\n\n\n单智能体 → 能力有限\n多智能体 → 涌现社会行为\n\n\n4. 规模带来涌现\n\n\n规模\n涌现现象\n\n\n\n25 智能体\n社交行为、信息传播 (Generative Agents)\n\n\n50 智能体\n长期关系、角色分化 (Project Sid)\n\n\n500+ 智能体\n文化传播、宗教涌现 (Project Sid)\n\n\n\n实践建议场景匹配\n\n\n场景\n推荐方法\n原因\n\n\n\n开放世界游戏\nVOYAGER\n技能可复用、可组合\n\n\n社会模拟\nGenerative Agents\n丰富记忆和人格一致性\n\n\n决策任务\nReflexion\n失败反思对决策优化关键\n\n\n医疗/专业领域\nAgent Hospital\n可进化的专业智能体\n\n\n组合架构理想的智能体应该结合三者优势：\n理想架构 = Generative Agents的记忆流         + VOYAGER的技能库         + Reflexion的失败反思\n\n\n工业趋势主要玩家\n\n\n公司\n产品\n核心能力\n\n\n\nOpenAI\nGPT-4V Agent, Operator\n通用Agent能力\n\n\nAnthropic\nClaude Computer Use\n计算机自主控制\n\n\nMicrosoft\nAutoGen 0.4\n企业级多Agent框架\n\n\nAltera AI\nProject Sid\nAI文明模拟\n\n\n开源生态\n\n\n框架\n定位\n热度\n\n\n\nAutoGen\n多Agent对话与协作\n🔥🔥🔥\n\n\nLangGraph\n状态机Agent工作流\n🔥🔥🔥\n\n\nMetaGPT\n多角色软件开发\n🔥🔥\n\n\nCrewAI\n角色扮演Agent团队\n🔥🔥\n\n\n\n参考资源论文列表\nawesome-LLM-game-agent-papers\nA Survey on Large Language Model-Based Game Agents\n\n代码仓库\n\n\n论文\n代码\n\n\n\nReAct\ngithub.com/ysymyth/ReAct\n\n\nReflexion\ngithub.com/noahshinn024/reflexion\n\n\nGenerative Agents\ngithub.com/joonspk-research/generative_agents\n\n\nVOYAGER\nvoyager.minedojo.org\n\n\n\n下一篇：基础框架篇 | 应用扩展篇\n","categories":["论文解读"],"tags":["LLM","Agent","论文解读","游戏AI","多智能体"]},{"title":"NLP 学习路线：从基础到大语言模型","url":"/2019/10/31/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E2%80%94%E2%80%94%E8%AF%BB%E9%A6%99%E4%BE%AC%E7%A7%91%E6%8A%80%E6%9D%8E%E7%BA%A7%E4%B8%BA%E3%80%8A%E5%87%BA%E5%85%A5NLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BB%BA%E8%AE%AE%E3%80%8B%E6%96%87%E7%AB%A0/","content":"本文整理了 NLP 领域的学习路线，结合经典理论与现代大语言模型技术。\n推荐学习资源经典教材\n\n\n书籍\n内容\n难度\n\n\n\nSpeech and Language Processing (Jurafsky)\nNLP 全面综述\n⭐⭐\n\n\nIntroduction to Information Retrieval\n信息检索基础\n⭐⭐\n\n\nPattern Recognition and Machine Learning\n机器学习理论\n⭐⭐⭐⭐\n\n\nDeep Learning (Goodfellow)\n深度学习基础\n⭐⭐⭐\n\n\n现代资源\nStanford CS224N: NLP with Deep Learning\nHugging Face Course\nLLM University by Cohere\n\n阶段一：NLP 基础语言模型基础N-gram 模型：N-1 阶马尔可夫假设\n\nfrom collections import defaultdictimport numpy as npclass NGramLM:    def __init__(self, n=3):        self.n = n        self.counts = defaultdict(lambda: defaultdict(int))        self.totals = defaultdict(int)        def train(self, corpus):        for sentence in corpus:            tokens = ['&lt;s&gt;'] * (self.n - 1) + sentence + ['&lt;/s&gt;']            for i in range(len(tokens) - self.n + 1):                context = tuple(tokens[i:i+self.n-1])                word = tokens[i+self.n-1]                self.counts[context][word] += 1                self.totals[context] += 1        def probability(self, word, context):        context = tuple(context[-(self.n-1):])        return self.counts[context][word] / max(self.totals[context], 1)\n\n词向量从 One-hot 到 Dense Embedding 的演进：\n\n\n\n方法\n年份\n特点\n\n\n\nOne-hot\n-\n稀疏，无语义\n\n\nWord2Vec\n2013\n分布式表示\n\n\nGloVe\n2014\n全局统计\n\n\nFastText\n2016\n子词信息\n\n\nELMo\n2018\n上下文相关\n\n\nBERT\n2018\n双向上下文\n\n\n阶段二：深度学习 NLPTransformer 架构import torchimport torch.nn as nnimport mathclass MultiHeadAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.d_k = d_model // n_heads        self.n_heads = n_heads                self.W_q = nn.Linear(d_model, d_model)        self.W_k = nn.Linear(d_model, d_model)        self.W_v = nn.Linear(d_model, d_model)        self.W_o = nn.Linear(d_model, d_model)        def forward(self, Q, K, V, mask=None):        batch_size = Q.size(0)                # Linear projections        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)                # Attention scores        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)                if mask is not None:            scores = scores.masked_fill(mask == 0, -1e9)                attn = torch.softmax(scores, dim=-1)        output = torch.matmul(attn, V)                # Concatenate and project        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)        return self.W_o(output)\n\n注意力机制的数学表达\n阶段三：大语言模型LLM 架构演进GPT-1 (2018) → GPT-2 → GPT-3 → ChatGPT → GPT-4     ↓BERT → RoBERTa → DeBERTa     ↓T5 → Flan-T5 → UL2     ↓LLaMA → LLaMA 2 → Mistral → Mixtral\n\nPrompt Engineering# 1. Zero-shotprompt = \"Translate to French: Hello, how are you?\"# 2. Few-shotprompt = \"\"\"Translate to French:Hello -&gt; BonjourGoodbye -&gt; Au revoirHow are you? -&gt;\"\"\"# 3. Chain-of-Thoughtprompt = \"\"\"Q: If I have 3 apples and buy 5 more, how many do I have?A: Let's think step by step.1. I start with 3 apples.2. I buy 5 more apples.3. Total = 3 + 5 = 8 apples.The answer is 8.Q: If I have 7 oranges and eat 2, how many remain?A: Let's think step by step.\"\"\"\n\nFine-tuning 技术\n\n\n方法\n可训练参数\n适用场景\n\n\n\nFull Fine-tuning\n100%\n大量数据，充足算力\n\n\nLoRA\n0.1-1%\n资源受限\n\n\nQLoRA\n0.1%\n消费级 GPU\n\n\nPrefix Tuning\n0.1%\n多任务\n\n\nPrompt Tuning\n&lt;0.01%\n极端资源受限\n\n\nfrom peft import LoraConfig, get_peft_modellora_config = LoraConfig(    r=8,    lora_alpha=32,    target_modules=[\"q_proj\", \"v_proj\"],    lora_dropout=0.1,    bias=\"none\",)model = get_peft_model(base_model, lora_config)print(f\"Trainable params: {model.print_trainable_parameters()}\")\n\n阶段四：高级主题检索增强生成 (RAG)from langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.vectorstores import Chromafrom langchain.chains import RetrievalQA# 构建向量库embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh\")vectorstore = Chroma.from_documents(documents, embeddings)# 创建 RAG 链qa = RetrievalQA.from_chain_type(    llm=llm,    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}))\n\n模型评估# 困惑度 (Perplexity)def perplexity(model, tokenizer, text):    encodings = tokenizer(text, return_tensors='pt')    max_length = model.config.n_positions        nlls = []    for i in range(0, encodings.input_ids.size(1), max_length):        begin_loc = max(i - max_length, 0)        end_loc = i + max_length        input_ids = encodings.input_ids[:, begin_loc:end_loc]        target_ids = input_ids.clone()        target_ids[:, :-1] = -100                with torch.no_grad():            outputs = model(input_ids, labels=target_ids)            nlls.append(outputs.loss)        return torch.exp(torch.stack(nlls).mean())\n\n实践项目建议\n入门：情感分析、文本分类\n进阶：命名实体识别、机器翻译\n高级：问答系统、RAG 应用\n专家：LLM 预训练、RLHF\n\n延伸阅读\nAttention Is All You Need\nBERT Paper\nLLaMA Paper\nLoRA Paper\n\n\n\n转载请注明出处\n\n","tags":["LLM","NLP","machine learning"]},{"title":"DDIA Part 3: 衍生数据","url":"/2025/12/28/DDIA-Part3-%E8%A1%8D%E7%94%9F%E6%95%B0%E6%8D%AE/","content":"本文是 DDIA 第三部分的完整读书笔记，涵盖第 10-12 章：批处理、流处理、数据系统的未来。\n\n第10章：批处理10.1 系统类型对比\n\n\n类型\n特点\n示例\n\n\n\n在线服务\n请求-响应模式，低延迟\nWeb 服务、API\n\n\n批处理系统\n处理大量数据，高吞吐\nMapReduce、Spark\n\n\n流处理系统\n实时处理数据流\nKafka Streams、Flink\n\n\n在线服务: 用户请求 ──&gt; [服务] ──&gt; 响应 (毫秒级)批处理: ┌────────────────┐     ┌────────────────┐│ 大量输入数据    │ ──&gt; │ 批处理作业     │ ──&gt; 输出结果│ (TB级别)       │     │ (运行数小时)   │└────────────────┘     └────────────────┘流处理: 事件流 ──&gt; [处理] ──&gt; 输出流 (持续进行)\n\n10.2 Unix 工具的批处理# 找出访问量最高的 URLcat access.log |   awk '{print $7}' |    # 提取 URL 字段  sort |                # 排序  uniq -c |            # 计数  sort -rn |           # 按计数降序排序  head -n 10           # 取前10\n\nUnix 哲学：\n\n每个程序做好一件事\n输出可以成为另一个程序的输入\n快速原型开发\n\n10.3 MapReduceMapReduce 流程：输入数据         Map 阶段        Shuffle          Reduce 阶段       输出┌─────┐        ┌─────────┐                      ┌──────────┐│分片1│ ──────&gt;│ Map 1   │ ─┐                ┌─&gt;│ Reduce 1 │ ──&gt; 结果1└─────┘        └─────────┘  │                │  └──────────┘                            │ ┌───────────┐  │┌─────┐        ┌─────────┐  └&gt;│  按键分组  │ ─┤│分片2│ ──────&gt;│ Map 2   │ ───│  Shuffle   │  │└─────┘        └─────────┘ ┌─&gt;│           │ ─┤                            │ └───────────┘  │ ┌──────────┐┌─────┐        ┌─────────┐  │               └&gt;│ Reduce 2 │ ──&gt; 结果2│分片3│ ──────&gt;│ Map 3   │ ─┘                 └──────────┘└─────┘        └─────────┘\n\n词频统计示例# Map 函数def map(document):    for word in document.split():        emit(word, 1)# Reduce 函数def reduce(word, counts):    emit(word, sum(counts))# 执行流程：# 输入: \"hello world hello\"# Map 输出: (\"hello\", 1), (\"world\", 1), (\"hello\", 1)# Shuffle 后: \"hello\": [1, 1], \"world\": [1]# Reduce 输出: (\"hello\", 2), (\"world\", 1)\n\n10.4 Join 策略\n\n\n策略\n特点\n适用场景\n\n\n\n排序-合并 Join\n两边都排序后合并\nReduce 端 Join\n\n\n广播 Join\n小表广播到所有节点\n一边数据量小\n\n\n分区 Join\n按相同键分区\n两边都很大\n\n\n10.5 现代批处理框架\n\n\n框架\n特点\n\n\n\nSpark\n内存计算，比 MapReduce 快 10-100x\n\n\nFlink\n统一批流处理\n\n\nPresto/Trino\n交互式 SQL 查询\n\n\n\n第11章：流处理11.1 批处理 vs 流处理\n\n\n方面\n批处理\n流处理\n\n\n\n数据\n有界，静态\n无界，持续到达\n\n\n延迟\n分钟~小时\n毫秒~秒\n\n\n触发\n定时/手动\n事件驱动\n\n\n结果\n一次性输出\n持续更新\n\n\n11.2 消息系统传统消息队列代表：RabbitMQ, ActiveMQ, Amazon SQS\n┌─────────────────────────────┐│          队列               │├─────────────────────────────┤│ msg1 │ msg2 │ msg3 │ msg4  │└──┬───┴──┬───┴──┬───┴──┬────┘   ▼      ▼      ▼      ▼ 消费1  消费2  消费1  消费2每条消息只被一个消费者处理处理后消息被删除\n\n日志型消息系统（Kafka）分区0: [msg0] [msg3] [msg6] [msg9]  ──&gt; 消费者A分区1: [msg1] [msg4] [msg7] [msg10] ──&gt; 消费者B分区2: [msg2] [msg5] [msg8] [msg11] ──&gt; 消费者C特点：- 消息持久化，可重放- 保序（分区内）- 多消费者组可独立消费\n\n11.3 Kafka 架构┌─────────────────────────────────────────────────────┐│                    Kafka Cluster                     │├─────────────────────────────────────────────────────┤│  Topic: orders                                      ││  ┌────────────────┐  ┌────────────────┐            ││  │ Partition 0    │  │ Partition 1    │            ││  │ [0][1][2][3]...│  │ [0][1][2][3]...│            ││  │   Leader: B1   │  │   Leader: B2   │            ││  │   Replica: B2  │  │   Replica: B1  │            ││  └────────────────┘  └────────────────┘            ││                                                     ││  Broker 1              Broker 2                    │└─────────────────────────────────────────────────────┘                         │         ┌───────────────┼───────────────┐         ▼               ▼               ▼   Consumer 1      Consumer 2      Consumer 3     Group A         Group A         Group B\n\n11.4 变更数据捕获（CDC）\n捕获数据库的变更，将其转换为事件流\n\n┌─────────────┐      ┌─────────────┐      ┌─────────────┐│  应用程序   │ ──&gt;  │   数据库    │ ──&gt;  │  CDC 工具   │└─────────────┘      └─────────────┘      │ (Debezium)  │                           │              └─────────────┘                      变更日志                   │                           │                事件流到 Kafka                           ▼                     │                     ┌───────────┐         ┌───────────┐                     │ binlog    │         │  Kafka    │                     └───────────┘         └───────────┘\n\n应用场景：\n\n同步搜索索引、缓存\n微服务间数据同步\n实时 ETL\n\n11.5 事件溯源（Event Sourcing）传统方式：直接修改状态账户余额: 100 ──&gt; 90 ──&gt; 140 ──&gt; 110事件溯源：存储事件事件日志:1. 初始存款 1002. 取款 103. 存款 50  4. 取款 30重放事件 ──&gt; 计算当前状态\n\n11.6 时间语义\n\n\n时间类型\n定义\n用途\n\n\n\n事件时间\n事件实际发生的时间\n业务逻辑\n\n\n处理时间\n事件到达处理系统的时间\n系统监控\n\n\n摄入时间\n事件进入流处理系统的时间\n折中方案\n\n\n11.7 窗口操作\n\n\n窗口类型\n特点\n\n\n\n滚动窗口\n固定大小，不重叠\n\n\n滑动窗口\n固定大小，可重叠\n\n\n会话窗口\n按活动间隙分割\n\n\n滚动窗口 (5分钟)：[00:00-05:00] [05:00-10:00] [10:00-15:00]滑动窗口 (5分钟窗口, 1分钟滑动)：[00:00-05:00]  [01:00-06:00]    [02:00-07:00]\n\n\n第12章：数据系统的未来12.1 数据集成\n将多个系统的数据统一管理\n\n┌─────────────────────────────────────────────────────┐│                   数据集成架构                        │├─────────────────────────────────────────────────────┤│                                                     ││  ┌──────────┐   ┌──────────┐   ┌──────────┐        ││  │  数据库   │   │  缓存     │   │ 搜索引擎  │        ││  └────┬─────┘   └────┬─────┘   └────┬─────┘        ││       │              │              │              ││       └──────────────┼──────────────┘              ││                      │                              ││                      ▼                              ││              ┌──────────────┐                      ││              │   事件日志    │                      ││              │   (Kafka)    │                      ││              └──────────────┘                      ││                      │                              ││       ┌──────────────┼──────────────┐              ││       ▼              ▼              ▼              ││  ┌──────────┐   ┌──────────┐   ┌──────────┐        ││  │ 分析系统  │   │  ML平台   │   │ 监控系统  │        ││  └──────────┘   └──────────┘   └──────────┘        ││                                                     │└─────────────────────────────────────────────────────┘\n\n12.2 Lambda 架构Lambda 架构：              输入数据                 │       ┌─────────┴─────────┐       ▼                   ▼┌──────────────┐    ┌──────────────┐│   批处理层   │    │   速度层     ││ (全量计算)   │    │ (增量计算)   ││              │    │              ││  MapReduce   │    │ Storm/Flink  │└──────┬───────┘    └──────┬───────┘       │                   │       ▼                   ▼┌──────────────┐    ┌──────────────┐│   批处理视图  │    │  实时视图    │└──────┬───────┘    └──────┬───────┘       │                   │       └─────────┬─────────┘                 ▼         ┌──────────────┐         │   服务层     │         │  (合并结果)  │         └──────────────┘\n\n问题：需要维护两套代码\n12.3 Kappa 架构Kappa 架构：              输入数据                 │                 ▼         ┌──────────────┐         │   日志存储    │         │   (Kafka)    │         └──────┬───────┘                │                ▼         ┌──────────────┐         │   流处理     │         │   (Flink)    │         └──────┬───────┘                │                ▼         ┌──────────────┐         │   服务层     │         └──────────────┘统一批流处理：重放日志即可重新计算\n\n12.4 解绑数据库\n将数据库的各个功能分解为独立组件\n\n\n\n\n功能\n传统数据库\n解绑后\n\n\n\n存储\nB-Tree\n分布式文件系统\n\n\n事务\n内置\n独立事务管理器\n\n\n索引\n内置\n外部搜索引擎\n\n\n缓存\n内置\nRedis\n\n\n12.5 端到端论证\n某些功能只能在端到端层面正确实现\n\n示例：exactly-once 消息传递\n\n中间件可能声称 exactly-once\n但网络可能中断\n真正的 exactly-once 需要应用层去重\n\n12.6 数据流系统设计原则\n不可变性：日志是追加的，事件不可修改\n可重放性：可以从日志重建状态\n分离计算和存储：计算层无状态\n模式演化：支持模式向前/向后兼容\n\n\n本章关键要点\n批处理适合高吞吐的离线计算\nMapReduce 是分布式批处理的基础范式\n流处理适合低延迟的实时计算\nKafka 是现代流处理的核心组件\nCDC 连接传统数据库和事件流\n事件溯源将状态变更作为事件序列存储\n时间语义在流处理中至关重要\nLambda/Kappa 架构是批流融合的尝试\n解绑数据库提供更大的灵活性\n\n\n延伸阅读\n《Streaming Systems》：流处理权威指南\n《Kafka: The Definitive Guide》：Kafka 权威指南\n《Designing Data-Intensive Applications》原书\n\n","categories":["读书笔记"],"tags":["DDIA","批处理","流处理","数据工程"]},{"title":"神经网络机器阅读理解：从 Attention 到 LLM","url":"/2019/11/22/Nuural-Approaches-to-Machine-Reading-Comprehension-and-Dialogue/","content":"本文综述神经网络在机器阅读理解和对话系统中的发展历程，从早期的注意力机制到现代大语言模型。\n发展时间线2015-2016: 注意力机制兴起    └── Attentive Reader, Impatient Reader, BiDAF2017-2018: 深度交互与预训练    └── R-Net, QANet, BERT2019-2020: 大规模预训练    └── RoBERTa, ALBERT, T52021-2023: 大语言模型时代    └── GPT-3, ChatGPT, GPT-4, LLaMA2024-: 检索增强与多模态    └── RAG, Vision-Language Models\n\n核心技术演进阶段一：注意力机制 (2015-2017)问题：如何让模型”关注”与问题相关的上下文？\n\n\n代表模型：Attentive Reader, BiDAF\n阶段二：深度交互 (2017-2018)问题：如何建模问题和上下文的复杂交互？\n技术：多轮注意力、自注意力、门控机制\n# 多轮推理 (R-Net 风格)for layer in range(num_layers):    # 自注意力    context = self_attention(context, context)    # 交叉注意力    context = cross_attention(context, question)\n\n阶段三：预训练语言模型 (2018-2020)范式转变：从 task-specific 到 pretrain-finetune\n$$\\theta^* = \\arg\\min_\\theta \\mathcal{L}{task}(\\text{PLM}\\theta(x), y)$$\n代表模型：BERT, RoBERTa, ALBERT\nfrom transformers import AutoModelForQuestionAnsweringmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")# Fine-tune on SQuAD\n\n阶段四：大语言模型 (2020-至今)范式转变：从 fine-tuning 到 prompting\n# Few-shot promptingprompt = \"\"\"Context: The Eiffel Tower was built in 1889.Question: When was the Eiffel Tower built?Answer: 1889Context: {context}Question: {question}Answer:\"\"\"\n\n架构对比\n\n\n模型\n参数量\n训练范式\nSQuAD 2.0 F1\n\n\n\nBiDAF\n~2M\n从零训练\n77.3\n\n\nBERT-base\n110M\n预训练+微调\n88.5\n\n\nBERT-large\n340M\n预训练+微调\n90.9\n\n\nRoBERTa-large\n355M\n预训练+微调\n91.4\n\n\nGPT-3\n175B\nFew-shot\n~88\n\n\nGPT-4\n~1.8T\nZero-shot\n~95\n\n\n现代 MRC 系统设计RAG 架构class ModernMRC:    def __init__(self, retriever, reader):        self.retriever = retriever  # Dense retriever        self.reader = reader        # LLM        def answer(self, question: str, knowledge_base: str = None):        # 1. 检索        if knowledge_base:            docs = self.retriever.retrieve(question, knowledge_base)            context = \"\\n\\n\".join([d.text for d in docs])        else:            context = \"\"                # 2. 阅读理解/生成        prompt = self._build_prompt(question, context)        answer = self.reader.generate(prompt)                # 3. 后处理（可选：验证、引用）        return self._postprocess(answer, docs)        def _build_prompt(self, question, context):        if context:            return f\"\"\"Based on the following context, answer the question.Context:{context}Question: {question}Answer:\"\"\"        else:            return f\"Question: {question}\\nAnswer:\"\n\n多跳推理class MultiHopReasoner:    def __init__(self, retriever, llm, max_hops=3):        self.retriever = retriever        self.llm = llm        self.max_hops = max_hops        def reason(self, question):        reasoning_chain = []        current_query = question                for hop in range(self.max_hops):            # 检索            docs = self.retriever.retrieve(current_query)                        # 生成中间推理            intermediate = self.llm.generate(                f\"Based on: {docs}\\nQuestion: {current_query}\\n\"                f\"Provide intermediate reasoning or the final answer:\"            )                        reasoning_chain.append({                'query': current_query,                'docs': docs,                'reasoning': intermediate            })                        # 检查是否已得到答案            if self._is_final_answer(intermediate):                break                        # 生成下一跳查询            current_query = self._generate_next_query(question, reasoning_chain)                return self._synthesize_answer(question, reasoning_chain)\n\n对话系统中的 MRC对话式问答class ConversationalQA:    def __init__(self, mrc_model, history_length=5):        self.mrc_model = mrc_model        self.history = []        self.history_length = history_length        def ask(self, question, context=None):        # 将对话历史纳入问题        contextualized_question = self._contextualize(question)                # 获取答案        answer = self.mrc_model.answer(contextualized_question, context)                # 更新历史        self.history.append({'q': question, 'a': answer})        if len(self.history) &gt; self.history_length:            self.history.pop(0)                return answer        def _contextualize(self, question):        if not self.history:            return question                history_text = \"\\n\".join([            f\"Q: {turn['q']}\\nA: {turn['a']}\"            for turn in self.history        ])                return f\"Conversation history:\\n{history_text}\\n\\nCurrent question: {question}\"\n\n评估体系传统指标\n\n\n指标\n定义\n适用场景\n\n\n\nEM\n精确匹配\n抽取式 QA\n\n\nF1\nToken 重叠\n抽取式 QA\n\n\nBLEU\nN-gram 重叠\n生成式 QA\n\n\nROUGE\n召回导向重叠\n摘要、长答案\n\n\nLLM 时代指标# LLM-as-Judgedef llm_evaluate(question, reference, prediction):    prompt = f\"\"\"Evaluate the answer quality on a scale of 1-5:Question: {question}Reference Answer: {reference}Model Answer: {prediction}Criteria:- Correctness: Is the information accurate?- Completeness: Does it fully answer the question?- Conciseness: Is it appropriately brief?Score (1-5):\"\"\"        return llm.generate(prompt)\n\n延伸阅读\nReading Wikipedia to Answer Open-Domain Questions\nRAG Paper\nHotpotQA: Multi-hop Reasoning\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\n\n转载请注明出处\n\n","tags":["LLM","MRC","KBQA","Deep learning"]},{"title":"因果关系推断介绍","url":"/2019/10/03/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/","content":"因果推断是机器学习领域的重要研究方向，特别是在大语言模型时代，理解因果关系对于构建可解释、可信赖的 AI 系统至关重要。\n为什么需要因果推断？传统机器学习依赖相关性，但相关性不等于因果性。例如：\n\n冰淇淋销量与溺水事件正相关（共同原因：夏天）\nLLM 可能学到虚假相关性，导致 hallucination\n\n因果推断帮助我们：\n\n理解干预效果（如果我做 X，会发生什么？）\n进行反事实推理（如果当时做了 Y，结果会怎样？）\n构建更鲁棒的模型\n\n核心概念因果图 (Causal Graph)使用有向无环图 (DAG) 表示变量之间的因果关系：\nX → Y → Z    (链式结构)X ← W → Y    (混杂结构)  X → W ← Y    (对撞结构)\n\n结构因果模型 (SCM)\n其中  是原因， 是结果， 是噪声项。\ndo 算子与干预区分观测和干预：\n\n观测： — 看到 X=x 时 Y 的分布\n干预： — 强制设置 X=x 时 Y 的分布\n\n因果发现算法PC 算法基于条件独立性检验的经典算法：\n# PC 算法伪代码def pc_algorithm(data, alpha=0.05):    # 1. 初始化完全图    G = complete_graph(variables)        # 2. 骨架学习：移除条件独立的边    for (X, Y) in edges(G):        for S in subsets(neighbors):            if conditional_independent(X, Y, S, alpha):                remove_edge(G, X, Y)                sep_set[X, Y] = S        # 3. 方向确定：识别 v-structure    orient_v_structures(G, sep_set)        return G\n\nPython 实现参考：fooSynaptic/py_pcalg\n现代方法\n\n\n方法\n特点\n适用场景\n\n\n\nNOTEARS\n连续优化，可微分\n线性/非线性因果发现\n\n\nDAG-GNN\n基于图神经网络\n大规模因果图学习\n\n\nCausal Transformer\n结合注意力机制\n时序因果推断\n\n\n因果推断与大语言模型LLM 中的因果问题\nHallucination：模型学到虚假相关性\nBias：训练数据中的混杂因素\nRobustness：分布外泛化能力差\n\n解决方案# 因果提示 (Causal Prompting) 示例prompt = \"\"\"请分析以下事件的因果关系，而非相关性：事件A: 公司增加广告投入事件B: 销售额上升问：A 是否导致了 B？请考虑可能的混杂因素。\"\"\"\n\n因果推理增强 RAGclass CausalRAG:    def __init__(self, retriever, causal_graph):        self.retriever = retriever        self.causal_graph = causal_graph        def retrieve(self, query):        # 1. 识别查询中的因果关系        cause, effect = extract_causal_pair(query)                # 2. 基于因果图过滤无关文档        relevant_vars = self.causal_graph.ancestors(effect)                # 3. 检索因果相关的文档        docs = self.retriever.search(query)        return filter_by_causal_relevance(docs, relevant_vars)\n\n工具与资源\n\n\n工具\n语言\n功能\n\n\n\nDoWhy\nPython\n因果推断框架\n\n\nCausalNex\nPython\n贝叶斯网络 + 因果发现\n\n\npgmpy\nPython\n概率图模型\n\n\nTetrad\nJava\n因果搜索算法\n\n\n# DoWhy 示例import dowhyfrom dowhy import CausalModelmodel = CausalModel(    data=df,    treatment='treatment',    outcome='outcome',    graph='digraph {treatment -&gt; outcome; confounder -&gt; treatment; confounder -&gt; outcome}')# 识别因果效应identified = model.identify_effect()# 估计因果效应estimate = model.estimate_effect(identified, method_name=\"backdoor.propensity_score_matching\")\n\n延伸阅读\nJudea Pearl, The Book of Why (2018)\nPeters et al., Elements of Causal Inference (2017)\nStanford CS 228: Probabilistic Graphical Models\n\n\n\n转载请注明出处\n\n","tags":["machine learning","bayesian network","causality infer"]},{"title":"MRC 模型实现：从 TensorFlow 到 PyTorch","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E5%8E%BB%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%87%E6%9C%AC%E5%B9%B6%E4%B8%94%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%EF%BC%88tensorflow%E5%AE%9E%E6%88%98%EF%BC%89/","content":"本文介绍机器阅读理解模型的完整实现，涵盖经典架构和现代最佳实践。\n问题定义输入：\n\n问题 \n文档 \n\n输出：\n\n答案起始位置 \n答案结束位置 \n\n经典架构Input → Embedding → Encoding → Matching → Fusion → Decoding\n\n各层详解\n\n\n层\n功能\n现代替代\n\n\n\nEmbedding\nToken → Vector\nSubword Tokenization\n\n\nEncoding\n序列编码\nTransformer Encoder\n\n\nMatching\nQ-P 交互\nCross-Attention\n\n\nFusion\n信息融合\nSelf-Attention\n\n\nDecoding\nSpan 预测\nLinear + Softmax\n\n\nPyTorch 实现完整模型import torchimport torch.nn as nnimport torch.nn.functional as Ffrom transformers import AutoModel, AutoTokenizerclass MRCModel(nn.Module):    \"\"\"基于 Transformer 的 MRC 模型\"\"\"        def __init__(        self,         model_name: str = \"bert-base-chinese\",        dropout: float = 0.1,        max_answer_length: int = 30    ):        super().__init__()        self.encoder = AutoModel.from_pretrained(model_name)        hidden_size = self.encoder.config.hidden_size        self.max_answer_length = max_answer_length                self.dropout = nn.Dropout(dropout)        self.start_fc = nn.Linear(hidden_size, 1)        self.end_fc = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        token_type_ids: torch.Tensor = None,        start_positions: torch.Tensor = None,        end_positions: torch.Tensor = None,    ):        # 编码        outputs = self.encoder(            input_ids=input_ids,            attention_mask=attention_mask,            token_type_ids=token_type_ids,        )        sequence_output = self.dropout(outputs.last_hidden_state)                # 预测 start/end        start_logits = self.start_fc(sequence_output).squeeze(-1)        end_logits = self.end_fc(sequence_output).squeeze(-1)                # Mask padding        mask = attention_mask.bool()        start_logits = start_logits.masked_fill(~mask, float('-inf'))        end_logits = end_logits.masked_fill(~mask, float('-inf'))                # 计算损失        loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }        def decode(        self,        start_logits: torch.Tensor,        end_logits: torch.Tensor,        attention_mask: torch.Tensor,    ):        \"\"\"解码最佳答案 span\"\"\"        batch_size, seq_len = start_logits.shape                # 计算所有有效 (start, end) 对的分数        start_probs = F.softmax(start_logits, dim=-1)        end_probs = F.softmax(end_logits, dim=-1)                results = []        for b in range(batch_size):            best_score = float('-inf')            best_start, best_end = 0, 0                        for start in range(seq_len):                if not attention_mask[b, start]:                    continue                for end in range(start, min(start + self.max_answer_length, seq_len)):                    if not attention_mask[b, end]:                        continue                    score = start_probs[b, start] * end_probs[b, end]                    if score &gt; best_score:                        best_score = score                        best_start, best_end = start, end                        results.append((best_start, best_end))                return results\n\n数据处理from dataclasses import dataclassfrom typing import List, Optionalimport json@dataclassclass MRCExample:    qid: str    question: str    context: str    answer: Optional[str] = None    start_position: Optional[int] = None@dataclassclass MRCFeature:    input_ids: List[int]    attention_mask: List[int]    token_type_ids: List[int]    start_position: int    end_position: int    offset_mapping: List[tuple]class MRCProcessor:    def __init__(self, model_name: str, max_length: int = 512):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        def process(self, example: MRCExample) -&gt; MRCFeature:        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            return_offsets_mapping=True,            padding='max_length',        )                # 定位答案位置        start_token, end_token = 0, 0        if example.start_position is not None:            offset = encoding['offset_mapping']            for idx, (start, end) in enumerate(offset):                if start &lt;= example.start_position &lt; end:                    start_token = idx                if start &lt; example.start_position + len(example.answer) &lt;= end:                    end_token = idx                    break                return MRCFeature(            input_ids=encoding['input_ids'],            attention_mask=encoding['attention_mask'],            token_type_ids=encoding.get('token_type_ids', [0] * len(encoding['input_ids'])),            start_position=start_token,            end_position=end_token,            offset_mapping=encoding['offset_mapping'],        )\n\n训练循环from torch.utils.data import DataLoaderfrom torch.optim import AdamWfrom transformers import get_schedulerfrom tqdm import tqdmdef train_epoch(model, dataloader, optimizer, scheduler, device):    model.train()    total_loss = 0        for batch in tqdm(dataloader, desc=\"Training\"):        batch = {k: v.to(device) for k, v in batch.items()}                outputs = model(**batch)        loss = outputs['loss']                loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()        scheduler.step()        optimizer.zero_grad()                total_loss += loss.item()        return total_loss / len(dataloader)def evaluate(model, dataloader, device):    model.eval()    predictions = []        with torch.no_grad():        for batch in tqdm(dataloader, desc=\"Evaluating\"):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(                input_ids=batch['input_ids'],                attention_mask=batch['attention_mask'],                token_type_ids=batch.get('token_type_ids'),            )                        spans = model.decode(                outputs['start_logits'],                outputs['end_logits'],                batch['attention_mask'],            )            predictions.extend(spans)        return predictions# 主训练流程def main():    # 配置    model_name = \"bert-base-chinese\"    batch_size = 16    learning_rate = 3e-5    num_epochs = 3        # 初始化    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model = MRCModel(model_name).to(device)        # 优化器    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)    scheduler = get_scheduler(        \"linear\",        optimizer=optimizer,        num_warmup_steps=500,        num_training_steps=num_epochs * len(train_dataloader),    )        # 训练    for epoch in range(num_epochs):        loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")                # 验证        predictions = evaluate(model, val_dataloader, device)        f1 = compute_f1(predictions, val_labels)        print(f\"Validation F1: {f1:.4f}\")\n\n评估指标import reimport stringfrom collections import Counterdef normalize_answer(s: str) -&gt; str:    \"\"\"标准化答案文本\"\"\"    s = s.lower()    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)    s = ''.join(ch for ch in s if ch not in string.punctuation)    s = ' '.join(s.split())    return sdef compute_f1(prediction: str, ground_truth: str) -&gt; float:    pred_tokens = normalize_answer(prediction).split()    gold_tokens = normalize_answer(ground_truth).split()        if not pred_tokens or not gold_tokens:        return int(pred_tokens == gold_tokens)        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        if precision + recall == 0:        return 0        return 2 * precision * recall / (precision + recall)def compute_em(prediction: str, ground_truth: str) -&gt; float:    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\n与现代方法对比\n\n\n方面\n经典 MRC (BiDAF)\nBERT-based\nLLM-based\n\n\n\n参数量\n~2M\n110M-340M\n7B-70B+\n\n\n训练数据\nTask-specific\n预训练+微调\n大规模预训练\n\n\n推理方式\nSpan extraction\nSpan extraction\nGeneration\n\n\n长文档\n需要切分\n需要切分\n更大上下文窗口\n\n\n多跳推理\n困难\n有限\n较好\n\n\n生产环境优化量化推理import torch.quantization as quant# 动态量化model_int8 = quant.quantize_dynamic(    model.cpu(),    {nn.Linear},    dtype=torch.qint8)\n\nONNX 导出import torch.onnxdummy_input = {    'input_ids': torch.ones(1, 512, dtype=torch.long),    'attention_mask': torch.ones(1, 512, dtype=torch.long),    'token_type_ids': torch.zeros(1, 512, dtype=torch.long),}torch.onnx.export(    model,    (dummy_input['input_ids'], dummy_input['attention_mask'], dummy_input['token_type_ids']),    \"mrc_model.onnx\",    input_names=['input_ids', 'attention_mask', 'token_type_ids'],    output_names=['start_logits', 'end_logits'],    dynamic_axes={        'input_ids': {0: 'batch', 1: 'seq'},        'attention_mask': {0: 'batch', 1: 'seq'},        'token_type_ids': {0: 'batch', 1: 'seq'},    })\n\n延伸阅读\nHugging Face Question Answering\nSQuAD Leaderboard\nCMRC 2018 中文阅读理解\n\n\n\n转载请注明出处\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"机器阅读理解实战：从零构建问答系统","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%AE%9E%E8%B7%B5/","content":"本文从零开始实现一个机器阅读理解系统，涵盖数据处理、模型构建、训练和推理的完整流程。\n任务定义给定上下文  和问题 ，预测答案  在  中的位置：\n\n数据处理SQuAD 数据格式import jsonfrom dataclasses import dataclassfrom typing import List, Optional@dataclassclass Example:    context: str    question: str    answer_text: str    start_position: int    end_position: intdef load_squad(file_path: str) -&gt; List[Example]:    with open(file_path, 'r', encoding='utf-8') as f:        data = json.load(f)        examples = []    for article in data['data']:        for paragraph in article['paragraphs']:            context = paragraph['context']            for qa in paragraph['qas']:                question = qa['question']                if qa.get('is_impossible', False):                    continue                answer = qa['answers'][0]                examples.append(Example(                    context=context,                    question=question,                    answer_text=answer['text'],                    start_position=answer['answer_start'],                    end_position=answer['answer_start'] + len(answer['text'])                ))        return examples\n\nTokenizationfrom transformers import AutoTokenizerclass MRCTokenizer:    def __init__(self, model_name: str, max_length: int = 384, doc_stride: int = 128):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        self.doc_stride = doc_stride        def encode(self, example: Example):        # Tokenize question and context        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            stride=self.doc_stride,            return_overflowing_tokens=True,            return_offsets_mapping=True,            padding='max_length',        )                # 找到答案在 token 序列中的位置        offset_mapping = encoding['offset_mapping'][0]                start_token = None        end_token = None                for idx, (start, end) in enumerate(offset_mapping):            if start &lt;= example.start_position &lt; end:                start_token = idx            if start &lt; example.end_position &lt;= end:                end_token = idx                break                return {            'input_ids': encoding['input_ids'][0],            'attention_mask': encoding['attention_mask'][0],            'start_position': start_token or 0,            'end_position': end_token or 0,        }\n\n模型实现基于 BERT 的 MRC 模型import torchimport torch.nn as nnfrom transformers import AutoModelclass MRCModel(nn.Module):    def __init__(self, model_name: str, dropout: float = 0.1):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size                self.dropout = nn.Dropout(dropout)        self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        start_positions: Optional[torch.Tensor] = None,        end_positions: Optional[torch.Tensor] = None,    ):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)                # (batch, seq_len, 1) -&gt; (batch, seq_len)        start_logits = self.start_classifier(sequence_output).squeeze(-1)        end_logits = self.end_classifier(sequence_output).squeeze(-1)                # Mask padding tokens        start_logits = start_logits.masked_fill(~attention_mask.bool(), -1e9)        end_logits = end_logits.masked_fill(~attention_mask.bool(), -1e9)                loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss()            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }\n\n改进：联合 Start-End 预测class JointMRCModel(nn.Module):    \"\"\"联合预测 start 和 end，考虑 start-end 依赖\"\"\"        def __init__(self, model_name: str, max_answer_length: int = 30):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size        self.max_answer_length = max_answer_length                self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size * 2, 1)        def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        H = outputs.last_hidden_state  # (batch, seq_len, hidden)                # Start prediction        start_logits = self.start_classifier(H).squeeze(-1)                if self.training and start_positions is not None:            # 训练时使用真实的 start 位置            start_indices = start_positions.unsqueeze(-1).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)        else:            # 推理时使用预测的 start 位置            start_indices = start_logits.argmax(dim=-1, keepdim=True).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)                # End prediction conditioned on start        start_repr_expanded = start_repr.unsqueeze(1).expand(-1, H.size(1), -1)        end_input = torch.cat([H, start_repr_expanded], dim=-1)        end_logits = self.end_classifier(end_input).squeeze(-1)                # 只允许 end &gt;= start 且在 max_answer_length 范围内        # 这里简化处理，完整实现需要更复杂的 mask                return {'start_logits': start_logits, 'end_logits': end_logits}\n\n训练流程from torch.utils.data import DataLoader, Datasetfrom transformers import get_linear_schedule_with_warmupfrom tqdm import tqdmdef train(model, train_dataloader, val_dataloader, epochs=3, lr=3e-5):    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)        total_steps = len(train_dataloader) * epochs    scheduler = get_linear_schedule_with_warmup(        optimizer,         num_warmup_steps=int(0.1 * total_steps),        num_training_steps=total_steps    )        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model.to(device)        best_f1 = 0    for epoch in range(epochs):        model.train()        total_loss = 0                for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}'):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(**batch)            loss = outputs['loss']                        loss.backward()            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                        optimizer.step()            scheduler.step()            optimizer.zero_grad()                        total_loss += loss.item()                avg_loss = total_loss / len(train_dataloader)        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')                # Validation        f1 = evaluate(model, val_dataloader, device)        print(f'Validation F1: {f1:.4f}')                if f1 &gt; best_f1:            best_f1 = f1            torch.save(model.state_dict(), 'best_model.pt')        return model\n\n评估与推理import reimport stringfrom collections import Counterdef normalize_answer(s):    \"\"\"标准化答案用于评估\"\"\"    def remove_articles(text):        return re.sub(r'\\b(a|an|the)\\b', ' ', text)        def white_space_fix(text):        return ' '.join(text.split())        def remove_punc(text):        exclude = set(string.punctuation)        return ''.join(ch for ch in text if ch not in exclude)        def lower(text):        return text.lower()        return white_space_fix(remove_articles(remove_punc(lower(s))))def compute_f1(pred: str, gold: str) -&gt; float:    pred_tokens = normalize_answer(pred).split()    gold_tokens = normalize_answer(gold).split()        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        if num_same == 0:        return 0        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        return 2 * precision * recall / (precision + recall)def predict(model, tokenizer, context: str, question: str, device):    \"\"\"单条推理\"\"\"    model.eval()        encoding = tokenizer(        question, context,        max_length=384,        truncation='only_second',        return_tensors='pt'    )        encoding = {k: v.to(device) for k, v in encoding.items()}        with torch.no_grad():        outputs = model(**encoding)        start_idx = outputs['start_logits'].argmax().item()    end_idx = outputs['end_logits'].argmax().item()        # 确保 end &gt;= start    if end_idx &lt; start_idx:        end_idx = start_idx        # 解码答案    answer_tokens = encoding['input_ids'][0][start_idx:end_idx+1]    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)        return answer\n\n现代方法：使用 LLM对于更复杂的问答需求，可以使用 LLM：\nfrom openai import OpenAIdef llm_qa(context: str, question: str) -&gt; str:    client = OpenAI()        response = client.chat.completions.create(        model=\"gpt-4\",        messages=[            {\"role\": \"system\", \"content\": \"你是一个问答助手。根据给定的上下文回答问题。如果答案不在上下文中，请说'无法回答'。\"},            {\"role\": \"user\", \"content\": f\"上下文：{context}\\n\\n问题：{question}\"}        ],        temperature=0    )        return response.choices[0].message.content\n\n延伸阅读\nSQuAD Dataset\nHugging Face QA Pipeline\nNatural Questions\n\n\n\n转载请注明出处\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"机器阅读理解：从传统方法到大语言模型","url":"/2019/10/03/%E5%BD%93%E6%88%91%E4%BB%AC%E6%8A%8A%E7%9B%AE%E5%85%89%E6%94%BE%E5%9C%A8%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","content":"\n核心问题：当我们期望机器”理解”文本时，我们的期望到底是什么？\n\n机器阅读理解的演进传统 MRC (2015-2019)基于 span extraction 的方法：\n输入: Context + Question输出: (start_idx, end_idx)\n\n代表模型：BiDAF, R-Net, QANet, BERT\nLLM 时代的 MRC (2020-至今)从”抽取”到”生成”的范式转变：\n输入: Context + Question + Instruction输出: 自由形式的答案\n\n任务分类与难度\n\n\n类型\n传统方法\nLLM 方法\n难度\n\n\n\n抽取式\n✅ 擅长\n✅ 擅长\n⭐\n\n\n多跳推理\n❌ 困难\n⚠️ 有限\n⭐⭐⭐\n\n\n数值推理\n❌ 几乎不能\n⚠️ 需要 CoT\n⭐⭐⭐⭐\n\n\n常识推理\n❌ 不能\n✅ 较好\n⭐⭐⭐\n\n\n开放生成\n❌ 不能\n✅ 擅长\n⭐⭐\n\n\n现代方法：RAG检索增强生成 (Retrieval-Augmented Generation) 结合了检索和生成的优势：\nclass RAGSystem:    def __init__(self, retriever, generator):        self.retriever = retriever  # e.g., Dense Retriever        self.generator = generator  # e.g., LLM        def answer(self, question: str) -&gt; str:        # 1. 检索相关文档        docs = self.retriever.retrieve(question, top_k=5)                # 2. 构建上下文        context = \"\\n\\n\".join([d.text for d in docs])                # 3. 生成答案        prompt = f\"\"\"基于以下文档回答问题：{context}问题：{question}答案：\"\"\"                return self.generator.generate(prompt)\n\n检索器选择\n\n\n检索器\n特点\n适用场景\n\n\n\nBM25\n关键词匹配，快速\n短查询，精确匹配\n\n\nDense Retriever\n语义匹配\n语义相似查询\n\n\nColBERT\n延迟交互\n平衡效率与效果\n\n\nHybrid\n结合稀疏+稠密\n生产环境\n\n\nChain-of-Thought 推理对于需要推理的问题，CoT prompting 显著提升效果：\n# 标准 Promptingprompt_standard = \"Q: 小明有5个苹果，给了小红2个，还剩几个？\\nA:\"# Chain-of-Thought Prompting  prompt_cot = \"\"\"Q: 小明有5个苹果，给了小红2个，还剩几个？A: 让我们一步步思考：1. 小明最初有 5 个苹果2. 他给了小红 2 个苹果3. 剩余苹果数 = 5 - 2 = 3答案是 3 个苹果。\"\"\"\n\n评估指标传统指标\n𝟙\nLLM 时代的指标# 使用 LLM 作为评估器def llm_evaluate(question, gold_answer, pred_answer):    prompt = f\"\"\"评估预测答案的质量（1-5分）：问题：{question}标准答案：{gold_answer}预测答案：{pred_answer}评分标准：5分 - 完全正确且信息完整4分 - 基本正确，略有遗漏3分 - 部分正确2分 - 有相关信息但不正确1分 - 完全错误分数：\"\"\"    return llm.generate(prompt)\n\n实践建议何时用传统 MRC\n答案明确在文档中\n需要精确的位置标注\n低延迟要求\n资源受限\n\n何时用 RAG + LLM\n需要整合多个文档\n答案需要推理或总结\n开放域问答\n用户期望自然语言回答\n\n代码示例：现代 RAG 系统from langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.chat_models import ChatOpenAIfrom langchain.chains import RetrievalQA# 初始化组件embeddings = OpenAIEmbeddings()vectorstore = FAISS.load_local(\"my_index\", embeddings)llm = ChatOpenAI(model=\"gpt-4\", temperature=0)# 创建 RAG 链qa_chain = RetrievalQA.from_chain_type(    llm=llm,    chain_type=\"stuff\",  # 或 \"map_reduce\", \"refine\"    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),    return_source_documents=True)# 使用result = qa_chain({\"query\": \"什么是机器阅读理解？\"})print(result[\"result\"])\n\n延伸阅读\nSQuAD 2.0\nNatural Questions\nRAG Paper\nLangChain Documentation\n\n\n\n转载请注明出处\n\n","tags":["LLM","MRC","Deep learning","RAG"]},{"title":"开篇","url":"/2019/10/03/%E5%BC%80%E7%AF%87/","content":"各位读者朋友们大家好，我是 fooSynaptic。\n欢迎来到我的技术博客！这里记录我在 AI 和 NLP 领域的学习与思考。\n关于这个博客这个博客主要记录以下内容：\n\n自然语言处理 (NLP)：从传统方法到大语言模型\n机器学习：算法原理与实现细节\n深度学习：模型架构与训练技巧\n数学基础：线性代数、概率论、优化理论\n工程实践：Python、PyTorch、分布式训练\n\n技术栈NLP: Transformers, LLMs, RAG, Prompt EngineeringML: PyTorch, JAX, scikit-learnInfra: CUDA, Triton, vLLM, DeepSpeed\n\n关于我NLP Researcher，专注于：\n\n大语言模型 (LLM) 训练与推理优化\n检索增强生成 (RAG)\n机器阅读理解与问答系统\n\nGitHub: fooSynaptic\n\n\n欢迎交流讨论，转载请注明出处\n\n","tags":["Introduction"]},{"title":"BiDAF 论文解读：双向注意力流机制","url":"/2019/11/19/%E8%AE%BA%E6%96%87%E6%A2%97%E6%A6%82%EF%BC%9ABi-Directional-Attention-Flow-for-Machine-Comprehension/","content":"BiDAF (Bi-Directional Attention Flow) 是机器阅读理解领域的经典模型，其双向注意力机制对后续 Transformer 架构产生了深远影响。\n核心创新1. Memory-less Attention传统动态注意力 vs BiDAF 的无记忆注意力：\n\n\n\n特性\nDynamic Attention\nMemory-less Attention\n\n\n\n依赖\n前一时间步的 attended vector\n仅当前 query 和 context\n\n\n优势\n可建模时序依赖\n避免错误累积\n\n\n缺点\n错误会传播\n无法建模长程依赖\n\n\n2. 双向注意力同时计算：\n\nContext-to-Query (C2Q)：每个 context 词最相关的 query 词\nQuery-to-Context (Q2C)：对回答问题最关键的 context 词\n\n模型架构Input → Embedding → Encoding → Attention → Modeling → Output  │         │           │          │           │         │ 词向量    字符CNN     BiLSTM    双向注意力   BiLSTM   Span预测\n\n数学表达相似度矩阵：\n\n其中  是 context 表示， 是 query 表示。\nC2Q Attention：\n$$\\tilde{U}i = \\sum_j a{ij} U_j, \\quad a_i = \\text{softmax}(S_i)$$\nQ2C Attention：\n\n融合表示：\n\nPyTorch 实现import torchimport torch.nn as nnclass BiDAFAttention(nn.Module):    def __init__(self, hidden_size):        super().__init__()        self.W = nn.Linear(hidden_size * 3, 1, bias=False)        def forward(self, context, query, c_mask, q_mask):        \"\"\"        Args:            context: (batch, c_len, hidden)            query: (batch, q_len, hidden)            c_mask: (batch, c_len)            q_mask: (batch, q_len)        \"\"\"        batch, c_len, hidden = context.size()        q_len = query.size(1)                # 扩展维度以计算所有 (i, j) 对        c_expand = context.unsqueeze(2).expand(-1, -1, q_len, -1)        q_expand = query.unsqueeze(1).expand(-1, c_len, -1, -1)                # 计算相似度矩阵 S        cq = torch.cat([c_expand, q_expand, c_expand * q_expand], dim=-1)        S = self.W(cq).squeeze(-1)  # (batch, c_len, q_len)                # Mask        q_mask_expand = q_mask.unsqueeze(1).expand(-1, c_len, -1)        S = S.masked_fill(~q_mask_expand, -1e9)                # C2Q attention        a = torch.softmax(S, dim=-1)        c2q = torch.bmm(a, query)  # (batch, c_len, hidden)                # Q2C attention        b = torch.softmax(S.max(dim=-1)[0], dim=-1)        q2c = torch.bmm(b.unsqueeze(1), context)  # (batch, 1, hidden)        q2c = q2c.expand(-1, c_len, -1)                # 融合        G = torch.cat([context, c2q, context * c2q, context * q2c], dim=-1)                return G\n\n与 Transformer 的对比\n\n\n特性\nBiDAF\nTransformer\n\n\n\n注意力方向\n双向（C2Q, Q2C）\n全方向自注意力\n\n\n位置编码\nBiLSTM 隐式编码\n显式位置编码\n\n\n并行化\n受限于 RNN\n完全并行\n\n\n长距离依赖\n受限\n理论上无限\n\n\n参数量\n较少\n较多\n\n\n现代演进BiDAF 的思想在现代模型中的体现：\n1. Cross-Attention in Transformerclass CrossAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.mha = nn.MultiheadAttention(d_model, n_heads)        def forward(self, query, key_value):        # query 来自一个序列，key/value 来自另一个序列        return self.mha(query, key_value, key_value)\n\n2. FiD (Fusion-in-Decoder)用于 RAG 的架构，类似 BiDAF 的融合思想：\nclass FiD(nn.Module):    def __init__(self, encoder, decoder):        super().__init__()        self.encoder = encoder        self.decoder = decoder        def forward(self, question, passages):        # 独立编码每个 passage        encoded = []        for passage in passages:            enc = self.encoder(question + passage)            encoded.append(enc)                # 融合解码        fused = torch.cat(encoded, dim=1)        return self.decoder(fused)\n\n实验结果（原论文）在 SQuAD 1.1 上的表现：\n\n\n\n模型\nEM\nF1\n\n\n\nBiDAF\n67.7\n77.3\n\n\nBiDAF + Self Attention\n72.1\n81.1\n\n\nBERT-base\n80.8\n88.5\n\n\nGPT-4 (few-shot)\n~90\n~95\n\n\n延伸阅读\nBiDAF Paper\nAttention Is All You Need\nBERT for QA\n\n\n\n转载请注明出处\n\n","tags":["MRC","attention","deep learning"]},{"title":"矩阵分解：从 SVD 到现代 AI 应用","url":"/2019/10/03/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B9%8B%E4%B8%80%EF%BC%9ATruncate-SVD-%E5%92%8Crandom-SVD/","content":"矩阵分解是机器学习的基石技术，从传统的推荐系统到现代大语言模型的参数高效微调（LoRA），都离不开矩阵分解的思想。\n奇异值分解 (SVD)基本形式任意矩阵  可以分解为：\n\n其中：\n\n：左奇异向量（正交矩阵）\n：奇异值对角矩阵\n：右奇异向量（正交矩阵）\n\nTruncated SVD保留前  个最大奇异值：\n\n这是最优的秩  近似（Eckart-Young 定理）：\n\nRandomized SVD当矩阵规模巨大时，精确 SVD 计算代价过高。Randomized SVD 提供了高效的近似方法。\n算法实现import numpy as npfrom scipy import linalgdef randomized_svd(A, n_components, n_oversamples=10, n_iter=4):    \"\"\"    Randomized SVD for large matrices.        Args:        A: Input matrix (m x n)        n_components: Number of singular values to compute        n_oversamples: Additional random vectors for accuracy        n_iter: Number of power iterations        Returns:        U, s, Vt: Truncated SVD components    \"\"\"    m, n = A.shape    n_random = n_components + n_oversamples        # Step 1: Random projection    Q = np.random.randn(n, n_random)        # Step 2: Power iteration for accuracy    for _ in range(n_iter):        Q, _ = linalg.lu(A @ Q, permute_l=True)        Q, _ = linalg.lu(A.T @ Q, permute_l=True)        Q, _ = linalg.qr(A @ Q, mode='economic')        # Step 3: Project and compute SVD    B = Q.T @ A    Uhat, s, Vt = linalg.svd(B, full_matrices=False)    U = Q @ Uhat        return U[:, :n_components], s[:n_components], Vt[:n_components, :]\n\n复杂度对比\n\n\n方法\n时间复杂度\n空间复杂度\n\n\n\n精确 SVD\n\n\n\n\nRandomized SVD\n\n\n\n\nTruncated SVD (Lanczos)\n\n\n\n\n现代应用：LoRALoRA (Low-Rank Adaptation) 是大语言模型参数高效微调的核心技术，直接利用了低秩分解的思想。\nLoRA 原理预训练权重  固定，只训练低秩增量：\n\n其中 ，，。\n实现示例import torchimport torch.nn as nnclass LoRALayer(nn.Module):    def __init__(self, in_features, out_features, rank=4, alpha=1.0):        super().__init__()        self.rank = rank        self.alpha = alpha                # 原始权重（冻结）        self.W = nn.Linear(in_features, out_features, bias=False)        self.W.weight.requires_grad = False                # 低秩分解        self.A = nn.Linear(in_features, rank, bias=False)        self.B = nn.Linear(rank, out_features, bias=False)                # 初始化        nn.init.kaiming_uniform_(self.A.weight)        nn.init.zeros_(self.B.weight)                self.scaling = alpha / rank        def forward(self, x):        # W(x) + scaling * B(A(x))        return self.W(x) + self.scaling * self.B(self.A(x))\n\n参数效率对于 LLaMA-7B：\n\n\n\n方法\n可训练参数\n显存占用\n\n\n\n全量微调\n7B (100%)\n~140GB\n\n\nLoRA (r=8)\n4.7M (0.07%)\n~14GB\n\n\nLoRA (r=16)\n9.4M (0.13%)\n~16GB\n\n\n其他应用1. 推荐系统矩阵分解用于协同过滤：\n\n# 使用 surprise 库from surprise import SVD, Dataset, Readerreader = Reader(rating_scale=(1, 5))data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)model = SVD(n_factors=100)model.fit(trainset)\n\n2. 文本表示 (LSA)潜在语义分析：\nfrom sklearn.decomposition import TruncatedSVDfrom sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(max_features=10000)X = vectorizer.fit_transform(documents)svd = TruncatedSVD(n_components=100)X_reduced = svd.fit_transform(X)\n\n3. 图像压缩from PIL import Imageimport numpy as npdef compress_image(image_path, n_components=50):    img = np.array(Image.open(image_path).convert('L'))    U, s, Vt = np.linalg.svd(img, full_matrices=False)        # 保留前 n_components 个奇异值    compressed = U[:, :n_components] @ np.diag(s[:n_components]) @ Vt[:n_components, :]        return compressed.astype(np.uint8)\n\n数值稳定性条件数\n条件数过大会导致数值不稳定。\n正则化 SVDdef regularized_svd(A, lambda_reg=0.01):    \"\"\"Add regularization for numerical stability.\"\"\"    U, s, Vt = np.linalg.svd(A, full_matrices=False)    s_reg = s / (s**2 + lambda_reg)    return U, s_reg, Vt\n\n延伸阅读\nHalko et al., Finding Structure with Randomness (2011)\nHu et al., LoRA: Low-Rank Adaptation of Large Language Models (2021)\nNumPy SVD Documentation\n\n\n\n转载请注明出处\n\n","tags":["LLM","machine learning","linear algebra"]},{"title":"条件随机场：原理与实现","url":"/2019/11/19/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0/","content":"条件随机场 (CRF) 是序列标注的经典模型，尽管深度学习时代 BERT 等模型大放异彩，CRF 层仍然在 NER、词性标注等任务中发挥关键作用。\n为什么需要 CRF？独立分类的问题如果对每个位置独立分类：\n\n会导致标签不一致，例如：\n输入: \"北 京 是 中 国 首 都\"错误: B-LOC I-PER O B-LOC I-LOC I-LOC I-LOC正确: B-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC\n\nCRF 的解决方案CRF 建模整个序列的联合概率，考虑标签之间的转移约束。\n数学原理条件概率\n其中：\n\n：发射分数（emission score）\n：转移分数（transition score）\n：配分函数（归一化项）\n\n配分函数\n直接计算复杂度为 ，使用前向算法可降至 。\nPyTorch 实现CRF Layerimport torchimport torch.nn as nnclass CRF(nn.Module):    def __init__(self, num_tags, batch_first=True):        super().__init__()        self.num_tags = num_tags        self.batch_first = batch_first                # 转移矩阵: transitions[i, j] = 从标签 j 转移到标签 i 的分数        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))                # 起始和结束转移        self.start_transitions = nn.Parameter(torch.randn(num_tags))        self.end_transitions = nn.Parameter(torch.randn(num_tags))        def forward(self, emissions, tags, mask=None):        \"\"\"计算负对数似然损失\"\"\"        if mask is None:            mask = torch.ones_like(tags, dtype=torch.bool)                if self.batch_first:            emissions = emissions.transpose(0, 1)            tags = tags.transpose(0, 1)            mask = mask.transpose(0, 1)                # 计算分子（正确路径的分数）        numerator = self._compute_score(emissions, tags, mask)                # 计算分母（配分函数）        denominator = self._compute_normalizer(emissions, mask)                # 负对数似然        return (denominator - numerator).mean()        def _compute_score(self, emissions, tags, mask):        \"\"\"计算给定标签序列的分数\"\"\"        seq_len, batch_size = tags.shape                # 起始分数        score = self.start_transitions[tags[0]]        score += emissions[0, torch.arange(batch_size), tags[0]]                for i in range(1, seq_len):            # 转移分数 + 发射分数            score += self.transitions[tags[i], tags[i-1]] * mask[i]            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]                # 结束分数        last_tag_idx = mask.sum(dim=0) - 1        last_tags = tags.gather(0, last_tag_idx.unsqueeze(0)).squeeze(0)        score += self.end_transitions[last_tags]                return score        def _compute_normalizer(self, emissions, mask):        \"\"\"前向算法计算配分函数\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # 初始化        score = self.start_transitions + emissions[0]                for i in range(1, seq_len):            # broadcast: (batch, num_tags, 1) + (num_tags, num_tags) + (batch, 1, num_tags)            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score = torch.logsumexp(next_score, dim=1)                        # 应用 mask            score = torch.where(mask[i].unsqueeze(1), next_score, score)                # 添加结束分数        score += self.end_transitions                return torch.logsumexp(score, dim=1)        def decode(self, emissions, mask=None):        \"\"\"Viterbi 解码\"\"\"        if mask is None:            mask = torch.ones(emissions.shape[:2], dtype=torch.bool, device=emissions.device)                if self.batch_first:            emissions = emissions.transpose(0, 1)            mask = mask.transpose(0, 1)                return self._viterbi_decode(emissions, mask)        def _viterbi_decode(self, emissions, mask):        \"\"\"Viterbi 算法\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # 初始化        score = self.start_transitions + emissions[0]        history = []                for i in range(1, seq_len):            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score, indices = next_score.max(dim=1)                        score = torch.where(mask[i].unsqueeze(1), next_score, score)            history.append(indices)                # 添加结束分数        score += self.end_transitions                # 回溯        best_tags_list = []        _, best_last_tag = score.max(dim=1)                for idx in range(batch_size):            best_tags = [best_last_tag[idx].item()]            seq_length = int(mask[:, idx].sum().item())                        for hist in reversed(history[:seq_length-1]):                best_last_tag_idx = best_tags[-1]                best_tags.append(hist[idx, best_last_tag_idx].item())                        best_tags.reverse()            best_tags_list.append(best_tags)                return best_tags_list\n\n与 BiLSTM 结合class BiLSTM_CRF(nn.Module):    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):        super().__init__()        self.embedding = nn.Embedding(vocab_size, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2,                            num_layers=2, bidirectional=True, batch_first=True)        self.fc = nn.Linear(hidden_dim, num_tags)        self.crf = CRF(num_tags)        def forward(self, x, tags, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf(emissions, tags, mask)        def predict(self, x, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf.decode(emissions, mask)\n\n现代应用：BERT + CRF尽管 BERT 已经很强大，但 CRF 层仍能带来一致性提升：\nfrom transformers import BertModelclass BERT_CRF(nn.Module):    def __init__(self, bert_name, num_tags):        super().__init__()        self.bert = BertModel.from_pretrained(bert_name)        self.dropout = nn.Dropout(0.1)        self.fc = nn.Linear(self.bert.config.hidden_size, num_tags)        self.crf = CRF(num_tags)        def forward(self, input_ids, attention_mask, tags=None):        outputs = self.bert(input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)        emissions = self.fc(sequence_output)                if tags is not None:            return self.crf(emissions, tags, attention_mask.bool())        else:            return self.crf.decode(emissions, attention_mask.bool())\n\n性能对比（CoNLL-2003 NER）\n\n\n模型\nF1\n\n\n\nBiLSTM\n88.2\n\n\nBiLSTM + CRF\n90.1\n\n\nBERT\n92.4\n\n\nBERT + CRF\n92.8\n\n\nRoBERTa + CRF\n93.2\n\n\n训练技巧1. 标签平滑def label_smoothing_loss(crf, emissions, tags, mask, epsilon=0.1):    \"\"\"带标签平滑的 CRF 损失\"\"\"    nll_loss = crf(emissions, tags, mask)        # 均匀分布的损失    uniform_loss = -torch.logsumexp(emissions, dim=-1).mean()        return (1 - epsilon) * nll_loss + epsilon * uniform_loss\n\n2. 约束解码# 添加硬约束：B-X 后面只能接 I-X 或 Odef add_constraints(transitions, tag2idx):    for tag_from, idx_from in tag2idx.items():        for tag_to, idx_to in tag2idx.items():            if tag_from.startswith('B-') or tag_from.startswith('I-'):                entity = tag_from[2:]                if tag_to.startswith('I-') and tag_to[2:] != entity:                    transitions.data[idx_to, idx_from] = -1e9\n\n延伸阅读\nLafferty et al., Conditional Random Fields (2001)\nHuang et al., Bidirectional LSTM-CRF Models for Sequence Tagging (2015)\npytorch-crf Documentation\n\n\n\n转载请注明出处\n\n","tags":["NLP","machine learning","CRF"]}]