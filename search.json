[{"title":"ç¥ç»ç½‘ç»œæœºå™¨é˜…è¯»ç†è§£ï¼šä» Attention åˆ° LLM","url":"/2019/11/22/Nuural-Approaches-to-Machine-Reading-Comprehension-and-Dialogue/","content":"æœ¬æ–‡ç»¼è¿°ç¥ç»ç½‘ç»œåœ¨æœºå™¨é˜…è¯»ç†è§£å’Œå¯¹è¯ç³»ç»Ÿä¸­çš„å‘å±•å†ç¨‹ï¼Œä»æ—©æœŸçš„æ³¨æ„åŠ›æœºåˆ¶åˆ°ç°ä»£å¤§è¯­è¨€æ¨¡å‹ã€‚\nå‘å±•æ—¶é—´çº¿2015-2016: æ³¨æ„åŠ›æœºåˆ¶å…´èµ·    â””â”€â”€ Attentive Reader, Impatient Reader, BiDAF2017-2018: æ·±åº¦äº¤äº’ä¸é¢„è®­ç»ƒ    â””â”€â”€ R-Net, QANet, BERT2019-2020: å¤§è§„æ¨¡é¢„è®­ç»ƒ    â””â”€â”€ RoBERTa, ALBERT, T52021-2023: å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£    â””â”€â”€ GPT-3, ChatGPT, GPT-4, LLaMA2024-: æ£€ç´¢å¢å¼ºä¸å¤šæ¨¡æ€    â””â”€â”€ RAG, Vision-Language Models\n\næ ¸å¿ƒæŠ€æœ¯æ¼”è¿›é˜¶æ®µä¸€ï¼šæ³¨æ„åŠ›æœºåˆ¶ (2015-2017)é—®é¢˜ï¼šå¦‚ä½•è®©æ¨¡å‹â€å…³æ³¨â€ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Ÿ\n\n\nä»£è¡¨æ¨¡å‹ï¼šAttentive Reader, BiDAF\né˜¶æ®µäºŒï¼šæ·±åº¦äº¤äº’ (2017-2018)é—®é¢˜ï¼šå¦‚ä½•å»ºæ¨¡é—®é¢˜å’Œä¸Šä¸‹æ–‡çš„å¤æ‚äº¤äº’ï¼Ÿ\næŠ€æœ¯ï¼šå¤šè½®æ³¨æ„åŠ›ã€è‡ªæ³¨æ„åŠ›ã€é—¨æ§æœºåˆ¶\n# å¤šè½®æ¨ç† (R-Net é£æ ¼)for layer in range(num_layers):    # è‡ªæ³¨æ„åŠ›    context = self_attention(context, context)    # äº¤å‰æ³¨æ„åŠ›    context = cross_attention(context, question)\n\né˜¶æ®µä¸‰ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (2018-2020)èŒƒå¼è½¬å˜ï¼šä» task-specific åˆ° pretrain-finetune\n$$\\theta^* = \\arg\\min_\\theta \\mathcal{L}{task}(\\text{PLM}\\theta(x), y)$$\nä»£è¡¨æ¨¡å‹ï¼šBERT, RoBERTa, ALBERT\nfrom transformers import AutoModelForQuestionAnsweringmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")# Fine-tune on SQuAD\n\né˜¶æ®µå››ï¼šå¤§è¯­è¨€æ¨¡å‹ (2020-è‡³ä»Š)èŒƒå¼è½¬å˜ï¼šä» fine-tuning åˆ° prompting\n# Few-shot promptingprompt = \"\"\"Context: The Eiffel Tower was built in 1889.Question: When was the Eiffel Tower built?Answer: 1889Context: {context}Question: {question}Answer:\"\"\"\n\næ¶æ„å¯¹æ¯”\n\n\næ¨¡å‹\nå‚æ•°é‡\nè®­ç»ƒèŒƒå¼\nSQuAD 2.0 F1\n\n\n\nBiDAF\n~2M\nä»é›¶è®­ç»ƒ\n77.3\n\n\nBERT-base\n110M\né¢„è®­ç»ƒ+å¾®è°ƒ\n88.5\n\n\nBERT-large\n340M\né¢„è®­ç»ƒ+å¾®è°ƒ\n90.9\n\n\nRoBERTa-large\n355M\né¢„è®­ç»ƒ+å¾®è°ƒ\n91.4\n\n\nGPT-3\n175B\nFew-shot\n~88\n\n\nGPT-4\n~1.8T\nZero-shot\n~95\n\n\nç°ä»£ MRC ç³»ç»Ÿè®¾è®¡RAG æ¶æ„class ModernMRC:    def __init__(self, retriever, reader):        self.retriever = retriever  # Dense retriever        self.reader = reader        # LLM        def answer(self, question: str, knowledge_base: str = None):        # 1. æ£€ç´¢        if knowledge_base:            docs = self.retriever.retrieve(question, knowledge_base)            context = \"\\n\\n\".join([d.text for d in docs])        else:            context = \"\"                # 2. é˜…è¯»ç†è§£/ç”Ÿæˆ        prompt = self._build_prompt(question, context)        answer = self.reader.generate(prompt)                # 3. åå¤„ç†ï¼ˆå¯é€‰ï¼šéªŒè¯ã€å¼•ç”¨ï¼‰        return self._postprocess(answer, docs)        def _build_prompt(self, question, context):        if context:            return f\"\"\"Based on the following context, answer the question.Context:{context}Question: {question}Answer:\"\"\"        else:            return f\"Question: {question}\\nAnswer:\"\n\nå¤šè·³æ¨ç†class MultiHopReasoner:    def __init__(self, retriever, llm, max_hops=3):        self.retriever = retriever        self.llm = llm        self.max_hops = max_hops        def reason(self, question):        reasoning_chain = []        current_query = question                for hop in range(self.max_hops):            # æ£€ç´¢            docs = self.retriever.retrieve(current_query)                        # ç”Ÿæˆä¸­é—´æ¨ç†            intermediate = self.llm.generate(                f\"Based on: {docs}\\nQuestion: {current_query}\\n\"                f\"Provide intermediate reasoning or the final answer:\"            )                        reasoning_chain.append({                'query': current_query,                'docs': docs,                'reasoning': intermediate            })                        # æ£€æŸ¥æ˜¯å¦å·²å¾—åˆ°ç­”æ¡ˆ            if self._is_final_answer(intermediate):                break                        # ç”Ÿæˆä¸‹ä¸€è·³æŸ¥è¯¢            current_query = self._generate_next_query(question, reasoning_chain)                return self._synthesize_answer(question, reasoning_chain)\n\nå¯¹è¯ç³»ç»Ÿä¸­çš„ MRCå¯¹è¯å¼é—®ç­”class ConversationalQA:    def __init__(self, mrc_model, history_length=5):        self.mrc_model = mrc_model        self.history = []        self.history_length = history_length        def ask(self, question, context=None):        # å°†å¯¹è¯å†å²çº³å…¥é—®é¢˜        contextualized_question = self._contextualize(question)                # è·å–ç­”æ¡ˆ        answer = self.mrc_model.answer(contextualized_question, context)                # æ›´æ–°å†å²        self.history.append({'q': question, 'a': answer})        if len(self.history) &gt; self.history_length:            self.history.pop(0)                return answer        def _contextualize(self, question):        if not self.history:            return question                history_text = \"\\n\".join([            f\"Q: {turn['q']}\\nA: {turn['a']}\"            for turn in self.history        ])                return f\"Conversation history:\\n{history_text}\\n\\nCurrent question: {question}\"\n\nè¯„ä¼°ä½“ç³»ä¼ ç»ŸæŒ‡æ ‡\n\n\næŒ‡æ ‡\nå®šä¹‰\né€‚ç”¨åœºæ™¯\n\n\n\nEM\nç²¾ç¡®åŒ¹é…\næŠ½å–å¼ QA\n\n\nF1\nToken é‡å \næŠ½å–å¼ QA\n\n\nBLEU\nN-gram é‡å \nç”Ÿæˆå¼ QA\n\n\nROUGE\nå¬å›å¯¼å‘é‡å \næ‘˜è¦ã€é•¿ç­”æ¡ˆ\n\n\nLLM æ—¶ä»£æŒ‡æ ‡# LLM-as-Judgedef llm_evaluate(question, reference, prediction):    prompt = f\"\"\"Evaluate the answer quality on a scale of 1-5:Question: {question}Reference Answer: {reference}Model Answer: {prediction}Criteria:- Correctness: Is the information accurate?- Completeness: Does it fully answer the question?- Conciseness: Is it appropriately brief?Score (1-5):\"\"\"        return llm.generate(prompt)\n\nå»¶ä¼¸é˜…è¯»\nReading Wikipedia to Answer Open-Domain Questions\nRAG Paper\nHotpotQA: Multi-hop Reasoning\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","KBQA","Deep learning","LLM"]},{"title":"å¼€ç¯‡","url":"/2019/10/03/%E5%BC%80%E7%AF%87/","content":"å„ä½è¯»è€…æœ‹å‹ä»¬å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ fooSynapticã€‚\næ¬¢è¿æ¥åˆ°æˆ‘çš„æŠ€æœ¯åšå®¢ï¼è¿™é‡Œè®°å½•æˆ‘åœ¨ AI å’Œ NLP é¢†åŸŸçš„å­¦ä¹ ä¸æ€è€ƒã€‚\nå…³äºè¿™ä¸ªåšå®¢è¿™ä¸ªåšå®¢ä¸»è¦è®°å½•ä»¥ä¸‹å†…å®¹ï¼š\n\nè‡ªç„¶è¯­è¨€å¤„ç† (NLP)ï¼šä»ä¼ ç»Ÿæ–¹æ³•åˆ°å¤§è¯­è¨€æ¨¡å‹\næœºå™¨å­¦ä¹ ï¼šç®—æ³•åŸç†ä¸å®ç°ç»†èŠ‚\næ·±åº¦å­¦ä¹ ï¼šæ¨¡å‹æ¶æ„ä¸è®­ç»ƒæŠ€å·§\næ•°å­¦åŸºç¡€ï¼šçº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºã€ä¼˜åŒ–ç†è®º\nå·¥ç¨‹å®è·µï¼šPythonã€PyTorchã€åˆ†å¸ƒå¼è®­ç»ƒ\n\næŠ€æœ¯æ ˆNLP: Transformers, LLMs, RAG, Prompt EngineeringML: PyTorch, JAX, scikit-learnInfra: CUDA, Triton, vLLM, DeepSpeed\n\nå…³äºæˆ‘NLP Researcherï¼Œä¸“æ³¨äºï¼š\n\nå¤§è¯­è¨€æ¨¡å‹ (LLM) è®­ç»ƒä¸æ¨ç†ä¼˜åŒ–\næ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)\næœºå™¨é˜…è¯»ç†è§£ä¸é—®ç­”ç³»ç»Ÿ\n\nGitHub: fooSynaptic\n\n\næ¬¢è¿äº¤æµè®¨è®ºï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["Introduction"]},{"title":"æœºå™¨é˜…è¯»ç†è§£ï¼šä»ä¼ ç»Ÿæ–¹æ³•åˆ°å¤§è¯­è¨€æ¨¡å‹","url":"/2019/10/03/%E5%BD%93%E6%88%91%E4%BB%AC%E6%8A%8A%E7%9B%AE%E5%85%89%E6%94%BE%E5%9C%A8%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","content":"\næ ¸å¿ƒé—®é¢˜ï¼šå½“æˆ‘ä»¬æœŸæœ›æœºå™¨â€ç†è§£â€æ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬çš„æœŸæœ›åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ\n\næœºå™¨é˜…è¯»ç†è§£çš„æ¼”è¿›ä¼ ç»Ÿ MRC (2015-2019)åŸºäº span extraction çš„æ–¹æ³•ï¼š\nè¾“å…¥: Context + Questionè¾“å‡º: (start_idx, end_idx)\n\nä»£è¡¨æ¨¡å‹ï¼šBiDAF, R-Net, QANet, BERT\nLLM æ—¶ä»£çš„ MRC (2020-è‡³ä»Š)ä»â€æŠ½å–â€åˆ°â€ç”Ÿæˆâ€çš„èŒƒå¼è½¬å˜ï¼š\nè¾“å…¥: Context + Question + Instructionè¾“å‡º: è‡ªç”±å½¢å¼çš„ç­”æ¡ˆ\n\nä»»åŠ¡åˆ†ç±»ä¸éš¾åº¦\n\n\nç±»å‹\nä¼ ç»Ÿæ–¹æ³•\nLLM æ–¹æ³•\néš¾åº¦\n\n\n\næŠ½å–å¼\nâœ… æ“…é•¿\nâœ… æ“…é•¿\nâ­\n\n\nå¤šè·³æ¨ç†\nâŒ å›°éš¾\nâš ï¸ æœ‰é™\nâ­â­â­\n\n\næ•°å€¼æ¨ç†\nâŒ å‡ ä¹ä¸èƒ½\nâš ï¸ éœ€è¦ CoT\nâ­â­â­â­\n\n\nå¸¸è¯†æ¨ç†\nâŒ ä¸èƒ½\nâœ… è¾ƒå¥½\nâ­â­â­\n\n\nå¼€æ”¾ç”Ÿæˆ\nâŒ ä¸èƒ½\nâœ… æ“…é•¿\nâ­â­\n\n\nç°ä»£æ–¹æ³•ï¼šRAGæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation) ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆçš„ä¼˜åŠ¿ï¼š\nclass RAGSystem:    def __init__(self, retriever, generator):        self.retriever = retriever  # e.g., Dense Retriever        self.generator = generator  # e.g., LLM        def answer(self, question: str) -&gt; str:        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£        docs = self.retriever.retrieve(question, top_k=5)                # 2. æ„å»ºä¸Šä¸‹æ–‡        context = \"\\n\\n\".join([d.text for d in docs])                # 3. ç”Ÿæˆç­”æ¡ˆ        prompt = f\"\"\"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š{context}é—®é¢˜ï¼š{question}ç­”æ¡ˆï¼š\"\"\"                return self.generator.generate(prompt)\n\næ£€ç´¢å™¨é€‰æ‹©\n\n\næ£€ç´¢å™¨\nç‰¹ç‚¹\né€‚ç”¨åœºæ™¯\n\n\n\nBM25\nå…³é”®è¯åŒ¹é…ï¼Œå¿«é€Ÿ\nçŸ­æŸ¥è¯¢ï¼Œç²¾ç¡®åŒ¹é…\n\n\nDense Retriever\nè¯­ä¹‰åŒ¹é…\nè¯­ä¹‰ç›¸ä¼¼æŸ¥è¯¢\n\n\nColBERT\nå»¶è¿Ÿäº¤äº’\nå¹³è¡¡æ•ˆç‡ä¸æ•ˆæœ\n\n\nHybrid\nç»“åˆç¨€ç–+ç¨ å¯†\nç”Ÿäº§ç¯å¢ƒ\n\n\nChain-of-Thought æ¨ç†å¯¹äºéœ€è¦æ¨ç†çš„é—®é¢˜ï¼ŒCoT prompting æ˜¾è‘—æå‡æ•ˆæœï¼š\n# æ ‡å‡† Promptingprompt_standard = \"Q: å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œç»™äº†å°çº¢2ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ\\nA:\"# Chain-of-Thought Prompting  prompt_cot = \"\"\"Q: å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œç»™äº†å°çº¢2ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼ŸA: è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š1. å°æ˜æœ€åˆæœ‰ 5 ä¸ªè‹¹æœ2. ä»–ç»™äº†å°çº¢ 2 ä¸ªè‹¹æœ3. å‰©ä½™è‹¹æœæ•° = 5 - 2 = 3ç­”æ¡ˆæ˜¯ 3 ä¸ªè‹¹æœã€‚\"\"\"\n\nè¯„ä¼°æŒ‡æ ‡ä¼ ç»ŸæŒ‡æ ‡\nğŸ™\nLLM æ—¶ä»£çš„æŒ‡æ ‡# ä½¿ç”¨ LLM ä½œä¸ºè¯„ä¼°å™¨def llm_evaluate(question, gold_answer, pred_answer):    prompt = f\"\"\"è¯„ä¼°é¢„æµ‹ç­”æ¡ˆçš„è´¨é‡ï¼ˆ1-5åˆ†ï¼‰ï¼šé—®é¢˜ï¼š{question}æ ‡å‡†ç­”æ¡ˆï¼š{gold_answer}é¢„æµ‹ç­”æ¡ˆï¼š{pred_answer}è¯„åˆ†æ ‡å‡†ï¼š5åˆ† - å®Œå…¨æ­£ç¡®ä¸”ä¿¡æ¯å®Œæ•´4åˆ† - åŸºæœ¬æ­£ç¡®ï¼Œç•¥æœ‰é—æ¼3åˆ† - éƒ¨åˆ†æ­£ç¡®2åˆ† - æœ‰ç›¸å…³ä¿¡æ¯ä½†ä¸æ­£ç¡®1åˆ† - å®Œå…¨é”™è¯¯åˆ†æ•°ï¼š\"\"\"    return llm.generate(prompt)\n\nå®è·µå»ºè®®ä½•æ—¶ç”¨ä¼ ç»Ÿ MRC\nç­”æ¡ˆæ˜ç¡®åœ¨æ–‡æ¡£ä¸­\néœ€è¦ç²¾ç¡®çš„ä½ç½®æ ‡æ³¨\nä½å»¶è¿Ÿè¦æ±‚\nèµ„æºå—é™\n\nä½•æ—¶ç”¨ RAG + LLM\néœ€è¦æ•´åˆå¤šä¸ªæ–‡æ¡£\nç­”æ¡ˆéœ€è¦æ¨ç†æˆ–æ€»ç»“\nå¼€æ”¾åŸŸé—®ç­”\nç”¨æˆ·æœŸæœ›è‡ªç„¶è¯­è¨€å›ç­”\n\nä»£ç ç¤ºä¾‹ï¼šç°ä»£ RAG ç³»ç»Ÿfrom langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.chat_models import ChatOpenAIfrom langchain.chains import RetrievalQA# åˆå§‹åŒ–ç»„ä»¶embeddings = OpenAIEmbeddings()vectorstore = FAISS.load_local(\"my_index\", embeddings)llm = ChatOpenAI(model=\"gpt-4\", temperature=0)# åˆ›å»º RAG é“¾qa_chain = RetrievalQA.from_chain_type(    llm=llm,    chain_type=\"stuff\",  # æˆ– \"map_reduce\", \"refine\"    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),    return_source_documents=True)# ä½¿ç”¨result = qa_chain({\"query\": \"ä»€ä¹ˆæ˜¯æœºå™¨é˜…è¯»ç†è§£ï¼Ÿ\"})print(result[\"result\"])\n\nå»¶ä¼¸é˜…è¯»\nSQuAD 2.0\nNatural Questions\nRAG Paper\nLangChain Documentation\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","Deep learning","LLM","RAG"]},{"title":"MRC æ¨¡å‹å®ç°ï¼šä» TensorFlow åˆ° PyTorch","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E5%8E%BB%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%87%E6%9C%AC%E5%B9%B6%E4%B8%94%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%EF%BC%88tensorflow%E5%AE%9E%E6%88%98%EF%BC%89/","content":"æœ¬æ–‡ä»‹ç»æœºå™¨é˜…è¯»ç†è§£æ¨¡å‹çš„å®Œæ•´å®ç°ï¼Œæ¶µç›–ç»å…¸æ¶æ„å’Œç°ä»£æœ€ä½³å®è·µã€‚\né—®é¢˜å®šä¹‰è¾“å…¥ï¼š\n\né—®é¢˜ \næ–‡æ¡£ \n\nè¾“å‡ºï¼š\n\nç­”æ¡ˆèµ·å§‹ä½ç½® \nç­”æ¡ˆç»“æŸä½ç½® \n\nç»å…¸æ¶æ„Input â†’ Embedding â†’ Encoding â†’ Matching â†’ Fusion â†’ Decoding\n\nå„å±‚è¯¦è§£\n\n\nå±‚\nåŠŸèƒ½\nç°ä»£æ›¿ä»£\n\n\n\nEmbedding\nToken â†’ Vector\nSubword Tokenization\n\n\nEncoding\nåºåˆ—ç¼–ç \nTransformer Encoder\n\n\nMatching\nQ-P äº¤äº’\nCross-Attention\n\n\nFusion\nä¿¡æ¯èåˆ\nSelf-Attention\n\n\nDecoding\nSpan é¢„æµ‹\nLinear + Softmax\n\n\nPyTorch å®ç°å®Œæ•´æ¨¡å‹import torchimport torch.nn as nnimport torch.nn.functional as Ffrom transformers import AutoModel, AutoTokenizerclass MRCModel(nn.Module):    \"\"\"åŸºäº Transformer çš„ MRC æ¨¡å‹\"\"\"        def __init__(        self,         model_name: str = \"bert-base-chinese\",        dropout: float = 0.1,        max_answer_length: int = 30    ):        super().__init__()        self.encoder = AutoModel.from_pretrained(model_name)        hidden_size = self.encoder.config.hidden_size        self.max_answer_length = max_answer_length                self.dropout = nn.Dropout(dropout)        self.start_fc = nn.Linear(hidden_size, 1)        self.end_fc = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        token_type_ids: torch.Tensor = None,        start_positions: torch.Tensor = None,        end_positions: torch.Tensor = None,    ):        # ç¼–ç         outputs = self.encoder(            input_ids=input_ids,            attention_mask=attention_mask,            token_type_ids=token_type_ids,        )        sequence_output = self.dropout(outputs.last_hidden_state)                # é¢„æµ‹ start/end        start_logits = self.start_fc(sequence_output).squeeze(-1)        end_logits = self.end_fc(sequence_output).squeeze(-1)                # Mask padding        mask = attention_mask.bool()        start_logits = start_logits.masked_fill(~mask, float('-inf'))        end_logits = end_logits.masked_fill(~mask, float('-inf'))                # è®¡ç®—æŸå¤±        loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }        def decode(        self,        start_logits: torch.Tensor,        end_logits: torch.Tensor,        attention_mask: torch.Tensor,    ):        \"\"\"è§£ç æœ€ä½³ç­”æ¡ˆ span\"\"\"        batch_size, seq_len = start_logits.shape                # è®¡ç®—æ‰€æœ‰æœ‰æ•ˆ (start, end) å¯¹çš„åˆ†æ•°        start_probs = F.softmax(start_logits, dim=-1)        end_probs = F.softmax(end_logits, dim=-1)                results = []        for b in range(batch_size):            best_score = float('-inf')            best_start, best_end = 0, 0                        for start in range(seq_len):                if not attention_mask[b, start]:                    continue                for end in range(start, min(start + self.max_answer_length, seq_len)):                    if not attention_mask[b, end]:                        continue                    score = start_probs[b, start] * end_probs[b, end]                    if score &gt; best_score:                        best_score = score                        best_start, best_end = start, end                        results.append((best_start, best_end))                return results\n\næ•°æ®å¤„ç†from dataclasses import dataclassfrom typing import List, Optionalimport json@dataclassclass MRCExample:    qid: str    question: str    context: str    answer: Optional[str] = None    start_position: Optional[int] = None@dataclassclass MRCFeature:    input_ids: List[int]    attention_mask: List[int]    token_type_ids: List[int]    start_position: int    end_position: int    offset_mapping: List[tuple]class MRCProcessor:    def __init__(self, model_name: str, max_length: int = 512):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        def process(self, example: MRCExample) -&gt; MRCFeature:        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            return_offsets_mapping=True,            padding='max_length',        )                # å®šä½ç­”æ¡ˆä½ç½®        start_token, end_token = 0, 0        if example.start_position is not None:            offset = encoding['offset_mapping']            for idx, (start, end) in enumerate(offset):                if start &lt;= example.start_position &lt; end:                    start_token = idx                if start &lt; example.start_position + len(example.answer) &lt;= end:                    end_token = idx                    break                return MRCFeature(            input_ids=encoding['input_ids'],            attention_mask=encoding['attention_mask'],            token_type_ids=encoding.get('token_type_ids', [0] * len(encoding['input_ids'])),            start_position=start_token,            end_position=end_token,            offset_mapping=encoding['offset_mapping'],        )\n\nè®­ç»ƒå¾ªç¯from torch.utils.data import DataLoaderfrom torch.optim import AdamWfrom transformers import get_schedulerfrom tqdm import tqdmdef train_epoch(model, dataloader, optimizer, scheduler, device):    model.train()    total_loss = 0        for batch in tqdm(dataloader, desc=\"Training\"):        batch = {k: v.to(device) for k, v in batch.items()}                outputs = model(**batch)        loss = outputs['loss']                loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                optimizer.step()        scheduler.step()        optimizer.zero_grad()                total_loss += loss.item()        return total_loss / len(dataloader)def evaluate(model, dataloader, device):    model.eval()    predictions = []        with torch.no_grad():        for batch in tqdm(dataloader, desc=\"Evaluating\"):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(                input_ids=batch['input_ids'],                attention_mask=batch['attention_mask'],                token_type_ids=batch.get('token_type_ids'),            )                        spans = model.decode(                outputs['start_logits'],                outputs['end_logits'],                batch['attention_mask'],            )            predictions.extend(spans)        return predictions# ä¸»è®­ç»ƒæµç¨‹def main():    # é…ç½®    model_name = \"bert-base-chinese\"    batch_size = 16    learning_rate = 3e-5    num_epochs = 3        # åˆå§‹åŒ–    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model = MRCModel(model_name).to(device)        # ä¼˜åŒ–å™¨    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)    scheduler = get_scheduler(        \"linear\",        optimizer=optimizer,        num_warmup_steps=500,        num_training_steps=num_epochs * len(train_dataloader),    )        # è®­ç»ƒ    for epoch in range(num_epochs):        loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")                # éªŒè¯        predictions = evaluate(model, val_dataloader, device)        f1 = compute_f1(predictions, val_labels)        print(f\"Validation F1: {f1:.4f}\")\n\nè¯„ä¼°æŒ‡æ ‡import reimport stringfrom collections import Counterdef normalize_answer(s: str) -&gt; str:    \"\"\"æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬\"\"\"    s = s.lower()    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)    s = ''.join(ch for ch in s if ch not in string.punctuation)    s = ' '.join(s.split())    return sdef compute_f1(prediction: str, ground_truth: str) -&gt; float:    pred_tokens = normalize_answer(prediction).split()    gold_tokens = normalize_answer(ground_truth).split()        if not pred_tokens or not gold_tokens:        return int(pred_tokens == gold_tokens)        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        if precision + recall == 0:        return 0        return 2 * precision * recall / (precision + recall)def compute_em(prediction: str, ground_truth: str) -&gt; float:    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\nä¸ç°ä»£æ–¹æ³•å¯¹æ¯”\n\n\næ–¹é¢\nç»å…¸ MRC (BiDAF)\nBERT-based\nLLM-based\n\n\n\nå‚æ•°é‡\n~2M\n110M-340M\n7B-70B+\n\n\nè®­ç»ƒæ•°æ®\nTask-specific\né¢„è®­ç»ƒ+å¾®è°ƒ\nå¤§è§„æ¨¡é¢„è®­ç»ƒ\n\n\næ¨ç†æ–¹å¼\nSpan extraction\nSpan extraction\nGeneration\n\n\né•¿æ–‡æ¡£\néœ€è¦åˆ‡åˆ†\néœ€è¦åˆ‡åˆ†\næ›´å¤§ä¸Šä¸‹æ–‡çª—å£\n\n\nå¤šè·³æ¨ç†\nå›°éš¾\næœ‰é™\nè¾ƒå¥½\n\n\nç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é‡åŒ–æ¨ç†import torch.quantization as quant# åŠ¨æ€é‡åŒ–model_int8 = quant.quantize_dynamic(    model.cpu(),    {nn.Linear},    dtype=torch.qint8)\n\nONNX å¯¼å‡ºimport torch.onnxdummy_input = {    'input_ids': torch.ones(1, 512, dtype=torch.long),    'attention_mask': torch.ones(1, 512, dtype=torch.long),    'token_type_ids': torch.zeros(1, 512, dtype=torch.long),}torch.onnx.export(    model,    (dummy_input['input_ids'], dummy_input['attention_mask'], dummy_input['token_type_ids']),    \"mrc_model.onnx\",    input_names=['input_ids', 'attention_mask', 'token_type_ids'],    output_names=['start_logits', 'end_logits'],    dynamic_axes={        'input_ids': {0: 'batch', 1: 'seq'},        'attention_mask': {0: 'batch', 1: 'seq'},        'token_type_ids': {0: 'batch', 1: 'seq'},    })\n\nå»¶ä¼¸é˜…è¯»\nHugging Face Question Answering\nSQuAD Leaderboard\nCMRC 2018 ä¸­æ–‡é˜…è¯»ç†è§£\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"NLP å­¦ä¹ è·¯çº¿ï¼šä»åŸºç¡€åˆ°å¤§è¯­è¨€æ¨¡å‹","url":"/2019/10/31/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E2%80%94%E2%80%94%E8%AF%BB%E9%A6%99%E4%BE%AC%E7%A7%91%E6%8A%80%E6%9D%8E%E7%BA%A7%E4%B8%BA%E3%80%8A%E5%87%BA%E5%85%A5NLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BB%BA%E8%AE%AE%E3%80%8B%E6%96%87%E7%AB%A0/","content":"æœ¬æ–‡æ•´ç†äº† NLP é¢†åŸŸçš„å­¦ä¹ è·¯çº¿ï¼Œç»“åˆç»å…¸ç†è®ºä¸ç°ä»£å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ã€‚\næ¨èå­¦ä¹ èµ„æºç»å…¸æ•™æ\n\n\nä¹¦ç±\nå†…å®¹\néš¾åº¦\n\n\n\nSpeech and Language Processing (Jurafsky)\nNLP å…¨é¢ç»¼è¿°\nâ­â­\n\n\nIntroduction to Information Retrieval\nä¿¡æ¯æ£€ç´¢åŸºç¡€\nâ­â­\n\n\nPattern Recognition and Machine Learning\næœºå™¨å­¦ä¹ ç†è®º\nâ­â­â­â­\n\n\nDeep Learning (Goodfellow)\næ·±åº¦å­¦ä¹ åŸºç¡€\nâ­â­â­\n\n\nç°ä»£èµ„æº\nStanford CS224N: NLP with Deep Learning\nHugging Face Course\nLLM University by Cohere\n\né˜¶æ®µä¸€ï¼šNLP åŸºç¡€è¯­è¨€æ¨¡å‹åŸºç¡€N-gram æ¨¡å‹ï¼šN-1 é˜¶é©¬å°”å¯å¤«å‡è®¾\n\nfrom collections import defaultdictimport numpy as npclass NGramLM:    def __init__(self, n=3):        self.n = n        self.counts = defaultdict(lambda: defaultdict(int))        self.totals = defaultdict(int)        def train(self, corpus):        for sentence in corpus:            tokens = ['&lt;s&gt;'] * (self.n - 1) + sentence + ['&lt;/s&gt;']            for i in range(len(tokens) - self.n + 1):                context = tuple(tokens[i:i+self.n-1])                word = tokens[i+self.n-1]                self.counts[context][word] += 1                self.totals[context] += 1        def probability(self, word, context):        context = tuple(context[-(self.n-1):])        return self.counts[context][word] / max(self.totals[context], 1)\n\nè¯å‘é‡ä» One-hot åˆ° Dense Embedding çš„æ¼”è¿›ï¼š\n\n\n\næ–¹æ³•\nå¹´ä»½\nç‰¹ç‚¹\n\n\n\nOne-hot\n-\nç¨€ç–ï¼Œæ— è¯­ä¹‰\n\n\nWord2Vec\n2013\nåˆ†å¸ƒå¼è¡¨ç¤º\n\n\nGloVe\n2014\nå…¨å±€ç»Ÿè®¡\n\n\nFastText\n2016\nå­è¯ä¿¡æ¯\n\n\nELMo\n2018\nä¸Šä¸‹æ–‡ç›¸å…³\n\n\nBERT\n2018\nåŒå‘ä¸Šä¸‹æ–‡\n\n\né˜¶æ®µäºŒï¼šæ·±åº¦å­¦ä¹  NLPTransformer æ¶æ„import torchimport torch.nn as nnimport mathclass MultiHeadAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.d_k = d_model // n_heads        self.n_heads = n_heads                self.W_q = nn.Linear(d_model, d_model)        self.W_k = nn.Linear(d_model, d_model)        self.W_v = nn.Linear(d_model, d_model)        self.W_o = nn.Linear(d_model, d_model)        def forward(self, Q, K, V, mask=None):        batch_size = Q.size(0)                # Linear projections        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)                # Attention scores        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)                if mask is not None:            scores = scores.masked_fill(mask == 0, -1e9)                attn = torch.softmax(scores, dim=-1)        output = torch.matmul(attn, V)                # Concatenate and project        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)        return self.W_o(output)\n\næ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦è¡¨è¾¾\né˜¶æ®µä¸‰ï¼šå¤§è¯­è¨€æ¨¡å‹LLM æ¶æ„æ¼”è¿›GPT-1 (2018) â†’ GPT-2 â†’ GPT-3 â†’ ChatGPT â†’ GPT-4     â†“BERT â†’ RoBERTa â†’ DeBERTa     â†“T5 â†’ Flan-T5 â†’ UL2     â†“LLaMA â†’ LLaMA 2 â†’ Mistral â†’ Mixtral\n\nPrompt Engineering# 1. Zero-shotprompt = \"Translate to French: Hello, how are you?\"# 2. Few-shotprompt = \"\"\"Translate to French:Hello -&gt; BonjourGoodbye -&gt; Au revoirHow are you? -&gt;\"\"\"# 3. Chain-of-Thoughtprompt = \"\"\"Q: If I have 3 apples and buy 5 more, how many do I have?A: Let's think step by step.1. I start with 3 apples.2. I buy 5 more apples.3. Total = 3 + 5 = 8 apples.The answer is 8.Q: If I have 7 oranges and eat 2, how many remain?A: Let's think step by step.\"\"\"\n\nFine-tuning æŠ€æœ¯\n\n\næ–¹æ³•\nå¯è®­ç»ƒå‚æ•°\né€‚ç”¨åœºæ™¯\n\n\n\nFull Fine-tuning\n100%\nå¤§é‡æ•°æ®ï¼Œå……è¶³ç®—åŠ›\n\n\nLoRA\n0.1-1%\nèµ„æºå—é™\n\n\nQLoRA\n0.1%\næ¶ˆè´¹çº§ GPU\n\n\nPrefix Tuning\n0.1%\nå¤šä»»åŠ¡\n\n\nPrompt Tuning\n&lt;0.01%\næç«¯èµ„æºå—é™\n\n\nfrom peft import LoraConfig, get_peft_modellora_config = LoraConfig(    r=8,    lora_alpha=32,    target_modules=[\"q_proj\", \"v_proj\"],    lora_dropout=0.1,    bias=\"none\",)model = get_peft_model(base_model, lora_config)print(f\"Trainable params: {model.print_trainable_parameters()}\")\n\né˜¶æ®µå››ï¼šé«˜çº§ä¸»é¢˜æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)from langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.vectorstores import Chromafrom langchain.chains import RetrievalQA# æ„å»ºå‘é‡åº“embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh\")vectorstore = Chroma.from_documents(documents, embeddings)# åˆ›å»º RAG é“¾qa = RetrievalQA.from_chain_type(    llm=llm,    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}))\n\næ¨¡å‹è¯„ä¼°# å›°æƒ‘åº¦ (Perplexity)def perplexity(model, tokenizer, text):    encodings = tokenizer(text, return_tensors='pt')    max_length = model.config.n_positions        nlls = []    for i in range(0, encodings.input_ids.size(1), max_length):        begin_loc = max(i - max_length, 0)        end_loc = i + max_length        input_ids = encodings.input_ids[:, begin_loc:end_loc]        target_ids = input_ids.clone()        target_ids[:, :-1] = -100                with torch.no_grad():            outputs = model(input_ids, labels=target_ids)            nlls.append(outputs.loss)        return torch.exp(torch.stack(nlls).mean())\n\nå®è·µé¡¹ç›®å»ºè®®\nå…¥é—¨ï¼šæƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»\nè¿›é˜¶ï¼šå‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘\né«˜çº§ï¼šé—®ç­”ç³»ç»Ÿã€RAG åº”ç”¨\nä¸“å®¶ï¼šLLM é¢„è®­ç»ƒã€RLHF\n\nå»¶ä¼¸é˜…è¯»\nAttention Is All You Need\nBERT Paper\nLLaMA Paper\nLoRA Paper\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["LLM","NLP","machine learning"]},{"title":"å› æœå…³ç³»æ¨æ–­ä»‹ç»","url":"/2019/10/03/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/","content":"å› æœæ¨æ–­æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œç†è§£å› æœå…³ç³»å¯¹äºæ„å»ºå¯è§£é‡Šã€å¯ä¿¡èµ–çš„ AI ç³»ç»Ÿè‡³å…³é‡è¦ã€‚\nä¸ºä»€ä¹ˆéœ€è¦å› æœæ¨æ–­ï¼Ÿä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¾èµ–ç›¸å…³æ€§ï¼Œä½†ç›¸å…³æ€§ä¸ç­‰äºå› æœæ€§ã€‚ä¾‹å¦‚ï¼š\n\nå†°æ·‡æ·‹é”€é‡ä¸æººæ°´äº‹ä»¶æ­£ç›¸å…³ï¼ˆå…±åŒåŸå› ï¼šå¤å¤©ï¼‰\nLLM å¯èƒ½å­¦åˆ°è™šå‡ç›¸å…³æ€§ï¼Œå¯¼è‡´ hallucination\n\nå› æœæ¨æ–­å¸®åŠ©æˆ‘ä»¬ï¼š\n\nç†è§£å¹²é¢„æ•ˆæœï¼ˆå¦‚æœæˆ‘åš Xï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿï¼‰\nè¿›è¡Œåäº‹å®æ¨ç†ï¼ˆå¦‚æœå½“æ—¶åšäº† Yï¼Œç»“æœä¼šæ€æ ·ï¼Ÿï¼‰\næ„å»ºæ›´é²æ£’çš„æ¨¡å‹\n\næ ¸å¿ƒæ¦‚å¿µå› æœå›¾ (Causal Graph)ä½¿ç”¨æœ‰å‘æ— ç¯å›¾ (DAG) è¡¨ç¤ºå˜é‡ä¹‹é—´çš„å› æœå…³ç³»ï¼š\nX â†’ Y â†’ Z    (é“¾å¼ç»“æ„)X â† W â†’ Y    (æ··æ‚ç»“æ„)  X â†’ W â† Y    (å¯¹æ’ç»“æ„)\n\nç»“æ„å› æœæ¨¡å‹ (SCM)\nå…¶ä¸­  æ˜¯åŸå› ï¼Œ æ˜¯ç»“æœï¼Œ æ˜¯å™ªå£°é¡¹ã€‚\ndo ç®—å­ä¸å¹²é¢„åŒºåˆ†è§‚æµ‹å’Œå¹²é¢„ï¼š\n\nè§‚æµ‹ï¼š â€” çœ‹åˆ° X=x æ—¶ Y çš„åˆ†å¸ƒ\nå¹²é¢„ï¼š â€” å¼ºåˆ¶è®¾ç½® X=x æ—¶ Y çš„åˆ†å¸ƒ\n\nå› æœå‘ç°ç®—æ³•PC ç®—æ³•åŸºäºæ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒçš„ç»å…¸ç®—æ³•ï¼š\n# PC ç®—æ³•ä¼ªä»£ç def pc_algorithm(data, alpha=0.05):    # 1. åˆå§‹åŒ–å®Œå…¨å›¾    G = complete_graph(variables)        # 2. éª¨æ¶å­¦ä¹ ï¼šç§»é™¤æ¡ä»¶ç‹¬ç«‹çš„è¾¹    for (X, Y) in edges(G):        for S in subsets(neighbors):            if conditional_independent(X, Y, S, alpha):                remove_edge(G, X, Y)                sep_set[X, Y] = S        # 3. æ–¹å‘ç¡®å®šï¼šè¯†åˆ« v-structure    orient_v_structures(G, sep_set)        return G\n\nPython å®ç°å‚è€ƒï¼šfooSynaptic/py_pcalg\nç°ä»£æ–¹æ³•\n\n\næ–¹æ³•\nç‰¹ç‚¹\né€‚ç”¨åœºæ™¯\n\n\n\nNOTEARS\nè¿ç»­ä¼˜åŒ–ï¼Œå¯å¾®åˆ†\nçº¿æ€§/éçº¿æ€§å› æœå‘ç°\n\n\nDAG-GNN\nåŸºäºå›¾ç¥ç»ç½‘ç»œ\nå¤§è§„æ¨¡å› æœå›¾å­¦ä¹ \n\n\nCausal Transformer\nç»“åˆæ³¨æ„åŠ›æœºåˆ¶\næ—¶åºå› æœæ¨æ–­\n\n\nå› æœæ¨æ–­ä¸å¤§è¯­è¨€æ¨¡å‹LLM ä¸­çš„å› æœé—®é¢˜\nHallucinationï¼šæ¨¡å‹å­¦åˆ°è™šå‡ç›¸å…³æ€§\nBiasï¼šè®­ç»ƒæ•°æ®ä¸­çš„æ··æ‚å› ç´ \nRobustnessï¼šåˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›å·®\n\nè§£å†³æ–¹æ¡ˆ# å› æœæç¤º (Causal Prompting) ç¤ºä¾‹prompt = \"\"\"è¯·åˆ†æä»¥ä¸‹äº‹ä»¶çš„å› æœå…³ç³»ï¼Œè€Œéç›¸å…³æ€§ï¼šäº‹ä»¶A: å…¬å¸å¢åŠ å¹¿å‘ŠæŠ•å…¥äº‹ä»¶B: é”€å”®é¢ä¸Šå‡é—®ï¼šA æ˜¯å¦å¯¼è‡´äº† Bï¼Ÿè¯·è€ƒè™‘å¯èƒ½çš„æ··æ‚å› ç´ ã€‚\"\"\"\n\nå› æœæ¨ç†å¢å¼º RAGclass CausalRAG:    def __init__(self, retriever, causal_graph):        self.retriever = retriever        self.causal_graph = causal_graph        def retrieve(self, query):        # 1. è¯†åˆ«æŸ¥è¯¢ä¸­çš„å› æœå…³ç³»        cause, effect = extract_causal_pair(query)                # 2. åŸºäºå› æœå›¾è¿‡æ»¤æ— å…³æ–‡æ¡£        relevant_vars = self.causal_graph.ancestors(effect)                # 3. æ£€ç´¢å› æœç›¸å…³çš„æ–‡æ¡£        docs = self.retriever.search(query)        return filter_by_causal_relevance(docs, relevant_vars)\n\nå·¥å…·ä¸èµ„æº\n\n\nå·¥å…·\nè¯­è¨€\nåŠŸèƒ½\n\n\n\nDoWhy\nPython\nå› æœæ¨æ–­æ¡†æ¶\n\n\nCausalNex\nPython\nè´å¶æ–¯ç½‘ç»œ + å› æœå‘ç°\n\n\npgmpy\nPython\næ¦‚ç‡å›¾æ¨¡å‹\n\n\nTetrad\nJava\nå› æœæœç´¢ç®—æ³•\n\n\n# DoWhy ç¤ºä¾‹import dowhyfrom dowhy import CausalModelmodel = CausalModel(    data=df,    treatment='treatment',    outcome='outcome',    graph='digraph {treatment -&gt; outcome; confounder -&gt; treatment; confounder -&gt; outcome}')# è¯†åˆ«å› æœæ•ˆåº”identified = model.identify_effect()# ä¼°è®¡å› æœæ•ˆåº”estimate = model.estimate_effect(identified, method_name=\"backdoor.propensity_score_matching\")\n\nå»¶ä¼¸é˜…è¯»\nJudea Pearl, The Book of Why (2018)\nPeters et al., Elements of Causal Inference (2017)\nStanford CS 228: Probabilistic Graphical Models\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["machine learning","bayesian network","causality infer"]},{"title":"çŸ©é˜µåˆ†è§£ï¼šä» SVD åˆ°ç°ä»£ AI åº”ç”¨","url":"/2019/10/03/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B9%8B%E4%B8%80%EF%BC%9ATruncate-SVD-%E5%92%8Crandom-SVD/","content":"çŸ©é˜µåˆ†è§£æ˜¯æœºå™¨å­¦ä¹ çš„åŸºçŸ³æŠ€æœ¯ï¼Œä»ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿåˆ°ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰ï¼Œéƒ½ç¦»ä¸å¼€çŸ©é˜µåˆ†è§£çš„æ€æƒ³ã€‚\nå¥‡å¼‚å€¼åˆ†è§£ (SVD)åŸºæœ¬å½¢å¼ä»»æ„çŸ©é˜µ  å¯ä»¥åˆ†è§£ä¸ºï¼š\n\nå…¶ä¸­ï¼š\n\nï¼šå·¦å¥‡å¼‚å‘é‡ï¼ˆæ­£äº¤çŸ©é˜µï¼‰\nï¼šå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µ\nï¼šå³å¥‡å¼‚å‘é‡ï¼ˆæ­£äº¤çŸ©é˜µï¼‰\n\nTruncated SVDä¿ç•™å‰  ä¸ªæœ€å¤§å¥‡å¼‚å€¼ï¼š\n\nè¿™æ˜¯æœ€ä¼˜çš„ç§©  è¿‘ä¼¼ï¼ˆEckart-Young å®šç†ï¼‰ï¼š\n\nRandomized SVDå½“çŸ©é˜µè§„æ¨¡å·¨å¤§æ—¶ï¼Œç²¾ç¡® SVD è®¡ç®—ä»£ä»·è¿‡é«˜ã€‚Randomized SVD æä¾›äº†é«˜æ•ˆçš„è¿‘ä¼¼æ–¹æ³•ã€‚\nç®—æ³•å®ç°import numpy as npfrom scipy import linalgdef randomized_svd(A, n_components, n_oversamples=10, n_iter=4):    \"\"\"    Randomized SVD for large matrices.        Args:        A: Input matrix (m x n)        n_components: Number of singular values to compute        n_oversamples: Additional random vectors for accuracy        n_iter: Number of power iterations        Returns:        U, s, Vt: Truncated SVD components    \"\"\"    m, n = A.shape    n_random = n_components + n_oversamples        # Step 1: Random projection    Q = np.random.randn(n, n_random)        # Step 2: Power iteration for accuracy    for _ in range(n_iter):        Q, _ = linalg.lu(A @ Q, permute_l=True)        Q, _ = linalg.lu(A.T @ Q, permute_l=True)        Q, _ = linalg.qr(A @ Q, mode='economic')        # Step 3: Project and compute SVD    B = Q.T @ A    Uhat, s, Vt = linalg.svd(B, full_matrices=False)    U = Q @ Uhat        return U[:, :n_components], s[:n_components], Vt[:n_components, :]\n\nå¤æ‚åº¦å¯¹æ¯”\n\n\næ–¹æ³•\næ—¶é—´å¤æ‚åº¦\nç©ºé—´å¤æ‚åº¦\n\n\n\nç²¾ç¡® SVD\n\n\n\n\nRandomized SVD\n\n\n\n\nTruncated SVD (Lanczos)\n\n\n\n\nç°ä»£åº”ç”¨ï¼šLoRALoRA (Low-Rank Adaptation) æ˜¯å¤§è¯­è¨€æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œç›´æ¥åˆ©ç”¨äº†ä½ç§©åˆ†è§£çš„æ€æƒ³ã€‚\nLoRA åŸç†é¢„è®­ç»ƒæƒé‡  å›ºå®šï¼Œåªè®­ç»ƒä½ç§©å¢é‡ï¼š\n\nå…¶ä¸­ ï¼Œï¼Œã€‚\nå®ç°ç¤ºä¾‹import torchimport torch.nn as nnclass LoRALayer(nn.Module):    def __init__(self, in_features, out_features, rank=4, alpha=1.0):        super().__init__()        self.rank = rank        self.alpha = alpha                # åŸå§‹æƒé‡ï¼ˆå†»ç»“ï¼‰        self.W = nn.Linear(in_features, out_features, bias=False)        self.W.weight.requires_grad = False                # ä½ç§©åˆ†è§£        self.A = nn.Linear(in_features, rank, bias=False)        self.B = nn.Linear(rank, out_features, bias=False)                # åˆå§‹åŒ–        nn.init.kaiming_uniform_(self.A.weight)        nn.init.zeros_(self.B.weight)                self.scaling = alpha / rank        def forward(self, x):        # W(x) + scaling * B(A(x))        return self.W(x) + self.scaling * self.B(self.A(x))\n\nå‚æ•°æ•ˆç‡å¯¹äº LLaMA-7Bï¼š\n\n\n\næ–¹æ³•\nå¯è®­ç»ƒå‚æ•°\næ˜¾å­˜å ç”¨\n\n\n\nå…¨é‡å¾®è°ƒ\n7B (100%)\n~140GB\n\n\nLoRA (r=8)\n4.7M (0.07%)\n~14GB\n\n\nLoRA (r=16)\n9.4M (0.13%)\n~16GB\n\n\nå…¶ä»–åº”ç”¨1. æ¨èç³»ç»ŸçŸ©é˜µåˆ†è§£ç”¨äºååŒè¿‡æ»¤ï¼š\n\n# ä½¿ç”¨ surprise åº“from surprise import SVD, Dataset, Readerreader = Reader(rating_scale=(1, 5))data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)model = SVD(n_factors=100)model.fit(trainset)\n\n2. æ–‡æœ¬è¡¨ç¤º (LSA)æ½œåœ¨è¯­ä¹‰åˆ†æï¼š\nfrom sklearn.decomposition import TruncatedSVDfrom sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(max_features=10000)X = vectorizer.fit_transform(documents)svd = TruncatedSVD(n_components=100)X_reduced = svd.fit_transform(X)\n\n3. å›¾åƒå‹ç¼©from PIL import Imageimport numpy as npdef compress_image(image_path, n_components=50):    img = np.array(Image.open(image_path).convert('L'))    U, s, Vt = np.linalg.svd(img, full_matrices=False)        # ä¿ç•™å‰ n_components ä¸ªå¥‡å¼‚å€¼    compressed = U[:, :n_components] @ np.diag(s[:n_components]) @ Vt[:n_components, :]        return compressed.astype(np.uint8)\n\næ•°å€¼ç¨³å®šæ€§æ¡ä»¶æ•°\næ¡ä»¶æ•°è¿‡å¤§ä¼šå¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚\næ­£åˆ™åŒ– SVDdef regularized_svd(A, lambda_reg=0.01):    \"\"\"Add regularization for numerical stability.\"\"\"    U, s, Vt = np.linalg.svd(A, full_matrices=False)    s_reg = s / (s**2 + lambda_reg)    return U, s_reg, Vt\n\nå»¶ä¼¸é˜…è¯»\nHalko et al., Finding Structure with Randomness (2011)\nHu et al., LoRA: Low-Rank Adaptation of Large Language Models (2021)\nNumPy SVD Documentation\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["LLM","machine learning","linear algebra"]},{"title":"æ¡ä»¶éšæœºåœºï¼šåŸç†ä¸å®ç°","url":"/2019/11/19/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0/","content":"æ¡ä»¶éšæœºåœº (CRF) æ˜¯åºåˆ—æ ‡æ³¨çš„ç»å…¸æ¨¡å‹ï¼Œå°½ç®¡æ·±åº¦å­¦ä¹ æ—¶ä»£ BERT ç­‰æ¨¡å‹å¤§æ”¾å¼‚å½©ï¼ŒCRF å±‚ä»ç„¶åœ¨ NERã€è¯æ€§æ ‡æ³¨ç­‰ä»»åŠ¡ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚\nä¸ºä»€ä¹ˆéœ€è¦ CRFï¼Ÿç‹¬ç«‹åˆ†ç±»çš„é—®é¢˜å¦‚æœå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åˆ†ç±»ï¼š\n\nä¼šå¯¼è‡´æ ‡ç­¾ä¸ä¸€è‡´ï¼Œä¾‹å¦‚ï¼š\nè¾“å…¥: \"åŒ— äº¬ æ˜¯ ä¸­ å›½ é¦– éƒ½\"é”™è¯¯: B-LOC I-PER O B-LOC I-LOC I-LOC I-LOCæ­£ç¡®: B-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC\n\nCRF çš„è§£å†³æ–¹æ¡ˆCRF å»ºæ¨¡æ•´ä¸ªåºåˆ—çš„è”åˆæ¦‚ç‡ï¼Œè€ƒè™‘æ ‡ç­¾ä¹‹é—´çš„è½¬ç§»çº¦æŸã€‚\næ•°å­¦åŸç†æ¡ä»¶æ¦‚ç‡\nå…¶ä¸­ï¼š\n\nï¼šå‘å°„åˆ†æ•°ï¼ˆemission scoreï¼‰\nï¼šè½¬ç§»åˆ†æ•°ï¼ˆtransition scoreï¼‰\nï¼šé…åˆ†å‡½æ•°ï¼ˆå½’ä¸€åŒ–é¡¹ï¼‰\n\né…åˆ†å‡½æ•°\nç›´æ¥è®¡ç®—å¤æ‚åº¦ä¸º ï¼Œä½¿ç”¨å‰å‘ç®—æ³•å¯é™è‡³ ã€‚\nPyTorch å®ç°CRF Layerimport torchimport torch.nn as nnclass CRF(nn.Module):    def __init__(self, num_tags, batch_first=True):        super().__init__()        self.num_tags = num_tags        self.batch_first = batch_first                # è½¬ç§»çŸ©é˜µ: transitions[i, j] = ä»æ ‡ç­¾ j è½¬ç§»åˆ°æ ‡ç­¾ i çš„åˆ†æ•°        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))                # èµ·å§‹å’Œç»“æŸè½¬ç§»        self.start_transitions = nn.Parameter(torch.randn(num_tags))        self.end_transitions = nn.Parameter(torch.randn(num_tags))        def forward(self, emissions, tags, mask=None):        \"\"\"è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±\"\"\"        if mask is None:            mask = torch.ones_like(tags, dtype=torch.bool)                if self.batch_first:            emissions = emissions.transpose(0, 1)            tags = tags.transpose(0, 1)            mask = mask.transpose(0, 1)                # è®¡ç®—åˆ†å­ï¼ˆæ­£ç¡®è·¯å¾„çš„åˆ†æ•°ï¼‰        numerator = self._compute_score(emissions, tags, mask)                # è®¡ç®—åˆ†æ¯ï¼ˆé…åˆ†å‡½æ•°ï¼‰        denominator = self._compute_normalizer(emissions, mask)                # è´Ÿå¯¹æ•°ä¼¼ç„¶        return (denominator - numerator).mean()        def _compute_score(self, emissions, tags, mask):        \"\"\"è®¡ç®—ç»™å®šæ ‡ç­¾åºåˆ—çš„åˆ†æ•°\"\"\"        seq_len, batch_size = tags.shape                # èµ·å§‹åˆ†æ•°        score = self.start_transitions[tags[0]]        score += emissions[0, torch.arange(batch_size), tags[0]]                for i in range(1, seq_len):            # è½¬ç§»åˆ†æ•° + å‘å°„åˆ†æ•°            score += self.transitions[tags[i], tags[i-1]] * mask[i]            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]                # ç»“æŸåˆ†æ•°        last_tag_idx = mask.sum(dim=0) - 1        last_tags = tags.gather(0, last_tag_idx.unsqueeze(0)).squeeze(0)        score += self.end_transitions[last_tags]                return score        def _compute_normalizer(self, emissions, mask):        \"\"\"å‰å‘ç®—æ³•è®¡ç®—é…åˆ†å‡½æ•°\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # åˆå§‹åŒ–        score = self.start_transitions + emissions[0]                for i in range(1, seq_len):            # broadcast: (batch, num_tags, 1) + (num_tags, num_tags) + (batch, 1, num_tags)            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score = torch.logsumexp(next_score, dim=1)                        # åº”ç”¨ mask            score = torch.where(mask[i].unsqueeze(1), next_score, score)                # æ·»åŠ ç»“æŸåˆ†æ•°        score += self.end_transitions                return torch.logsumexp(score, dim=1)        def decode(self, emissions, mask=None):        \"\"\"Viterbi è§£ç \"\"\"        if mask is None:            mask = torch.ones(emissions.shape[:2], dtype=torch.bool, device=emissions.device)                if self.batch_first:            emissions = emissions.transpose(0, 1)            mask = mask.transpose(0, 1)                return self._viterbi_decode(emissions, mask)        def _viterbi_decode(self, emissions, mask):        \"\"\"Viterbi ç®—æ³•\"\"\"        seq_len, batch_size, num_tags = emissions.shape                # åˆå§‹åŒ–        score = self.start_transitions + emissions[0]        history = []                for i in range(1, seq_len):            broadcast_score = score.unsqueeze(2)            broadcast_emissions = emissions[i].unsqueeze(1)                        next_score = broadcast_score + self.transitions + broadcast_emissions            next_score, indices = next_score.max(dim=1)                        score = torch.where(mask[i].unsqueeze(1), next_score, score)            history.append(indices)                # æ·»åŠ ç»“æŸåˆ†æ•°        score += self.end_transitions                # å›æº¯        best_tags_list = []        _, best_last_tag = score.max(dim=1)                for idx in range(batch_size):            best_tags = [best_last_tag[idx].item()]            seq_length = int(mask[:, idx].sum().item())                        for hist in reversed(history[:seq_length-1]):                best_last_tag_idx = best_tags[-1]                best_tags.append(hist[idx, best_last_tag_idx].item())                        best_tags.reverse()            best_tags_list.append(best_tags)                return best_tags_list\n\nä¸ BiLSTM ç»“åˆclass BiLSTM_CRF(nn.Module):    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):        super().__init__()        self.embedding = nn.Embedding(vocab_size, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2,                            num_layers=2, bidirectional=True, batch_first=True)        self.fc = nn.Linear(hidden_dim, num_tags)        self.crf = CRF(num_tags)        def forward(self, x, tags, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf(emissions, tags, mask)        def predict(self, x, mask=None):        embeddings = self.embedding(x)        lstm_out, _ = self.lstm(embeddings)        emissions = self.fc(lstm_out)                return self.crf.decode(emissions, mask)\n\nç°ä»£åº”ç”¨ï¼šBERT + CRFå°½ç®¡ BERT å·²ç»å¾ˆå¼ºå¤§ï¼Œä½† CRF å±‚ä»èƒ½å¸¦æ¥ä¸€è‡´æ€§æå‡ï¼š\nfrom transformers import BertModelclass BERT_CRF(nn.Module):    def __init__(self, bert_name, num_tags):        super().__init__()        self.bert = BertModel.from_pretrained(bert_name)        self.dropout = nn.Dropout(0.1)        self.fc = nn.Linear(self.bert.config.hidden_size, num_tags)        self.crf = CRF(num_tags)        def forward(self, input_ids, attention_mask, tags=None):        outputs = self.bert(input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)        emissions = self.fc(sequence_output)                if tags is not None:            return self.crf(emissions, tags, attention_mask.bool())        else:            return self.crf.decode(emissions, attention_mask.bool())\n\næ€§èƒ½å¯¹æ¯”ï¼ˆCoNLL-2003 NERï¼‰\n\n\næ¨¡å‹\nF1\n\n\n\nBiLSTM\n88.2\n\n\nBiLSTM + CRF\n90.1\n\n\nBERT\n92.4\n\n\nBERT + CRF\n92.8\n\n\nRoBERTa + CRF\n93.2\n\n\nè®­ç»ƒæŠ€å·§1. æ ‡ç­¾å¹³æ»‘def label_smoothing_loss(crf, emissions, tags, mask, epsilon=0.1):    \"\"\"å¸¦æ ‡ç­¾å¹³æ»‘çš„ CRF æŸå¤±\"\"\"    nll_loss = crf(emissions, tags, mask)        # å‡åŒ€åˆ†å¸ƒçš„æŸå¤±    uniform_loss = -torch.logsumexp(emissions, dim=-1).mean()        return (1 - epsilon) * nll_loss + epsilon * uniform_loss\n\n2. çº¦æŸè§£ç # æ·»åŠ ç¡¬çº¦æŸï¼šB-X åé¢åªèƒ½æ¥ I-X æˆ– Odef add_constraints(transitions, tag2idx):    for tag_from, idx_from in tag2idx.items():        for tag_to, idx_to in tag2idx.items():            if tag_from.startswith('B-') or tag_from.startswith('I-'):                entity = tag_from[2:]                if tag_to.startswith('I-') and tag_to[2:] != entity:                    transitions.data[idx_to, idx_from] = -1e9\n\nå»¶ä¼¸é˜…è¯»\nLafferty et al., Conditional Random Fields (2001)\nHuang et al., Bidirectional LSTM-CRF Models for Sequence Tagging (2015)\npytorch-crf Documentation\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["NLP","machine learning","CRF"]},{"title":"æœºå™¨é˜…è¯»ç†è§£å®æˆ˜ï¼šä»é›¶æ„å»ºé—®ç­”ç³»ç»Ÿ","url":"/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%AE%9E%E8%B7%B5/","content":"æœ¬æ–‡ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªæœºå™¨é˜…è¯»ç†è§£ç³»ç»Ÿï¼Œæ¶µç›–æ•°æ®å¤„ç†ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒå’Œæ¨ç†çš„å®Œæ•´æµç¨‹ã€‚\nä»»åŠ¡å®šä¹‰ç»™å®šä¸Šä¸‹æ–‡  å’Œé—®é¢˜ ï¼Œé¢„æµ‹ç­”æ¡ˆ  åœ¨  ä¸­çš„ä½ç½®ï¼š\n\næ•°æ®å¤„ç†SQuAD æ•°æ®æ ¼å¼import jsonfrom dataclasses import dataclassfrom typing import List, Optional@dataclassclass Example:    context: str    question: str    answer_text: str    start_position: int    end_position: intdef load_squad(file_path: str) -&gt; List[Example]:    with open(file_path, 'r', encoding='utf-8') as f:        data = json.load(f)        examples = []    for article in data['data']:        for paragraph in article['paragraphs']:            context = paragraph['context']            for qa in paragraph['qas']:                question = qa['question']                if qa.get('is_impossible', False):                    continue                answer = qa['answers'][0]                examples.append(Example(                    context=context,                    question=question,                    answer_text=answer['text'],                    start_position=answer['answer_start'],                    end_position=answer['answer_start'] + len(answer['text'])                ))        return examples\n\nTokenizationfrom transformers import AutoTokenizerclass MRCTokenizer:    def __init__(self, model_name: str, max_length: int = 384, doc_stride: int = 128):        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.max_length = max_length        self.doc_stride = doc_stride        def encode(self, example: Example):        # Tokenize question and context        encoding = self.tokenizer(            example.question,            example.context,            max_length=self.max_length,            truncation='only_second',            stride=self.doc_stride,            return_overflowing_tokens=True,            return_offsets_mapping=True,            padding='max_length',        )                # æ‰¾åˆ°ç­”æ¡ˆåœ¨ token åºåˆ—ä¸­çš„ä½ç½®        offset_mapping = encoding['offset_mapping'][0]                start_token = None        end_token = None                for idx, (start, end) in enumerate(offset_mapping):            if start &lt;= example.start_position &lt; end:                start_token = idx            if start &lt; example.end_position &lt;= end:                end_token = idx                break                return {            'input_ids': encoding['input_ids'][0],            'attention_mask': encoding['attention_mask'][0],            'start_position': start_token or 0,            'end_position': end_token or 0,        }\n\næ¨¡å‹å®ç°åŸºäº BERT çš„ MRC æ¨¡å‹import torchimport torch.nn as nnfrom transformers import AutoModelclass MRCModel(nn.Module):    def __init__(self, model_name: str, dropout: float = 0.1):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size                self.dropout = nn.Dropout(dropout)        self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size, 1)        def forward(        self,        input_ids: torch.Tensor,        attention_mask: torch.Tensor,        start_positions: Optional[torch.Tensor] = None,        end_positions: Optional[torch.Tensor] = None,    ):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        sequence_output = self.dropout(outputs.last_hidden_state)                # (batch, seq_len, 1) -&gt; (batch, seq_len)        start_logits = self.start_classifier(sequence_output).squeeze(-1)        end_logits = self.end_classifier(sequence_output).squeeze(-1)                # Mask padding tokens        start_logits = start_logits.masked_fill(~attention_mask.bool(), -1e9)        end_logits = end_logits.masked_fill(~attention_mask.bool(), -1e9)                loss = None        if start_positions is not None and end_positions is not None:            loss_fct = nn.CrossEntropyLoss()            start_loss = loss_fct(start_logits, start_positions)            end_loss = loss_fct(end_logits, end_positions)            loss = (start_loss + end_loss) / 2                return {            'loss': loss,            'start_logits': start_logits,            'end_logits': end_logits,        }\n\næ”¹è¿›ï¼šè”åˆ Start-End é¢„æµ‹class JointMRCModel(nn.Module):    \"\"\"è”åˆé¢„æµ‹ start å’Œ endï¼Œè€ƒè™‘ start-end ä¾èµ–\"\"\"        def __init__(self, model_name: str, max_answer_length: int = 30):        super().__init__()        self.bert = AutoModel.from_pretrained(model_name)        hidden_size = self.bert.config.hidden_size        self.max_answer_length = max_answer_length                self.start_classifier = nn.Linear(hidden_size, 1)        self.end_classifier = nn.Linear(hidden_size * 2, 1)        def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)        H = outputs.last_hidden_state  # (batch, seq_len, hidden)                # Start prediction        start_logits = self.start_classifier(H).squeeze(-1)                if self.training and start_positions is not None:            # è®­ç»ƒæ—¶ä½¿ç”¨çœŸå®çš„ start ä½ç½®            start_indices = start_positions.unsqueeze(-1).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)        else:            # æ¨ç†æ—¶ä½¿ç”¨é¢„æµ‹çš„ start ä½ç½®            start_indices = start_logits.argmax(dim=-1, keepdim=True).unsqueeze(-1)            start_repr = H.gather(1, start_indices.expand(-1, -1, H.size(-1))).squeeze(1)                # End prediction conditioned on start        start_repr_expanded = start_repr.unsqueeze(1).expand(-1, H.size(1), -1)        end_input = torch.cat([H, start_repr_expanded], dim=-1)        end_logits = self.end_classifier(end_input).squeeze(-1)                # åªå…è®¸ end &gt;= start ä¸”åœ¨ max_answer_length èŒƒå›´å†…        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®Œæ•´å®ç°éœ€è¦æ›´å¤æ‚çš„ mask                return {'start_logits': start_logits, 'end_logits': end_logits}\n\nè®­ç»ƒæµç¨‹from torch.utils.data import DataLoader, Datasetfrom transformers import get_linear_schedule_with_warmupfrom tqdm import tqdmdef train(model, train_dataloader, val_dataloader, epochs=3, lr=3e-5):    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)        total_steps = len(train_dataloader) * epochs    scheduler = get_linear_schedule_with_warmup(        optimizer,         num_warmup_steps=int(0.1 * total_steps),        num_training_steps=total_steps    )        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model.to(device)        best_f1 = 0    for epoch in range(epochs):        model.train()        total_loss = 0                for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}'):            batch = {k: v.to(device) for k, v in batch.items()}                        outputs = model(**batch)            loss = outputs['loss']                        loss.backward()            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)                        optimizer.step()            scheduler.step()            optimizer.zero_grad()                        total_loss += loss.item()                avg_loss = total_loss / len(train_dataloader)        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')                # Validation        f1 = evaluate(model, val_dataloader, device)        print(f'Validation F1: {f1:.4f}')                if f1 &gt; best_f1:            best_f1 = f1            torch.save(model.state_dict(), 'best_model.pt')        return model\n\nè¯„ä¼°ä¸æ¨ç†import reimport stringfrom collections import Counterdef normalize_answer(s):    \"\"\"æ ‡å‡†åŒ–ç­”æ¡ˆç”¨äºè¯„ä¼°\"\"\"    def remove_articles(text):        return re.sub(r'\\b(a|an|the)\\b', ' ', text)        def white_space_fix(text):        return ' '.join(text.split())        def remove_punc(text):        exclude = set(string.punctuation)        return ''.join(ch for ch in text if ch not in exclude)        def lower(text):        return text.lower()        return white_space_fix(remove_articles(remove_punc(lower(s))))def compute_f1(pred: str, gold: str) -&gt; float:    pred_tokens = normalize_answer(pred).split()    gold_tokens = normalize_answer(gold).split()        common = Counter(pred_tokens) &amp; Counter(gold_tokens)    num_same = sum(common.values())        if num_same == 0:        return 0        precision = num_same / len(pred_tokens)    recall = num_same / len(gold_tokens)        return 2 * precision * recall / (precision + recall)def predict(model, tokenizer, context: str, question: str, device):    \"\"\"å•æ¡æ¨ç†\"\"\"    model.eval()        encoding = tokenizer(        question, context,        max_length=384,        truncation='only_second',        return_tensors='pt'    )        encoding = {k: v.to(device) for k, v in encoding.items()}        with torch.no_grad():        outputs = model(**encoding)        start_idx = outputs['start_logits'].argmax().item()    end_idx = outputs['end_logits'].argmax().item()        # ç¡®ä¿ end &gt;= start    if end_idx &lt; start_idx:        end_idx = start_idx        # è§£ç ç­”æ¡ˆ    answer_tokens = encoding['input_ids'][0][start_idx:end_idx+1]    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)        return answer\n\nç°ä»£æ–¹æ³•ï¼šä½¿ç”¨ LLMå¯¹äºæ›´å¤æ‚çš„é—®ç­”éœ€æ±‚ï¼Œå¯ä»¥ä½¿ç”¨ LLMï¼š\nfrom openai import OpenAIdef llm_qa(context: str, question: str) -&gt; str:    client = OpenAI()        response = client.chat.completions.create(        model=\"gpt-4\",        messages=[            {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·è¯´'æ— æ³•å›ç­”'ã€‚\"},            {\"role\": \"user\", \"content\": f\"ä¸Šä¸‹æ–‡ï¼š{context}\\n\\né—®é¢˜ï¼š{question}\"}        ],        temperature=0    )        return response.choices[0].message.content\n\nå»¶ä¼¸é˜…è¯»\nSQuAD Dataset\nHugging Face QA Pipeline\nNatural Questions\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","Deep learning","PyTorch"]},{"title":"BiDAF è®ºæ–‡è§£è¯»ï¼šåŒå‘æ³¨æ„åŠ›æµæœºåˆ¶","url":"/2019/11/19/%E8%AE%BA%E6%96%87%E6%A2%97%E6%A6%82%EF%BC%9ABi-Directional-Attention-Flow-for-Machine-Comprehension/","content":"BiDAF (Bi-Directional Attention Flow) æ˜¯æœºå™¨é˜…è¯»ç†è§£é¢†åŸŸçš„ç»å…¸æ¨¡å‹ï¼Œå…¶åŒå‘æ³¨æ„åŠ›æœºåˆ¶å¯¹åç»­ Transformer æ¶æ„äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚\næ ¸å¿ƒåˆ›æ–°1. Memory-less Attentionä¼ ç»ŸåŠ¨æ€æ³¨æ„åŠ› vs BiDAF çš„æ— è®°å¿†æ³¨æ„åŠ›ï¼š\n\n\n\nç‰¹æ€§\nDynamic Attention\nMemory-less Attention\n\n\n\nä¾èµ–\nå‰ä¸€æ—¶é—´æ­¥çš„ attended vector\nä»…å½“å‰ query å’Œ context\n\n\nä¼˜åŠ¿\nå¯å»ºæ¨¡æ—¶åºä¾èµ–\né¿å…é”™è¯¯ç´¯ç§¯\n\n\nç¼ºç‚¹\né”™è¯¯ä¼šä¼ æ’­\næ— æ³•å»ºæ¨¡é•¿ç¨‹ä¾èµ–\n\n\n2. åŒå‘æ³¨æ„åŠ›åŒæ—¶è®¡ç®—ï¼š\n\nContext-to-Query (C2Q)ï¼šæ¯ä¸ª context è¯æœ€ç›¸å…³çš„ query è¯\nQuery-to-Context (Q2C)ï¼šå¯¹å›ç­”é—®é¢˜æœ€å…³é”®çš„ context è¯\n\næ¨¡å‹æ¶æ„Input â†’ Embedding â†’ Encoding â†’ Attention â†’ Modeling â†’ Output  â”‚         â”‚           â”‚          â”‚           â”‚         â”‚ è¯å‘é‡    å­—ç¬¦CNN     BiLSTM    åŒå‘æ³¨æ„åŠ›   BiLSTM   Spané¢„æµ‹\n\næ•°å­¦è¡¨è¾¾ç›¸ä¼¼åº¦çŸ©é˜µï¼š\n\nå…¶ä¸­  æ˜¯ context è¡¨ç¤ºï¼Œ æ˜¯ query è¡¨ç¤ºã€‚\nC2Q Attentionï¼š\n$$\\tilde{U}i = \\sum_j a{ij} U_j, \\quad a_i = \\text{softmax}(S_i)$$\nQ2C Attentionï¼š\n\nèåˆè¡¨ç¤ºï¼š\n\nPyTorch å®ç°import torchimport torch.nn as nnclass BiDAFAttention(nn.Module):    def __init__(self, hidden_size):        super().__init__()        self.W = nn.Linear(hidden_size * 3, 1, bias=False)        def forward(self, context, query, c_mask, q_mask):        \"\"\"        Args:            context: (batch, c_len, hidden)            query: (batch, q_len, hidden)            c_mask: (batch, c_len)            q_mask: (batch, q_len)        \"\"\"        batch, c_len, hidden = context.size()        q_len = query.size(1)                # æ‰©å±•ç»´åº¦ä»¥è®¡ç®—æ‰€æœ‰ (i, j) å¯¹        c_expand = context.unsqueeze(2).expand(-1, -1, q_len, -1)        q_expand = query.unsqueeze(1).expand(-1, c_len, -1, -1)                # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ S        cq = torch.cat([c_expand, q_expand, c_expand * q_expand], dim=-1)        S = self.W(cq).squeeze(-1)  # (batch, c_len, q_len)                # Mask        q_mask_expand = q_mask.unsqueeze(1).expand(-1, c_len, -1)        S = S.masked_fill(~q_mask_expand, -1e9)                # C2Q attention        a = torch.softmax(S, dim=-1)        c2q = torch.bmm(a, query)  # (batch, c_len, hidden)                # Q2C attention        b = torch.softmax(S.max(dim=-1)[0], dim=-1)        q2c = torch.bmm(b.unsqueeze(1), context)  # (batch, 1, hidden)        q2c = q2c.expand(-1, c_len, -1)                # èåˆ        G = torch.cat([context, c2q, context * c2q, context * q2c], dim=-1)                return G\n\nä¸ Transformer çš„å¯¹æ¯”\n\n\nç‰¹æ€§\nBiDAF\nTransformer\n\n\n\næ³¨æ„åŠ›æ–¹å‘\nåŒå‘ï¼ˆC2Q, Q2Cï¼‰\nå…¨æ–¹å‘è‡ªæ³¨æ„åŠ›\n\n\nä½ç½®ç¼–ç \nBiLSTM éšå¼ç¼–ç \næ˜¾å¼ä½ç½®ç¼–ç \n\n\nå¹¶è¡ŒåŒ–\nå—é™äº RNN\nå®Œå…¨å¹¶è¡Œ\n\n\né•¿è·ç¦»ä¾èµ–\nå—é™\nç†è®ºä¸Šæ— é™\n\n\nå‚æ•°é‡\nè¾ƒå°‘\nè¾ƒå¤š\n\n\nç°ä»£æ¼”è¿›BiDAF çš„æ€æƒ³åœ¨ç°ä»£æ¨¡å‹ä¸­çš„ä½“ç°ï¼š\n1. Cross-Attention in Transformerclass CrossAttention(nn.Module):    def __init__(self, d_model, n_heads):        super().__init__()        self.mha = nn.MultiheadAttention(d_model, n_heads)        def forward(self, query, key_value):        # query æ¥è‡ªä¸€ä¸ªåºåˆ—ï¼Œkey/value æ¥è‡ªå¦ä¸€ä¸ªåºåˆ—        return self.mha(query, key_value, key_value)\n\n2. FiD (Fusion-in-Decoder)ç”¨äº RAG çš„æ¶æ„ï¼Œç±»ä¼¼ BiDAF çš„èåˆæ€æƒ³ï¼š\nclass FiD(nn.Module):    def __init__(self, encoder, decoder):        super().__init__()        self.encoder = encoder        self.decoder = decoder        def forward(self, question, passages):        # ç‹¬ç«‹ç¼–ç æ¯ä¸ª passage        encoded = []        for passage in passages:            enc = self.encoder(question + passage)            encoded.append(enc)                # èåˆè§£ç         fused = torch.cat(encoded, dim=1)        return self.decoder(fused)\n\nå®éªŒç»“æœï¼ˆåŸè®ºæ–‡ï¼‰åœ¨ SQuAD 1.1 ä¸Šçš„è¡¨ç°ï¼š\n\n\n\næ¨¡å‹\nEM\nF1\n\n\n\nBiDAF\n67.7\n77.3\n\n\nBiDAF + Self Attention\n72.1\n81.1\n\n\nBERT-base\n80.8\n88.5\n\n\nGPT-4 (few-shot)\n~90\n~95\n\n\nå»¶ä¼¸é˜…è¯»\nBiDAF Paper\nAttention Is All You Need\nBERT for QA\n\n\n\nè½¬è½½è¯·æ³¨æ˜å‡ºå¤„\n\n","tags":["MRC","attention","deep learning"]}]