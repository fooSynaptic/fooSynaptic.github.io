<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/jojo_median.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/jojo_small.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/jojo_small.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, Jiaxin">










<meta name="description" content="Head first to the Truth as Synaptic.">
<meta property="og:type" content="website">
<meta property="og:title" content="fooSynaptic">
<meta property="og:url" content="http://fooSynaptic.com/index.html">
<meta property="og:site_name" content="fooSynaptic">
<meta property="og:description" content="Head first to the Truth as Synaptic.">
<meta property="og:locale" content="zh">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="fooSynaptic">
<meta name="twitter:description" content="Head first to the Truth as Synaptic.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://fooSynaptic.com/">





  <title>fooSynaptic</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">fooSynaptic</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Any problem, please Contact me.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/11/22/MRC_and_dialogue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/22/MRC_and_dialogue/" itemprop="url">Nuural Approaches to Machine Reading Comprehension and Dialogue</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-22T10:18:53+08:00">
                2019-11-22
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/22/MRC_and_dialogue/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/11/22/MRC_and_dialogue/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong><em>While you find <code>reading notes</code> tags in the article pages, which means these information are sketched from other sources.</em></strong></p>
<p><a href="https://berkeley-deep-learning.github.io/cs294-131-f17/slides/MSR%20and%20Dialogue%20-%20UCBerkeley%2009182017%20-%20JianfengGao.pdf" target="_blank" rel="noopener">ref</a></p>
<p>This material is presented by Jianfeng Gao from Microsoft AI&amp;Research.</p>
<hr>
<h2 id="KBQA-question-answering-on-Knowledge-Base"><a href="#KBQA-question-answering-on-Knowledge-Base" class="headerlink" title="KBQA - question answering on Knowledge Base"></a>KBQA - question answering on Knowledge Base</h2><h3 id="Large-scale-knowledge-graphs"><a href="#Large-scale-knowledge-graphs" class="headerlink" title="Large-scale knowledge graphs."></a>Large-scale knowledge graphs.</h3><ul>
<li>Properties of billionss of entities.</li>
<li>Plus relations among them.</li>
</ul>
<h2 id="Symbolic-approaches-to-QA-production-systen"><a href="#Symbolic-approaches-to-QA-production-systen" class="headerlink" title="Symbolic approaches to QA: production systen"></a>Symbolic approaches to QA: production systen</h2><ul>
<li>Production rules<br>condition-action pairs<br>Represent(world) knowledege as a graph.</li>
</ul>
<ul>
<li><p>Working memory<br>Contains a description of the current state of the world in a reasoning process.</p>
</li>
<li><p>Recongnizer-act controller<br>Update working memory by searching and firing a production rule.</p>
</li>
</ul>
<h2 id><a href="#" class="headerlink" title></a></h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/11/19/bidaf_sketch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/19/bidaf_sketch/" itemprop="url">论文梗概：Bi-Directional Attention Flow for Machine Comprehension</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-19T16:47:58+08:00">
                2019-11-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/19/bidaf_sketch/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/11/19/bidaf_sketch/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Abstract-insights"><a href="#Abstract-insights" class="headerlink" title="Abstract insights:"></a>Abstract insights:</h1><ul>
<li>requires modeling complex interactions between the context and the query.</li>
<li>use attention to focus on a small portion of the context and summarize it with a fixed-size of vector</li>
<li>muliti-stage hierachical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization.</li>
</ul>
<h1 id="Introduction-insights"><a href="#Introduction-insights" class="headerlink" title="Introduction insights:"></a>Introduction insights:</h1><h2 id="Bi-directional-attention-flow"><a href="#Bi-directional-attention-flow" class="headerlink" title="Bi-directional attention flow:"></a>Bi-directional attention flow:</h2><ul>
<li>First: the attention layer Is not used to summarize the context paragraph into a fixed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations form previous layers, is allowed to flow through to the subsequent modeling layer.(how similar to self-attention), prevent the information loss by early summary.</li>
<li>Second, we use a memory-less attention mechanism. Thai is while we iteratively compute attention through time, the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step.<br>Second mechanism forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation( the output of the attention layer). It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps.</li>
</ul>
<h3 id="keynotes"><a href="#keynotes" class="headerlink" title="keynotes:"></a>keynotes:</h3><p>Conventional dynamic attention: the attention weights at the current time step are a function of the attended vector at the previous time step.<br>bidaf: the attention is a computed for every time step, and the attended vector at each time step. The memeory-less attention: the attention at each time step is a function of only the query and the context paragraph at the current time step and doses not directly depend on the attention at the previous time step.</p>
<p><em>The author claim the memory-less attention gives a clear advanatge over dynamic attention.</em></p>
<ul>
<li>Third: the bi-direction provide complimentary information to each other.</li>
</ul>
<h1 id="BiDAF-network-Architecture"><a href="#BiDAF-network-Architecture" class="headerlink" title="BiDAF network Architecture:"></a>BiDAF network Architecture:</h1><p>First three layers, computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation of convolutional NN in the CV field.</p>
<ul>
<li>character-level</li>
<li>word-level</li>
<li>contextual embedding： utilizes contextual cues from surrounding words to refine the embedding of the words.<br>::=&gt;We use a LSTM on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions and concatenate the outputs of the two LSTMs,</li>
</ul>
<ul>
<li>Attention Flow Layer: couples the query and context vectors and produces a set of query-aware feature vectors for each word in the context.<br>α(h, u) = W[h;u;h@u]<br>Context-to-query attention. C2q attenton signifies which query words are most relevant to each context word.<br>Query-to-context(Q2C) attention signifies wich context words have the closest similarity to one of the query words and are hence critical for answering the query.</li>
</ul>
<ul>
<li>Modeling Layer: employs a Recurrent Neural Network to scan the context.</li>
<li>output Layer: provides an answer to the query(task oriented realization).</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/11/19/crf_from_scratch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/19/crf_from_scratch/" itemprop="url">条件随机场的原理以及从零实现</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-19T13:53:52+08:00">
                2019-11-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/19/crf_from_scratch/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/11/19/crf_from_scratch/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>条件随机场在概率图模型当中相对比较复杂，因此也比较难理解，随着深度学习在工业界渐渐地后劲不足，传统的概率语言模型渐渐地被程序员青睐，因此深入理解这些统计语言模型对于一个合格的算法岗位来说是非常有必要的。很多人觉得crf很难，包括我也是，但是希望通过阅读这篇文章并且了解CRF的从头实现过程能够帮助您理解条件随机场。</p>
<p>条件随机场（CRF）是给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布模型，因此广泛用于序列标注任务，例如基于crf的分词，词性标注，命名实体识别，拼写检查纠错和多音字的问题。</p>
<h1 id="条件随机场中涉及到三个问题"><a href="#条件随机场中涉及到三个问题" class="headerlink" title="条件随机场中涉及到三个问题"></a>条件随机场中涉及到三个问题</h1><ul>
<li>概率计算问题：给定的条件随机场P(Y|X)和输入序列，计算对应的输出序列的条件概率和期望值。</li>
<li>条件随机场的预测问题：给定条件随机场和输入序列，计算概率最大的输出序列。</li>
<li>条件随机场的学习问题：给定训练数据集估计条件随机场模型的参数问题。</li>
</ul>
<p>其中概率计算问题可以通过前向后向算法来完成，预测问题可以通过序列预测的标配维特比算法来完成，而条件随机场的学习问题可以有不同的优化方法，优化的目标就是期望最大化，在这里我们使用的是随机初始化每个feature到标签的权重和随机梯度下降来来优化参数来使得期望最大。</p>
<p>我们选择reference tagging作为我们的目标来完成这次从零开始的CRF学习，本质上是一个命名实体识别的任务，先看一眼数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;NEWREFERENCE&gt;</span><br><span class="line">&lt;author&gt; A. Cau, R. Kuiper, and W.-P. de Roever. &lt;/author&gt; &lt;title&gt; Formalising Dijkstra&apos;s development strategy within Stark&apos;s formalism. &lt;/title&gt; &lt;editor&gt; In C. B. Jones, R. C. Shaw, and T. Denvir, editors, &lt;/editor&gt; &lt;booktitle&gt; Proc. 5th. BCS-FACS Refinement Workshop, &lt;/booktitle&gt; &lt;date&gt; 1992. &lt;/date&gt;</span><br><span class="line">&lt;NEWREFERENCE&gt;</span><br><span class="line">&lt;author&gt; M. Kitsuregawa, H. Tanaka, and T. Moto-oka. &lt;/author&gt; &lt;title&gt; Application of hash to data base machine and its architecture. &lt;/title&gt; &lt;journal&gt; New Generation Computing, &lt;/journal&gt; &lt;volume&gt; 1(1), &lt;/volume&gt; &lt;date&gt; 1983. &lt;/date&gt;</span><br><span class="line">&lt;NEWREFERENCE&gt;</span><br><span class="line">&lt;author&gt; Alexander Vrchoticky. &lt;/author&gt; &lt;title&gt; Modula/R language definition. &lt;/title&gt; &lt;tech&gt; Technical Report TU Wien rr-02-92, version 2.0, &lt;/tech&gt; &lt;institution&gt; Dept. for Real-Time Systems, Technical University of Vienna, &lt;/institution&gt; &lt;date&gt; May 1993. &lt;/date&gt;</span><br></pre></td></tr></table></figure>

<p>如你所见在每个reference中，有标注好的author，title和journal，完整的数据中标注有8到9个，我是通过beautifulSoup来parse token和标签的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def parser(line):</span><br><span class="line">    &quot;&quot;&quot;parse a tagged reference.&quot;&quot;&quot;</span><br><span class="line">    lines = bs(line, features=&quot;lxml&quot;)</span><br><span class="line">    sublines = [_line for _line in lines.body.descendants if isinstance(_line, bs4.element.Tag)]</span><br><span class="line">    res = []</span><br><span class="line">    for p in sublines:</span><br><span class="line">        tag = p.name</span><br><span class="line">        for token in p.string.split():</span><br><span class="line">            res.append((tag, token))</span><br><span class="line">    return res</span><br></pre></td></tr></table></figure>

<p>在完整的条件随机场形式化当中，有两个特征函数，一个是状态特征函数，一个是转移特征函数，通过观察数据我采用了构词学的角度以及前后词的特征作为一个位置的全部特征，可以理解为一个特征的集合，我分别定义了词的特征函数和转移特征函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenfeatures</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="string">""" Obsevation state features """</span></span><br><span class="line">    features = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 1</span></span><br><span class="line">    features.append(<span class="string">"token=="</span> + word)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 2</span></span><br><span class="line">    mid = len(word)//<span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> mid &gt; <span class="number">0</span>:</span><br><span class="line">        features.append(<span class="string">'Swith+'</span>+word[:mid])</span><br><span class="line">        features.append(<span class="string">'Ewith+'</span>+word[mid:])</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 3</span></span><br><span class="line">    features.append(<span class="string">'abbre=='</span> + re.sub(<span class="string">'[0-9]'</span>, <span class="string">'0'</span>, re.sub(<span class="string">'[^a-zA-Z0-9()\.\,]'</span>, <span class="string">''</span>, word.lower())))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 4</span></span><br><span class="line">    fsymolic = re.findall(<span class="string">'[^a-zA-Z0-9]'</span>, word)</span><br><span class="line">    <span class="keyword">if</span> len(fsymolic) &gt; <span class="number">0</span>:</span><br><span class="line">        features.extend([<span class="string">"contains"</span> + c <span class="keyword">for</span> c <span class="keyword">in</span> fsymolic])</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Transferfeature</span><span class="params">(seq)</span>:</span></span><br><span class="line">    <span class="string">""" Transfer features """</span></span><br><span class="line">    seq.insert(<span class="number">0</span>, [<span class="string">'seq_start'</span>])</span><br><span class="line">    seq.append([<span class="string">'seq_end'</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seq)):</span><br><span class="line">        seq[i].extend(tokenfeatures(seq[i][<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># previous token feature</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(seq)):</span><br><span class="line">            seq[i].extend(<span class="string">'@-1:'</span>+f <span class="keyword">for</span> f <span class="keyword">in</span> seq[i<span class="number">-1</span>][<span class="number">1</span>:] \</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> (f.startswith(<span class="string">'@+1:'</span>) <span class="keyword">or</span> f.startswith(<span class="string">'@-1:'</span>)))</span><br><span class="line">        <span class="comment"># next token feature</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seq)<span class="number">-1</span>):</span><br><span class="line">            seq[i].extend(<span class="string">'@+1:'</span>+f <span class="keyword">for</span> f <span class="keyword">in</span> seq[i+<span class="number">1</span>][<span class="number">1</span>:] \</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> (f.startswith(<span class="string">'@+1:'</span>) <span class="keyword">or</span> f.startswith(<span class="string">'@-1:'</span>)))</span><br><span class="line">    <span class="keyword">return</span> seq</span><br></pre></td></tr></table></figure>

<p>因为CRF是一个无向图模型，所以可以定义所有观测词到状态（隐藏状态，标注特征）的一个路径。</p>
<p>观测特征的路径参考<img src="https://pic1.zhimg.com/80/v2-2a8951656433bbafaa116f1fc031d624_hd.jpg" alt="《统计学习方法》"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">linearChainCRF</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, xnode, ynode)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        xnode: the mapping from observation features to int</span></span><br><span class="line"><span class="string">        ynode: the mapping from target label to int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.K = len(ynode)              <span class="comment"># size of target label</span></span><br><span class="line">        self.M = len(xnode)	        <span class="comment"># total feature dimension length</span></span><br><span class="line">        self.featureNodes, self.labelNodes = xnode, ynode</span><br><span class="line">        self.W = np.zeros((self.M, self.K))</span><br><span class="line">        print(<span class="string">"feature dimension: "</span>, self.W.shape)</span><br></pre></td></tr></table></figure>

<p>参考上面的代码，xnode代表着特征集合，ynode代表着状态的集合，他们都会map到一个int方便我们利用数据进行访问从特征到状态的权重（其实这里的思想就是简化的条件随机场的简化形式，将两个特征函数简化成一个）</p>
<p><img src="https://pic1.zhimg.com/80/v2-f6496848d92f5721585ca56ba858b420_hd.png" alt="简化形式"></p>
<p>这里面需要披露更多的CRF的细节来帮助我们理解接下来的东西</p>
<p>1 对于每个输入观测数据的token list，将其完整映射到词属性的list集合，并且在这里我们定义了featureTable这个类的getitem方法来使其支持[]中括号operator（？？）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureTable</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokenlist, xnode, ynode)</span>:</span></span><br><span class="line">        self.seq = []</span><br><span class="line">        <span class="keyword">for</span> tokenfeatures <span class="keyword">in</span> tokenlist:</span><br><span class="line">            <span class="comment"># tokenfeatures[0] is the raw token, others are token attributes.</span></span><br><span class="line">            self.seq.append(np.array([xnode[f] <span class="keyword">for</span> f <span class="keyword">in</span> tokenfeatures], dtype=np.int32))</span><br><span class="line"></span><br><span class="line">        self.xdim, self.ydim = len(xnode), len(ynode)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        t, yp, y = item</span><br><span class="line">        ftoken = self.seq[t]</span><br><span class="line">        <span class="keyword">if</span> yp <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">            ftoken = np.append(ftoken, yp)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [(f, y) <span class="keyword">for</span> f <span class="keyword">in</span> ftoken]</span><br></pre></td></tr></table></figure>

<p>下面的几个内容基本上这篇文章的全部重心，那就是我们考虑CRF之所以不同于多分类问题和多元的逻辑斯特回归问题那就是观测状态和隐藏状态之间的约束（无向图之间的边），那么一个很重要的问题就是给定输入序列和标注序列，我们要得到在完整的图中，输入状态能够企及（可以更新）那些参数，<strong>在约束之内，条件随机场和多元的logistic回归没有差异</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edgefeatures</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This is important</span></span><br><span class="line"><span class="string">    for our raw input t1, t2, t3, t4 ...</span></span><br><span class="line"><span class="string">    transfer the token sequence information into &#123;f1, ... ft&#125;1, &#123;f1, ... ft&#125;2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.seq) == len(y), <span class="string">"seq to label not alignmentted..."</span></span><br><span class="line">    features = list(x[<span class="number">0</span>, <span class="literal">None</span>, y[<span class="number">0</span>]])   <span class="comment">#tuple list</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, x.seq.__len__()):</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> x[t, y[t<span class="number">-1</span>], y[t]]:</span><br><span class="line">            features.append(idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">currentField</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="string">"""calculate potential parameter given current obeservation data. """</span></span><br><span class="line">    <span class="string">"""x is a taggedFeatureTable object"""</span></span><br><span class="line">    N, K, W = x.seq.__len__(), self.K, self.W</span><br><span class="line"></span><br><span class="line">    <span class="comment"># estimate</span></span><br><span class="line">    g0 = empty(K)</span><br><span class="line">    g = empty((N<span class="number">-1</span>, K, K))    <span class="comment">#at timestep n, the logit of i-th label transfer to j-th label</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(K):</span><br><span class="line">        <span class="comment"># at step 0, the logit of label k (from time -1)</span></span><br><span class="line">        g0[y] = sum(W[idx[<span class="number">0</span>], idx[<span class="number">1</span>]] <span class="keyword">for</span> idx <span class="keyword">in</span> x[<span class="number">0</span>, <span class="literal">None</span>, y])</span><br><span class="line">        <span class="comment">#g0[y] = W[x[0, None, y]].sum()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, N):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(K):</span><br><span class="line">            <span class="keyword">for</span> yp <span class="keyword">in</span> range(K):</span><br><span class="line">                g[t<span class="number">-1</span>,yp,y] = sum(W[idx[<span class="number">0</span>], idx[<span class="number">1</span>]] <span class="keyword">for</span> idx <span class="keyword">in</span> x[t, yp, y])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> g0, g</span><br></pre></td></tr></table></figure>

<p>在currentField这个方法中，我们需要返回的是参数，其中go是初始状态的权重，而g是在时间步n，从状态i转移到状态j的似然概率（未归一化），得到的每个特征下的归一化概率用以计算完整的期望，参考文末代码中的Exp方法，期望的计算需要前向和后向概率。<br><img src="https://pic4.zhimg.com/80/v2-3b116e13f0af2c9b155a2157baf5b557_hd.jpg" alt="求期望"></p>
<p>下面来逐一解决crf的三个问题：</p>
<h2 id="概率计算问题"><a href="#概率计算问题" class="headerlink" title="概率计算问题"></a>概率计算问题</h2><p>给定的条件随机场P(Y|X)和输入序列，计算对应的输出序列的条件概率和期望值。对于给定label的似然估计（未归一化概率），包括了前向和后向算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logitEstimator</span><span class="params">(self, x, targetLabel)</span>:</span></span><br><span class="line">    <span class="string">"""estimate label likely-hood given W"""</span></span><br><span class="line">    N = x.seq.__len__()</span><br><span class="line">    K = self.K</span><br><span class="line">    W = self.W</span><br><span class="line">    g0, g = self.currentField(x)</span><br><span class="line">    a = self.forward(g0, g, N, K)</span><br><span class="line">    logZ = logsumexp(a[N<span class="number">-1</span>, :])</span><br><span class="line">    <span class="keyword">return</span> sum(W[k] <span class="keyword">for</span> k <span class="keyword">in</span> targetLabel) - logZ</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, g0, g, N, K)</span>:</span></span><br><span class="line">    <span class="string">"""forward stepwise"""</span></span><br><span class="line">    a = zeros((N, K))</span><br><span class="line">    a[<span class="number">0</span>, :] = g0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, N):</span><br><span class="line">        yp = a[t<span class="number">-1</span>, :]</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(K):</span><br><span class="line">            a[t, y] = logsumexp(yp + g[t<span class="number">-1</span>, :, y])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, g, N, K)</span>:</span></span><br><span class="line">    <span class="string">"""backword stepwise"""</span></span><br><span class="line">    b = zeros((N, K))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(N<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        yp = b[t+<span class="number">1</span>, :]</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(K):</span><br><span class="line">            b[t, y] = logsumexp(yp + g[t, y, :])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure>

<h2 id="序列预测问题"><a href="#序列预测问题" class="headerlink" title="序列预测问题"></a>序列预测问题</h2><p>维特比算法（由代码中可以看出维特比的复杂度是kn方，所以有人告诉你NLG用维特比可以放心大胆的喷他）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vertebi_inference</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="string">"""sequence labeling, inference the most likely label given x"""</span></span><br><span class="line">    <span class="string">"""x is a taggedFeatureTable object"""</span></span><br><span class="line">    N, K = x.seq.__len__(), self.K</span><br><span class="line">    g0, g = self.currentField(x)</span><br><span class="line">    B = ones((N, K), dtype=int32) * <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute max-marginals and backtrace matrix</span></span><br><span class="line">    V = g0</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, N):</span><br><span class="line">        U = empty(K)            <span class="comment"># at time step t, only record the maxlogit pre label</span></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(K):</span><br><span class="line">            w = V + g[t<span class="number">-1</span>,:,y]  <span class="comment"># at time t, the logit of all labels in t-1 transfer to label y</span></span><br><span class="line">            B[t,y] = b = w.argmax()     <span class="comment"># recore which label in time t-1 have the max logit</span></span><br><span class="line">            U[y] = w[b]</span><br><span class="line">        V = U</span><br><span class="line"></span><br><span class="line">    <span class="comment"># extract the best path by brack-tracking</span></span><br><span class="line">    y = V.argmax()</span><br><span class="line">    trace = []</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(N<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        trace.append(y)</span><br><span class="line">        y = B[t, y]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> trace[::<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="参数学习问题"><a href="#参数学习问题" class="headerlink" title="参数学习问题"></a>参数学习问题</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(self, data, iters = <span class="number">20</span>, Nwarm_up = <span class="number">10</span>, saveParam = False, modelpath = <span class="string">'linearCRF'</span>)</span>:</span></span><br><span class="line">        W = self.W  <span class="comment">## reference</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">            print(<span class="string">"Epochs: "</span>, i)</span><br><span class="line">            lr_rate = Nwarm_up/(i+<span class="number">1</span>)**<span class="number">0.501</span></span><br><span class="line">            <span class="keyword">for</span> x, labels <span class="keyword">in</span> tqdm(data):</span><br><span class="line">                ftoken = FeatureTable(x, self.featureNodes, self.labelNodes)</span><br><span class="line">                flabel = self.edgefeatures(ftoken, [self.labelNodes[label] <span class="keyword">for</span> label <span class="keyword">in</span> labels])</span><br><span class="line">                <span class="keyword">for</span> k, explogit <span class="keyword">in</span> self.Exp(ftoken).items():</span><br><span class="line">                    W[k] -= lr_rate * explogit</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> flabel:</span><br><span class="line">                    W[k] += lr_rate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> saveParam:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(modelpath):</span><br><span class="line">                os.mkdir(modelpath)</span><br><span class="line">            joblib.dump(self.W, os.path.join(modelpath, <span class="string">'crf_W.pkl'</span>))</span><br></pre></td></tr></table></figure>

<p>可以参考我的开源代码，在20个epoch之后的f1（多标签分类）达到了91%以上。测试数据占完整数据的百分之十。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Average estimator logit likelyhood is -80.44978854578937.</span><br><span class="line">Average f1 is 0.9166892806881518.</span><br></pre></td></tr></table></figure>

<p>参考我的开源代码：<br><a href="https://link.zhihu.com/?target=https%3A//github.com/fooSynaptic/NLP_utils/blob/master/ML/PGM/CRF/crf_from_scratch/models.py" target="_blank" rel="noopener">crf from scratch</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/11/19/MRC_craft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/19/MRC_craft/" itemprop="url">如何教会机器去理解问题和文本并且回答问题（tensorflow实战）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-19T13:16:27+08:00">
                2019-11-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/19/MRC_craft/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/11/19/MRC_craft/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="这部分主要是为了阐述机器阅读理解的实现"><a href="#这部分主要是为了阐述机器阅读理解的实现" class="headerlink" title="这部分主要是为了阐述机器阅读理解的实现"></a>这部分主要是为了阐述机器阅读理解的实现</h1><p>我们有的输入由这几部分组成：</p>
<ul>
<li>问题q</li>
<li>和问题q相关的文档集合（利用symbolic matching做初步的召回）</li>
<li>标注好的文档中的起始和结束位点</li>
</ul>
<p>现在需要解决的问题有：</p>
<ul>
<li>我们需要找到一个映射来讲文档和问题从他们自己的空间映射到二维的整数空间；那么这个函数的具体形式是什么？（neuron network）</li>
<li>如何在向量空间中表示问题和文档？</li>
<li>什么样的损失函数是一个好的损失？</li>
<li>在实际的答案中，如果答案存在于不连续的区间，如何解决这个问题？</li>
<li>对于部分问题，如果要回答一定需要生成的手段而不是直接从原内容中抽取，要如何解决？</li>
</ul>
<h1 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h1><ul>
<li>embedding layer</li>
<li>matching layer</li>
<li>funsion layer</li>
<li>decoding layer</li>
</ul>
<h1 id="Embedding-and-Encoding-Layers"><a href="#Embedding-and-Encoding-Layers" class="headerlink" title="Embedding and Encoding Layers"></a>Embedding and Encoding Layers</h1><ul>
<li><p>embedding and encoding layers输入一个token的序列，输出一个向量的序列，在下面的演示中，使用一个预训练的embedding matrix和双向的GRU来初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">embed_shape = [vocab_size, vocab_embed_dim]</span><br><span class="line">embed_placeholder = tf.placeholder(tf.float32, embed_shape)</span><br><span class="line">word_embed = tf.get_variable(<span class="string">"word_embeddings"</span>, embed_shape, trainable = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">embed_init_op = word_embed.assign(embed_placeholder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># to load precomputed embedding from numpy array `pre_embed` to the graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(embed_init_op, feed_dict = &#123;embed_placeholder: pre_embed&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>encode问题和文档</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">q_emb = tf.nn.embedding_lookup(word_embed, q)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Question_Encoder"</span>):</span><br><span class="line">	cell_fw = GRUCell(num_units=hidden_size)</span><br><span class="line">	cell_bw = GRUCell(num_units=hidden_size)</span><br><span class="line">	output, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, q_emb, sequence_length = q_len)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># concat the forwaed and backward encoded information</span></span><br><span class="line">	q_encodes = tf.concat(output, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># do the same to get `p_encodes`</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="Match-Layer"><a href="#Match-Layer" class="headerlink" title="Match Layer"></a>Match Layer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">p_mask = tf.sequence_mask(p_len, tf.shape(p)[<span class="number">1</span>], dtype=tf.float32, name=<span class="string">"passage_mask"</span>)</span><br><span class="line">q_mask = tf.sequence_mask(q_len, tf.shape(q)[<span class="number">1</span>], dtype=tf.float32, name=<span class="string">"question_mask"</span>)</span><br><span class="line"></span><br><span class="line">sim_matrix = tf.matmul(p_encodes, q_encodes, transpose_b = <span class="literal">True</span>)</span><br><span class="line">sim_mask = tf.matmul(tf.expand_dims(p_mask, <span class="number">-1</span>), tf.expand_dims(q_mask, <span class="number">-1</span>), transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mask out zeros by replacing it with very small number</span></span><br><span class="line">sim_matrix -= (<span class="number">1</span>-sim_mask)*<span class="number">1E30</span></span><br><span class="line"></span><br><span class="line">passage2question_attn = tf.matmul(tf.nn.softmax(sim_matrix, <span class="number">-1</span>), q_encodes)</span><br><span class="line">b = tf.nn.softmax(tf.expand_dims(tf.reduce_max(sim_matrix, <span class="number">2</span>), <span class="number">1</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">question2passage_attn = tf.tile(tf.matmul(b, p_encodes),[<span class="number">1</span>, tf.shape(p_encodes)[<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">p_mask = tf.expand_dims(p_mask, <span class="number">-1</span>)</span><br><span class="line">passage2question_attn *= p_mask</span><br><span class="line">question2passage_attn *= p_mask</span><br><span class="line"></span><br><span class="line">match_out = tf.concat([p_encodes,</span><br><span class="line">	p_encodes*passage2question_attn,</span><br><span class="line">	p_encodes*question2passage_attn], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Fusing-Layer"><a href="#Fusing-Layer" class="headerlink" title="Fusing Layer"></a>Fusing Layer</h1><p>fusing layer的目的是为了：</p>
<ul>
<li>first:获取到match_out中长程的依赖。</li>
<li>second: 获取到目前为止尽可能多的信息然后准备最好的decoding阶段。</li>
</ul>
<p>采用的方法有：</p>
<ul>
<li>将match_out作为双向RNN的输入，输出就是fusing layer.</li>
<li>CNN,用多个conv1d to cross-correlated with match-out to produce the output of the fusing layer.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use CNN</span></span><br><span class="line">out_dim = <span class="number">64</span></span><br><span class="line">window_len = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">conv_match = tf.layers.conv1d(match_out, out_dim, window_len, strides = window_len)</span><br><span class="line">conv_match_up = tf.squeeze(tf.image.resize_images(tf.expand_dims(conv_match, axis=<span class="number">-1</span>),</span><br><span class="line">	[tf.shape(match_out)[<span class="number">1</span>], out_dim],</span><br><span class="line">	method = ResizeMethod.NEAREST_NEIGHBOR), axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">fuse_out - tf.concat([p_encodes, match_out, conv_match_up], axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>

<p><em>The upsampling step is required for concatenating the convoluted features with match_out and p_encodes. It can be implemented with resize_images from Tensorflow API. The size of fuse_out is [B,L,D], where B is the batch size; L is the passage length and D is the depth controlled by the convolution filters in the fusing layer.</em></p>
<h1 id="Decoding-Layer-amp-Loss-Function"><a href="#Decoding-Layer-amp-Loss-Function" class="headerlink" title="Decoding Layer &amp; Loss Function"></a>Decoding Layer &amp; Loss Function</h1><p>decode <code>fuse_out</code> as an answer span.<br>A simple way to get such distribution is to reduce the last dimension of <code>fuse_out</code> to 1 using a dense layer, and then put a softmax over its output.<br>利用交叉熵损失来评估损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">start_logit = tf.layers.dense(fuse_out, <span class="number">1</span>)</span><br><span class="line">end_logit = tf.layers.dense(fuse_out, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mask out those padded symbols before softmax</span></span><br><span class="line">start_logit -= (<span class="number">1</span>-p_mask)*<span class="number">1E30</span></span><br><span class="line">end_logit -= (<span class="number">1</span>-p_mask)*<span class="number">1E30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the loss</span></span><br><span class="line">start_loss = tf.losses.sparse_softmax_cross_entropy(labels = start_label, logit=start_logit)</span><br><span class="line">end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logit=end_logit)</span><br><span class="line">loss = (start_loss+end_loss)/<span class="number">2</span></span><br></pre></td></tr></table></figure>

<h1 id="generate-final-answer"><a href="#generate-final-answer" class="headerlink" title="generate final answer"></a>generate final answer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">max_answ_len = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">start_prob = tf.nn.softmax(start_logit, axis=<span class="number">1</span>)</span><br><span class="line">end_prob = tf.nn.softmax(end_logit, axis= <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># do the outer product</span></span><br><span class="line">outer = tf.matmul(tf.expand_dims(start_prob, axis=<span class="number">2</span>),</span><br><span class="line">		tf.expand_dims(end_prob, axis=<span class="number">1</span>))</span><br><span class="line">outer = tf.matrix_band_part(outer, <span class="number">0</span>, max_answ_len)</span><br><span class="line"></span><br><span class="line">start_pos = tf.argmax(tf.reduce_max(outer, axis=<span class="number">2</span>), axis=<span class="number">1</span>)</span><br><span class="line">end_pos = tf.argmax(tf.reduce_max(outer, axis=<span class="number">1</span>), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract the answer from the original passages</span></span><br><span class="line">final_answer = passage_tokens[start_pos: end_pos+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ul>
<li><a href="http://hanxiao.io/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/" target="_blank" rel="noopener">http://hanxiao.io/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/10/31/nlp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/31/nlp/" itemprop="url">NLP学习笔记之——读香侬科技李级为《出入NLP领域的一些小建议》文章</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-31T17:31:53+08:00">
                2019-10-31
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/31/nlp/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/10/31/nlp/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="涉及到的书籍和学习材料"><a href="#涉及到的书籍和学习材料" class="headerlink" title="涉及到的书籍和学习材料"></a>涉及到的书籍和学习材料</h3><ul>
<li>Speech and Language Processing</li>
<li>Introduction to infromation retrieval</li>
<li>吴恩达的机器学习</li>
<li>Pattern recognition and Machine learning</li>
</ul>
<h2 id="1-了解NLP的最基本的知识"><a href="#1-了解NLP的最基本的知识" class="headerlink" title="1.了解NLP的最基本的知识"></a>1.了解NLP的最基本的知识</h2><h3 id="Ngram"><a href="#Ngram" class="headerlink" title="Ngram"></a>Ngram</h3><p>ngram模型中蕴含了概率语言模型的假设，同时也是一个n-1阶的马尔科夫假设，也就是一个词出现的概率只和它前面n-1个词相关<br>p(w_k|w1…k-1) = count(w(k-n+1…k)) / count(w(k-n+1…k-1))</p>
<h3 id="Bert-里面训练LM的随机替换能够使得训练结果变得更好地原因"><a href="#Bert-里面训练LM的随机替换能够使得训练结果变得更好地原因" class="headerlink" title="Bert 里面训练LM的随机替换能够使得训练结果变得更好地原因"></a>Bert 里面训练LM的随机替换能够使得训练结果变得更好地原因</h3><p>斯坦福的吴恩达组的ziang Xie的Data Noising as Smoothing in Neurala Network Language Models就首次提出了此方法，并且给了理论解释，这种random的替换其实本质上属于language model里面基于interpolation的平滑方式。</p>
<h2 id="了解早点经典的NLP模型以及论文"><a href="#了解早点经典的NLP模型以及论文" class="headerlink" title="了解早点经典的NLP模型以及论文"></a>了解早点经典的NLP模型以及论文</h2><h3 id="机器翻译中的IBM模型大概是干嘛的"><a href="#机器翻译中的IBM模型大概是干嘛的" class="headerlink" title="机器翻译中的IBM模型大概是干嘛的"></a>机器翻译中的IBM模型大概是干嘛的</h3><p>对于每个词有一个alignment的list，目标函数为P(a|src, tgt)，表示以平行翻译语料作为条件，找到它对齐的概率。<br>参数估计的方法就是EM算法，通过概率模型中寻找最大似然估计或者最大后验估计的算法。概率模型中有无法观测的隐形变量，em算法经过两个步骤交替进行计算，第一步是计算期望，利用对隐藏变量的现有估计值（起始位随机初始化），计算其最大似然估计值；第二步是最大后验概率，最大后验概率通过求得最大似然的目标来估计参数，然后新股寄出来的参数用于下一个计算期望的步骤，不断迭代进行。<br>IBMmodel中的隐变量就是句子中词语的对齐信息a；<br>所以<br>P(a|src, tgt)<br>= P(a, src|tgt) / P(src|tgt)<br>P(src|tgt) 重点<br>= P(src, a|tgt)</p>
<h3 id="神经机器翻译中正向翻译和反向翻译预测的target要一致对齐，这个是通过双向attention的约束项来实现的。"><a href="#神经机器翻译中正向翻译和反向翻译预测的target要一致对齐，这个是通过双向attention的约束项来实现的。" class="headerlink" title="神经机器翻译中正向翻译和反向翻译预测的target要一致对齐，这个是通过双向attention的约束项来实现的。"></a>神经机器翻译中正向翻译和反向翻译预测的target要一致对齐，这个是通过双向attention的约束项来实现的。</h3><h3 id="处理对话系统的无聊回复"><a href="#处理对话系统的无聊回复" class="headerlink" title="处理对话系统的无聊回复"></a>处理对话系统的无聊回复</h3><p>用反向概率做rerank可以进一步提高检索的结果。在早期的统计机器翻译中（phrase-base MT）需要对一个大的N-best list用MERT做reranking，反向概率（given target， the p of source）是reranking中feature的重要标志。</p>
<h3 id="诞生于神经网络机器翻译的attention，其实就是IBM模型的神经网络版本。"><a href="#诞生于神经网络机器翻译的attention，其实就是IBM模型的神经网络版本。" class="headerlink" title="诞生于神经网络机器翻译的attention，其实就是IBM模型的神经网络版本。"></a>诞生于神经网络机器翻译的attention，其实就是IBM模型的神经网络版本。</h3><h2 id="了解机器学习的基本模型"><a href="#了解机器学习的基本模型" class="headerlink" title="了解机器学习的基本模型"></a>了解机器学习的基本模型</h2><h3 id="EM算法是什么"><a href="#EM算法是什么" class="headerlink" title="EM算法是什么"></a>EM算法是什么</h3><p>参数估计的方法就是EM算法，通过概率模型中寻找最大似然估计或者最大后验估计的算法。概率模型中有无法观测的隐形变量，em算法经过两个步骤交替进行计算，第一步是计算期望，利用对隐藏变量的现有估计值（起始位随机初始化），计算其最大似然估计值；第二步是最大后验概率，最大后验概率通过求得最大似然的目标来估计参数，然后新股寄出来的参数用于下一个计算期望的步骤，不断迭代进行。</p>
<h3 id="什么是variational-inference"><a href="#什么是variational-inference" class="headerlink" title="什么是variational inference"></a>什么是variational inference</h3><h3 id="如何理解CRF"><a href="#如何理解CRF" class="headerlink" title="如何理解CRF"></a>如何理解CRF</h3><p>参考我的文章<a href="https://zhuanlan.zhihu.com/p/89553094" target="_blank" rel="noopener">ref</a></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>一种bagging模型集成策略</p>
<h3 id="SGD-momentum-adaboost-adagrad-adam"><a href="#SGD-momentum-adaboost-adagrad-adam" class="headerlink" title="SGD, momentum, adaboost, adagrad, adam"></a>SGD, momentum, adaboost, adagrad, adam</h3><h2 id="多看NLP其他子领域的论文"><a href="#多看NLP其他子领域的论文" class="headerlink" title="多看NLP其他子领域的论文"></a>多看NLP其他子领域的论文</h2><ul>
<li>MT, information extracition, parsing, ragging, sentiment analysis, machine reading comprehension.</li>
</ul>
<h2 id="了解CV和data-mining领域的基本重大进展"><a href="#了解CV和data-mining领域的基本重大进展" class="headerlink" title="了解CV和data mining领域的基本重大进展"></a>了解CV和data mining领域的基本重大进展</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/10/04/当我们把目光放在机器阅读理解，我们的期望到底是什么？/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/当我们把目光放在机器阅读理解，我们的期望到底是什么？/" itemprop="url">当我们把目光放在机器阅读理解，我们的期望到底是什么？</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T07:41:50+08:00">
                2019-10-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/04/当我们把目光放在机器阅读理解，我们的期望到底是什么？/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/10/04/当我们把目光放在机器阅读理解，我们的期望到底是什么？/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>=========<br>转载请注明出处<br>=========<br><strong><em>机器阅读理解不是一个end2end的任务，当你把目光看向了机器阅读理解，你需要非常的谨慎，自己的问题是否定义的足够清晰</em></strong></p>
<p>好了我想我们首先还是要先介绍一下机器阅读理解这个任务，在学术圈这是一个端到端的任务，并且总体可以分成两类：</p>
<ul>
<li>开放领域的机器阅读理解，这个被google用于搜索技术中。</li>
<li>针对于文档库的机器阅读理解，这个传统都是用检索系统，现在由于深度学习非常火热，所以大家都拿阅读理解的考试试题的模拟这部分任务。</li>
</ul>
<p>两者从技术原理上并没有太大的分歧，只是第一种稍难，第二种虽然说开放数据上被各种刷新SOTA（state of the art），但是远达不到公共对于深度学习能够理解文本信息的程度。<br>那么具体来看机器阅读理解经历了一个什么样的发展历程呢？</p>
<pre><code>最著名的早期作品之一是 Lehnert ( 1977 ) 中详细描述的 QUALM。基于脚本和计划框架，Lehnert ( 1977 ) 设计了一个问答的理论，并且专注于语用问题和故事上下文在问答中的重要性，来作为对人类阅读理解的建模 ( Schank and Abelson， 1977 )。这个早期工作为语言理解设置了一个强大的愿景，但是当时构建的实际系统非常小，仅限于手工编码的脚本，并且很难推广到更广泛的领域。

由于问题的复杂性，这方面的研究在20世纪80年代和90年代大多被忽视。在20世纪90年代末，人们对阅读理解的兴趣有了一些小小的复苏，例如 Hirschman 等人 ( 1999 ) 创建了一个阅读理解数据集，以及随后在 ANLP/NAACL 2000年举办了一个关于阅读理解测试作为基于计算机的理解系统评估的研讨会。数据集包括60个用于开发的故事和60个用于测试的三至六年级的故事，附有一些简单的 who，what，when，where，why 这样的简单问题。它只需要系统返回包含正确答案的句子。这一阶段开发的系统主要是基于规则的词包方法。例如 DEEP READ 系统 ( Hirschman et al. 1999 ) 中进行词干分析、语义类识别和代词解析等浅层语言处理，或者像是 QUARC 系统 ( Riloff and THElen，2000 ) 中手动生成基于词汇和语义对应的规则或者是以上两个的组合体 ( Charnizak et al.， 2000 )。这些系统在检索正确句子时达到了30%-40%的准确率。</code></pre><p><a href="https://mp.weixin.qq.com/s/2gUhlgIaL_Qi0M8iPbWMkA" target="_blank" rel="noopener">ref</a><br>早期的工作者看到这个任务，首选的就是系统，这很符合直觉也和本文的观点一致，不管是设计qa问答机器人还是文本检索系统，其本身必定是一个系统，需要一定的特征设计和结合数据的程序设计。</p>
<p>那么现在基于深度学习设计的阅读理解模型是如何做的呢？<br>涉及到深度学习模型的MRC就一定要介绍SQUAD这个数据集了，在早期span detection这个数据f1已经刷到了超过90%，但是大家任然对包括Bert在内的这些机器阅读理解模型不抱太大期望能够做到理解文本，所以现在也已经推出了更新的SQUAD2。。。<br>根据笔者的经验，机器阅读理解这个任务，在深度学习里面经历了这么几个过程<br>denote： q和c分别对应阅读理解中的问题和文本内容</p>
<ul>
<li>Match-LSTM 先把q和c分别经过双向的LSTM，再通过attention机制，得到c中每个词q里所有词的权重，得到新的q的向量</li>
<li>BIDAF 分别计算q到c和c到q的attention 权重，利用这个权重乘以q和c得到q-wise的c和c-wise的q，concat起来做fusion</li>
<li>R-net 利用了self-attention，这边文章在 May. 6, 2017，在transformer之后，很显然self-attention的借鉴打破了之前到处attend的局势</li>
<li>QA-net 利用了CNN，和self-attention一直cnn也能够捕捉到并行的词依赖特征，虽然不如self-attention那么棒，但是QAnet经过了精心设计的让他好transformer的encoder结构很像。</li>
<li>Bert bert开始刷榜</li>
</ul>
<hr>
<p>我们介绍的这些模型都不差，但是存在的问题也很明显，首先阅读理解这个任务分为好几个任务</p>
<ul>
<li>完形填空类型 ( Cloze style )：问题包含一个 placeholder ( 占位符 )。</li>
<li>多项选择类型 ( Multiple choice )：在这个类别下，正确答案从 k 个假设答案中选择 ( 比如：k=4 ) </li>
<li>范围预测类型 ( Span prediction )：这个类别也被称为抽取式问答 ( extractive question answering ) 并且答案必须是文本中的一个范围。因此，答案可以表示为( astart，aend )，其中 1 ≤ astart ≤ aend ≤ lp。并且答案对英语 pastart, . . . , paend。</li>
<li>自由形式回答类型 ( Free-form answer )：最后一种类型允许答案是任何形式的文本 ( 即，任意长度的单词序列 )，形式上：a ∈ V∗。</li>
</ul>
<p>那么现有的模型能够做的相对好的，只有完形填空和范围预测，前者是predict token，后者是CrossEntry(p-end,p-start)，我不知道读者现在脑海里想的是什么，我想到的就是检索，机器阅读理解就是更家复杂的文本检索，这既是当今机器阅读理解的全部，如果你期望他能够做一些推理的任务，比如帮你数多少个人，时间差，在当前是不可能完成的。<br>那么我们能够用检索来完成，为何不用传统的方法BM25？<br>事实上，BM25在这些任务上不会比机器阅读理解差太多，一个中规中矩的深度学习baseline不会比BM25加上更多的文本特征工程更好（仅限于范围预测），但是深度学习带来的优势是，他真的是在帮你做阅读理解，他会抛出一个短语，几个词，而不是像BM25那样给出一句一句话。<br>根据我的个人实验，我使用了self-attention+Match_lstm+BIDAF+pointer_network的结构实现了一个阅读理解模型，比BM25召回的文本序列要短非常多，但是BLEU和Rouge要比BM25高。<a href="https://github.com/fooSynaptic/transfromer_NN_Block/tree/master/transformer_RC" target="_blank" rel="noopener">ref</a></p>
<p>经验：</p>
<ul>
<li>把问题分类，如果问题中详细的问道数字，人名，记录这部分信息。</li>
<li>对文章进行命名实体识别，把能够对应上的实体特征，提高该部分词的权重。</li>
<li>用最长的匹配字符串去找到后面的内容<a href="https://github.com/fooSynaptic/exam/blob/master/Coding/LCS_string.py" target="_blank" rel="noopener">lcs_continue_string</a></li>
<li>数字和特殊字符一定要保留完整信息并且成词</li>
</ul>
<p>如果您仍然觉得机器阅读理解不够棒，不能满足您的需求，请重新定义您的问题，或者不要期望阅读理解为您做更多。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/10/04/因果关系推断介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/因果关系推断介绍/" itemprop="url">因果关系推断介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T07:34:37+08:00">
                2019-10-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/04/因果关系推断介绍/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/10/04/因果关系推断介绍/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>咳咳，真正的科普写起来太费劲又有点枯燥，先给大家带来一些相关工具的文档可以上手，理论部分如果以后还是懒得写的话就去抄之前的论文（过分。<br>这篇文章的内容来自于“  Causal Inference with Graphical Models in R Package pcalg ”，如果有时间的话可以直接跳转阅读。<br>如果你在寻求尝试因果推断的结构学习算法，可以参考PC算法的python版本，<a href="https://github.com/fooSynaptic/py_pcalg" target="_blank" rel="noopener">fooSynaptic/py_pcalg</a>，rep中提供了PC算法的结构学习算法并且支持可视化。</p>
<p>有关贝叶斯网络的内容其实火起来是从图模型，结构方程到因果推断，因此如果很介意因果推断这个说法的话，那么其实可以讲其等价于两个部分：1.变量之间的图结构的学习；2.图结构中方向的推断（大部分人其实不认同因果关系能够从数据中推断出来，然而在因果推断的研究中默认研究的是非时序的观测数据）。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>在很多学科当中，去理解变量之间的cause-effect关系都是一个非常让人感兴趣的话题。一般的，实验科学都会通过在实验中干预特殊的因素来观察这种关系。但是在现实生活中，存在因为时间，费用以及伦理等限制是没有办法做实验来发现这一关系的。<br>所以数据科学家们对于从观测数据中推断出因果的信息这个问题很感兴趣，也产生了非常多有意义的研究结果。通过合理的假设（这个以后补充），类似像PC算法这种可以通过观测的数据来推断出因果的结构。这些结果能够告诉我们-我们感兴趣的一部分变量能不能够成为另外一部分变量的cause（forgive me）。但是他并不像结构方程那样能够告诉我们这个影响的效应有多强（除非部分funcitonal causal model）。因此有了IDA算法的出现，它能够在没有隐变量和选择变量的存在下通过观测数据来推断出因果效应的边界，它综合了PC算法和Judea pear的后门准则。并且IDA算法在大规模的生物系统中得到了证实。然而，让观测系统中不存在隐变量是一个非常强的假设，现实的情况往往没法满足这种情况，因此，作者将后门准则衍化成一些其他类型的图结构来描述满足马尔可夫等价类的DAGs（有向无环图）。<br>R package pcalg整合了这诸多算法，包括了PC，FCI，RFCI，GES和GIES以及IDA。这篇文章也是通过一些模拟数据来应用这些方法的调用。<br>先简单看一个例子来理解因果推断</p>
<p>在图中，左边是真实的因果结构，右边是PC算法计算推断出来的因果结构，他的变现形式是一个马尔可夫等价类的DAG，主要蕴含了条件独立性的信息。<br>如图中所示，在算法推断出来的因果结构中有单向和双向的箭头。单向的箭头代表着直接的有向因果效应。双向的箭头意味着对于PC算法来说无法去判断这个因果效应的方向应该是⬅️还是➡️。因此，在推断的结果当中，双向的箭头代表这因果关系的不确定性。这其中有一个非常重要的事实：普遍的，PC算法类似的算法无法从观测数据中得到一个单一的DAG，即便说这个数据量非常有限，因为存在的事实是多个DAGs可以描述相同的条件独立性信息。<br>然后我们重点介绍一下马尔可夫等价类：为什么是类其实也就是因为多个结构可以描述相同的条件概率（贝叶斯概率），举个例子——</p>
<p>花了15分钟做的图，office online真辣鸡<br>对于： a. A&lt;—— C ——&gt;B 和b. A——&gt; C ——&gt; B是一对马尔可夫等价类，因为对于<br>a:   有- P(C)P(A|C)P(B|C) 。<br>b：有- P(A)P(C|A)P(B|C) = P(C)P(A|C)P(B|C) 。<br>所以a等价于b。<br>马尔科夫等价类在贝叶斯概率的计算上是等价的，却有着完全不同的结构。对于如上的链式图有两种贝叶斯概率。</p>
<h2 id="研究方法背景介绍"><a href="#研究方法背景介绍" class="headerlink" title="研究方法背景介绍"></a>研究方法背景介绍</h2><p>一般的，贝叶斯网络图、模型常用来进行因果结构的推断，图模型可以被理解为从联合概率到有依赖关系的结构的一种映射关系。就好像是地图一样，如果你想要使用地图，那么你需要两个要素，第一，你需要一个物理位置的图包含了点和线的符号，第二，你需要合理的规则来对图上的符号来作出解释。从这个观点考虑就能够理解，虽然高速公路地图和电车轨道地图看起来非常相像，不过他们对于符号的解释规则会存在较大的差别，所以图模型可以被理解为一张地图。图模型中都包含了一张具备了点，线以及潜在的mark，比如说箭头或者环，并且，图模型都有一套解释自己的的规则。一般的，在统计学习中，图中的节点代表着随机变量，而边代表了某种依赖关系。<br>我们先考虑不存在隐变量的情况</p>
<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><p>有向无环图-DAG模型，无环意味着从图中任何一个节点出发沿着边延伸都无法回到起始节点。先考虑一种简单的解释规则- d-separation。<br>Define D-separation：<br>如果在模型图中存在两个节点x和y，他们被一个node集合S所分隔-d-separated，那么相应的在集合S的存在下，两个随机变量Vx和Vy条件性相互独立。<br>denote： Vx \bot Vy  | S .<br>相应的，满足d-separation的变量分布被称之为faithful，并且在统计上也被证明了大部分的变量分布都是faithful的。所以in practice这个假设并不strong。<br>因为DAG模型可以编码条件独立性关系，PC算法利用这一关系来推断我们前面提到的因果关系。PC算法被证明能够重构潜在的DAG模型结构，这个算法依赖于马尔可夫等价类（some variable with some 联合分布）中的条件独立性关系。实际在算法中，条件独立性骨架通过一个条件独立性的统计检验来完成。在某些不存在隐变量的情况下，即便存在非常高纬度的随机变量（意味着可能会有非常稀疏的众多DAG模型）PC算法利用统计检验来进行条件独立性分析是有较好的计算效率。<br>PC算法伪代码<br>输入：一个代表图模型节点的随机变量集合V；条件独立性信息；统计检验显著性水平 \alpha 。<br>输出：部分的完备有向无环图CPDAG \tilde{G} ，分隔变量集合 \tilde{S} ，以及边的方向👈还是👉。<br>构造随机变量集合V的完全的联通图（所有节点相连接）。<br>利用显著性水平\alpha对相邻的统计变量集合进行条件独立性检验，如果存在条件性独立，就将两个变量之间的边去除掉。<br>确定V结构（确定方向）。<br>树立剩余的边。</p>
<p>=========<br>转载请注明出处<br>=========</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/10/04/矩阵分解之一：Truncate-SVD-和random-SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/矩阵分解之一：Truncate-SVD-和random-SVD/" itemprop="url">矩阵分解之一：Truncate SVD 和random SVD</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T07:32:18+08:00">
                2019-10-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/04/矩阵分解之一：Truncate-SVD-和random-SVD/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/10/04/矩阵分解之一：Truncate-SVD-和random-SVD/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近看了一点矩阵分解的论文发现了一些好玩的东西，所以会准备总结几篇矩阵分解的文章</p>
<h2 id="奇异值分解相对于矩阵分解的问题"><a href="#奇异值分解相对于矩阵分解的问题" class="headerlink" title="奇异值分解相对于矩阵分解的问题"></a>奇异值分解相对于矩阵分解的问题</h2><p>每个学过线性代数的人都不会对奇异值分解感到陌生，因为SVD广泛地应用于统计学，信号处理以及机器学习当中。形式上来看，一个维度为 m × n的实数矩阵的奇异值分列可以表示为A = U Σ Vᵀ；其中U代表的是一个维度为m × m的正交奇异向量矩阵，∑代表着奇异值的对角矩阵，最后一项是一个n × n的正交奇异向量矩阵。<br>在矩阵分解这个问题上，奇异值分解提供的策略是计算一个相对于A更低秩的近似矩阵Aᵣ，意味着<code>r&lt;m, n</code>， 并且要使得<code>||Aᵣ – A||</code>最小。那么对于A = U Σ Vᵀtruncate SVD的策略是：<br>对于对角矩阵上面的奇异值进行降序排序；<br>在对角矩阵∑上面取前r个奇异值，相对应的在左右两边的奇异向量矩阵上面也取相对于的r列，最后分别得到了Σᵣ，Uᵣ， Vᵣ。<br>将Aᵣ = Uᵣ Σᵣ Vᵣᵀ作为最终矩阵分解的产物。<br>上述过程可以用如下的示意图来表示：</p>
<p>这样，通过SVD就可以成功的完成矩阵降维的过程。我们从矩阵分解来到了很相近的另外一个主题——降维，作为降维届的代表技术——PCA，而矩阵的奇异值分解能够直接得到矩阵通过PCA的投影空间。对于一个协变量矩阵X为 m × p（m为观测）的观测特征矩阵，计算一个p × l 的矩阵W和一个 l × l 的对角矩阵Λ，能够使得 Xᵀ X ≈ W Λ Wᵀ。这样地近似可以将原先的观测矩阵投影到一个维度为l的空间从而达到降维。有一种精简的PCA算法计算的形式是通过直接近似一个低秩的协变量矩阵X ≈ Uᵣ Σᵣ Vᵣᵀ，然后乘上转置，最终因为左奇异向量矩阵正交而直接得到投影的向量空间。<br> Xᵀ X ≈ (Uᵣ Σᵣ Vᵣᵀ)ᵀ (Uᵣ Σᵣ Vᵣᵀ) = Vᵣ Σᵣ² Vᵣᵀ<br>如上最终的对角矩阵就是新的投影空间。<br>但是truncate SVD有一些缺点（通过上述和PCA的论述其实可以得到这个缺点是传统降维技术共有的）：<br>实际的工业环境中，矩阵的维度是巨大的，并且数据往往是缺失，不准确的；当不准确的输入限制了输出的精确性时，仅仅依靠这些实际的数据只会白白浪费计算资源。<br>传统的降维技术是不支持并行计算的，如果你熟悉工业数据你就应该知道这一点有多么可怕。</p>
<p>到这里我们就要进入这篇文章的转折点引出我们的主角了——</p>
<h2 id="Randomized-SVD"><a href="#Randomized-SVD" class="headerlink" title="Randomized SVD"></a>Randomized SVD</h2><p>它相对于truncate SVD的优势有这么几点：<br>很稳定。<br>它的性能并不依赖于局部的特征。<br>大量的矩阵乘法过程，可以利用GPU并行计算，所以它比truncate SVD更快。<br>相比于直接从理论上阐述randomized SVD，我想更直接一点，直接参考它的实现过程能够让我们更快地理解。<br>首先我们需要定义一个方法来找到一个正交矩阵，这个矩阵的范围近似于观测矩阵的范围（这个和我们上面的思路很像，一个更小的矩阵），这里我们会用到一些传统的LU和QR分解。<br><strong><em>在这里的LU和QR分解起的是规范子的作用，QR相对LU更慢但是更准确，所以QR规范放在最后一层。</em></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomized_range_finder</span><span class="params">(A, size, n_iter=<span class="number">5</span>)</span>:</span></span><br><span class="line">    Q = np.random.normal(size=(A.shape[<span class="number">1</span>], size))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iter):</span><br><span class="line">        Q, _ = linalg.lu(A @ Q, permute_l=<span class="literal">True</span>)</span><br><span class="line">        Q, _ = linalg.lu(A.T @ Q, permute_l=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    Q, _ = linalg.qr(A @ Q, mode=<span class="string">'economic'</span>)</span><br><span class="line">    <span class="keyword">return</span> Q</span><br></pre></td></tr></table></figure>

<p>现在我们能够得到了观测矩阵范围的近似Q，我们利用Q来得到最终的近似结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomized_svd</span><span class="params">(M, n_components, n_oversamples=<span class="number">10</span>, n_iter=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="comment">#这里n_random就是truncate SVD中的r</span></span><br><span class="line">    n_random = n_components + n_oversamples</span><br><span class="line">    </span><br><span class="line">    Q = randomized_range_finder(M, n_random, n_iter)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#把原始观测投影到(k + p)维度空间</span></span><br><span class="line">    B = Q.T @ M</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对B进行奇异值分解</span></span><br><span class="line">    Uhat, s, V = linalg.svd(B, full_matrices=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">del</span> B</span><br><span class="line">    U = Q @ Uhat</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> U[:, :n_components], s[:n_components], V[:n_components, :]</span><br></pre></td></tr></table></figure>

<p>没有看懂？好吧让我们来讲讲这个原理，randomize SVD对于矩阵分解能够作为一种通用的算法，简单讲分成两步，<br>第一步是求原始矩阵范围的近似Q，这个过程中通过对一个随机初始化的小维度矩阵Q不断地进行和原矩阵相乘然后分解，最终得到一个稳定的向量矩阵，为什么不取对角矩阵，因为对角矩阵是特征基矩阵，而左边的向量矩阵可以作为量纲矩阵。最终我们的目标是得到A ≈ Q Qᵀ A。<br>第二步很简单，有了A ≈ Q Qᵀ A，我们构造一个矩阵B = Qᵀ A，因为Q是低秩的，所以矩阵B很小。我们可以用传统SVD的方法来对矩阵B进行分解B = S Σ Vᵀ，得到左奇异，奇异值矩阵，右奇异向量矩阵。到这里A ≈ Q Qᵀ A = Q (S Σ Vᵀ)，最右边所有量都已知，成功地对A求到了一个低秩的近似U Σ Vᵀ。<br>（通过上面的代码，可以很直接得理解这两步原理。）</p>
<h2 id="Tricks-and-intuition"><a href="#Tricks-and-intuition" class="headerlink" title="Tricks and intuition"></a>Tricks and intuition</h2><p>randomized SVD的trick就是能够非常高效得求得范围近似矩阵Q，从直觉上来思考，为了估计原始矩阵的范围，我们可以用一些随机的向量，通过原始矩阵A和这些随机向量的相乘所作出的变动的近似来得到A的波动范围。<br>Specific：假设我们使用一个高斯向量矩阵M来和原矩阵相乘，计算Y = A M，然后对Y进行QR分解Q R = Y，这样得到的矩阵Q，它的没一列就是Y的范围的正交基，所以可以作为A的范围近似。</p>
<p>=========<br>转载请注明出处<br>=========</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/10/04/开篇/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/开篇/" itemprop="url">开篇</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T07:18:17+08:00">
                2019-10-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/04/开篇/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/10/04/开篇/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>各位同学们大家好，我是fooSynaptic，本来买好了机器打算做一个个人网站，但是没有想到的是hexo居然这么好用，而且markdown对于我来说用的还算比较熟，总体体验还算是可以的。</p>
<h1 id="关于这个Blog"><a href="#关于这个Blog" class="headerlink" title="关于这个Blog"></a>关于这个Blog</h1><p>这个博客主要会用来记录包括但不限于如下的一些内容：</p>
<ul>
<li>NLP技术</li>
<li>机器学习算法的实现细节</li>
<li>深度学习框架建模细节</li>
<li>Mathmatic</li>
<li>程序算法设计</li>
<li>Python</li>
<li>web应用开发</li>
</ul>
<h1 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h1><p>NLP researcher<br>如果你想了解更多关于我的项目，请参考我的github项目：<a href="https://github.com/fooSynaptic" target="_blank" rel="noopener">ref link</a></p>
<p>=========<br>转载请注明出处<br>=========</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fooSynaptic.com/2019/10/04/article/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jiaxin hu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/article/" itemprop="url">article</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T06:42:41+08:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/demo/" itemprop="url" rel="index">
                    <span itemprop="name">demo</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/demo/Let-s-begin/" itemprop="url" rel="index">
                    <span itemprop="name">Let's begin</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/demo/Let-s-begin/MRC/" itemprop="url" rel="index">
                    <span itemprop="name">MRC</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/demo/Let-s-begin/MRC/tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">tensorflow</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/demo/Let-s-begin/MRC/tensorflow/Matrix-factorization/" itemprop="url" rel="index">
                    <span itemprop="name">Matrix factorization</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/04/article/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/10/04/article/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">jiaxin hu</p>
              <p class="site-description motion-element" itemprop="description">Head first to the Truth as Synaptic.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/fooSynaptic" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/jiu-san-gong-ren/activities" target="_blank" title="ZhiHu">
                      
                        <i class="fa fa-fw fa-globe"></i>ZhiHu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jiaxin hu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"your-duoshuo-shortname"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  
















  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'm3IyaPmV3FOoJuQ9ic9ToH5x-gzGzoHsz',
        appKey: 'Gu82zvrKj6u6gHD5UN0iL222',
        placeholder: 'Welecome to share your idea!',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  

  

  

</body>
</html>
