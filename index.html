<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/jojo_median.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/jojo_small.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"foosynaptic.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="Head first to the Truth as Synaptic.">
<meta property="og:type" content="website">
<meta property="og:title" content="fooSynaptic">
<meta property="og:url" content="https://foosynaptic.github.io/index.html">
<meta property="og:site_name" content="fooSynaptic">
<meta property="og:description" content="Head first to the Truth as Synaptic.">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="fooSynaptic">
<meta property="article:tag" content="Hello, Jiaxin">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://foosynaptic.github.io/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>fooSynaptic</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>







  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">fooSynaptic</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Any problem, please Contact me.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="fooSynaptic"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">fooSynaptic</p>
  <div class="site-description" itemprop="description">Head first to the Truth as Synaptic.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fooSynaptic" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fooSynaptic" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/11/22/Nuural-Approaches-to-Machine-Reading-Comprehension-and-Dialogue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/11/22/Nuural-Approaches-to-Machine-Reading-Comprehension-and-Dialogue/" class="post-title-link" itemprop="url">神经网络机器阅读理解：从 Attention 到 LLM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-11-22 05:31:56" itemprop="dateCreated datePublished" datetime="2019-11-22T05:31:56+08:00">2019-11-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本文综述神经网络在机器阅读理解和对话系统中的发展历程，从早期的注意力机制到现代大语言模型。</p>
<h2 id="发展时间线"><a href="#发展时间线" class="headerlink" title="发展时间线"></a>发展时间线</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2015-2016: 注意力机制兴起</span><br><span class="line">    └── Attentive Reader, Impatient Reader, BiDAF</span><br><span class="line"></span><br><span class="line">2017-2018: 深度交互与预训练</span><br><span class="line">    └── R-Net, QANet, BERT</span><br><span class="line"></span><br><span class="line">2019-2020: 大规模预训练</span><br><span class="line">    └── RoBERTa, ALBERT, T5</span><br><span class="line"></span><br><span class="line">2021-2023: 大语言模型时代</span><br><span class="line">    └── GPT-3, ChatGPT, GPT-4, LLaMA</span><br><span class="line"></span><br><span class="line">2024-: 检索增强与多模态</span><br><span class="line">    └── RAG, Vision-Language Models</span><br></pre></td></tr></table></figure>

<h2 id="核心技术演进"><a href="#核心技术演进" class="headerlink" title="核心技术演进"></a>核心技术演进</h2><h3 id="阶段一：注意力机制-2015-2017"><a href="#阶段一：注意力机制-2015-2017" class="headerlink" title="阶段一：注意力机制 (2015-2017)"></a>阶段一：注意力机制 (2015-2017)</h3><p><strong>问题</strong>：如何让模型”关注”与问题相关的上下文？</p>
<p>$$<br>\alpha_i &#x3D; \text{softmax}(s(h_i, q))<br>$$</p>
<p>$$<br>c &#x3D; \sum_i \alpha_i h_i<br>$$</p>
<p><strong>代表模型</strong>：Attentive Reader, BiDAF</p>
<h3 id="阶段二：深度交互-2017-2018"><a href="#阶段二：深度交互-2017-2018" class="headerlink" title="阶段二：深度交互 (2017-2018)"></a>阶段二：深度交互 (2017-2018)</h3><p><strong>问题</strong>：如何建模问题和上下文的复杂交互？</p>
<p><strong>技术</strong>：多轮注意力、自注意力、门控机制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多轮推理 (R-Net 风格)</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">    <span class="comment"># 自注意力</span></span><br><span class="line">    context = self_attention(context, context)</span><br><span class="line">    <span class="comment"># 交叉注意力</span></span><br><span class="line">    context = cross_attention(context, question)</span><br></pre></td></tr></table></figure>

<h3 id="阶段三：预训练语言模型-2018-2020"><a href="#阶段三：预训练语言模型-2018-2020" class="headerlink" title="阶段三：预训练语言模型 (2018-2020)"></a>阶段三：预训练语言模型 (2018-2020)</h3><p><strong>范式转变</strong>：从 task-specific 到 pretrain-finetune</p>
<p>$$<br>\theta^* &#x3D; \arg\min_\theta \mathcal{L}<em>{task}(\text{PLM}</em>\theta(x), y)<br>$$</p>
<p><strong>代表模型</strong>：BERT, RoBERTa, ALBERT</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForQuestionAnswering</span><br><span class="line"></span><br><span class="line">model = AutoModelForQuestionAnswering.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="comment"># Fine-tune on SQuAD</span></span><br></pre></td></tr></table></figure>

<h3 id="阶段四：大语言模型-2020-至今"><a href="#阶段四：大语言模型-2020-至今" class="headerlink" title="阶段四：大语言模型 (2020-至今)"></a>阶段四：大语言模型 (2020-至今)</h3><p><strong>范式转变</strong>：从 fine-tuning 到 prompting</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Few-shot prompting</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Context: The Eiffel Tower was built in 1889.</span></span><br><span class="line"><span class="string">Question: When was the Eiffel Tower built?</span></span><br><span class="line"><span class="string">Answer: 1889</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Context: &#123;context&#125;</span></span><br><span class="line"><span class="string">Question: &#123;question&#125;</span></span><br><span class="line"><span class="string">Answer:&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="架构对比"><a href="#架构对比" class="headerlink" title="架构对比"></a>架构对比</h2><table>
<thead>
<tr>
<th>模型</th>
<th>参数量</th>
<th>训练范式</th>
<th>SQuAD 2.0 F1</th>
</tr>
</thead>
<tbody><tr>
<td>BiDAF</td>
<td>~2M</td>
<td>从零训练</td>
<td>77.3</td>
</tr>
<tr>
<td>BERT-base</td>
<td>110M</td>
<td>预训练+微调</td>
<td>88.5</td>
</tr>
<tr>
<td>BERT-large</td>
<td>340M</td>
<td>预训练+微调</td>
<td>90.9</td>
</tr>
<tr>
<td>RoBERTa-large</td>
<td>355M</td>
<td>预训练+微调</td>
<td>91.4</td>
</tr>
<tr>
<td>GPT-3</td>
<td>175B</td>
<td>Few-shot</td>
<td>~88</td>
</tr>
<tr>
<td>GPT-4</td>
<td>~1.8T</td>
<td>Zero-shot</td>
<td>~95</td>
</tr>
</tbody></table>
<h2 id="现代-MRC-系统设计"><a href="#现代-MRC-系统设计" class="headerlink" title="现代 MRC 系统设计"></a>现代 MRC 系统设计</h2><h3 id="RAG-架构"><a href="#RAG-架构" class="headerlink" title="RAG 架构"></a>RAG 架构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ModernMRC</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, retriever, reader</span>):</span><br><span class="line">        <span class="variable language_">self</span>.retriever = retriever  <span class="comment"># Dense retriever</span></span><br><span class="line">        <span class="variable language_">self</span>.reader = reader        <span class="comment"># LLM</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">answer</span>(<span class="params">self, question: <span class="built_in">str</span>, knowledge_base: <span class="built_in">str</span> = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 1. 检索</span></span><br><span class="line">        <span class="keyword">if</span> knowledge_base:</span><br><span class="line">            docs = <span class="variable language_">self</span>.retriever.retrieve(question, knowledge_base)</span><br><span class="line">            context = <span class="string">&quot;\n\n&quot;</span>.join([d.text <span class="keyword">for</span> d <span class="keyword">in</span> docs])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            context = <span class="string">&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 阅读理解/生成</span></span><br><span class="line">        prompt = <span class="variable language_">self</span>._build_prompt(question, context)</span><br><span class="line">        answer = <span class="variable language_">self</span>.reader.generate(prompt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 后处理（可选：验证、引用）</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._postprocess(answer, docs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_prompt</span>(<span class="params">self, question, context</span>):</span><br><span class="line">        <span class="keyword">if</span> context:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;Based on the following context, answer the question.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Context:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;context&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: <span class="subst">&#123;question&#125;</span></span></span><br><span class="line"><span class="string">Answer:&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">f&quot;Question: <span class="subst">&#123;question&#125;</span>\nAnswer:&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="多跳推理"><a href="#多跳推理" class="headerlink" title="多跳推理"></a>多跳推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHopReasoner</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, retriever, llm, max_hops=<span class="number">3</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.retriever = retriever</span><br><span class="line">        <span class="variable language_">self</span>.llm = llm</span><br><span class="line">        <span class="variable language_">self</span>.max_hops = max_hops</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reason</span>(<span class="params">self, question</span>):</span><br><span class="line">        reasoning_chain = []</span><br><span class="line">        current_query = question</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> hop <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.max_hops):</span><br><span class="line">            <span class="comment"># 检索</span></span><br><span class="line">            docs = <span class="variable language_">self</span>.retriever.retrieve(current_query)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 生成中间推理</span></span><br><span class="line">            intermediate = <span class="variable language_">self</span>.llm.generate(</span><br><span class="line">                <span class="string">f&quot;Based on: <span class="subst">&#123;docs&#125;</span>\nQuestion: <span class="subst">&#123;current_query&#125;</span>\n&quot;</span></span><br><span class="line">                <span class="string">f&quot;Provide intermediate reasoning or the final answer:&quot;</span></span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line">            reasoning_chain.append(&#123;</span><br><span class="line">                <span class="string">&#x27;query&#x27;</span>: current_query,</span><br><span class="line">                <span class="string">&#x27;docs&#x27;</span>: docs,</span><br><span class="line">                <span class="string">&#x27;reasoning&#x27;</span>: intermediate</span><br><span class="line">            &#125;)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 检查是否已得到答案</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>._is_final_answer(intermediate):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 生成下一跳查询</span></span><br><span class="line">            current_query = <span class="variable language_">self</span>._generate_next_query(question, reasoning_chain)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._synthesize_answer(question, reasoning_chain)</span><br></pre></td></tr></table></figure>

<h2 id="对话系统中的-MRC"><a href="#对话系统中的-MRC" class="headerlink" title="对话系统中的 MRC"></a>对话系统中的 MRC</h2><h3 id="对话式问答"><a href="#对话式问答" class="headerlink" title="对话式问答"></a>对话式问答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConversationalQA</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mrc_model, history_length=<span class="number">5</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.mrc_model = mrc_model</span><br><span class="line">        <span class="variable language_">self</span>.history = []</span><br><span class="line">        <span class="variable language_">self</span>.history_length = history_length</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ask</span>(<span class="params">self, question, context=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 将对话历史纳入问题</span></span><br><span class="line">        contextualized_question = <span class="variable language_">self</span>._contextualize(question)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取答案</span></span><br><span class="line">        answer = <span class="variable language_">self</span>.mrc_model.answer(contextualized_question, context)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新历史</span></span><br><span class="line">        <span class="variable language_">self</span>.history.append(&#123;<span class="string">&#x27;q&#x27;</span>: question, <span class="string">&#x27;a&#x27;</span>: answer&#125;)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.history) &gt; <span class="variable language_">self</span>.history_length:</span><br><span class="line">            <span class="variable language_">self</span>.history.pop(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> answer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_contextualize</span>(<span class="params">self, question</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.history:</span><br><span class="line">            <span class="keyword">return</span> question</span><br><span class="line">        </span><br><span class="line">        history_text = <span class="string">&quot;\n&quot;</span>.join([</span><br><span class="line">            <span class="string">f&quot;Q: <span class="subst">&#123;turn[<span class="string">&#x27;q&#x27;</span>]&#125;</span>\nA: <span class="subst">&#123;turn[<span class="string">&#x27;a&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">            <span class="keyword">for</span> turn <span class="keyword">in</span> <span class="variable language_">self</span>.history</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;Conversation history:\n<span class="subst">&#123;history_text&#125;</span>\n\nCurrent question: <span class="subst">&#123;question&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="评估体系"><a href="#评估体系" class="headerlink" title="评估体系"></a>评估体系</h2><h3 id="传统指标"><a href="#传统指标" class="headerlink" title="传统指标"></a>传统指标</h3><table>
<thead>
<tr>
<th>指标</th>
<th>定义</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>EM</td>
<td>精确匹配</td>
<td>抽取式 QA</td>
</tr>
<tr>
<td>F1</td>
<td>Token 重叠</td>
<td>抽取式 QA</td>
</tr>
<tr>
<td>BLEU</td>
<td>N-gram 重叠</td>
<td>生成式 QA</td>
</tr>
<tr>
<td>ROUGE</td>
<td>召回导向重叠</td>
<td>摘要、长答案</td>
</tr>
</tbody></table>
<h3 id="LLM-时代指标"><a href="#LLM-时代指标" class="headerlink" title="LLM 时代指标"></a>LLM 时代指标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LLM-as-Judge</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">llm_evaluate</span>(<span class="params">question, reference, prediction</span>):</span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;Evaluate the answer quality on a scale of 1-5:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: <span class="subst">&#123;question&#125;</span></span></span><br><span class="line"><span class="string">Reference Answer: <span class="subst">&#123;reference&#125;</span></span></span><br><span class="line"><span class="string">Model Answer: <span class="subst">&#123;prediction&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Criteria:</span></span><br><span class="line"><span class="string">- Correctness: Is the information accurate?</span></span><br><span class="line"><span class="string">- Completeness: Does it fully answer the question?</span></span><br><span class="line"><span class="string">- Conciseness: Is it appropriately brief?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Score (1-5):&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> llm.generate(prompt)</span><br></pre></td></tr></table></figure>

<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.00051">Reading Wikipedia to Answer Open-Domain Questions</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11401">RAG Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.09600">HotpotQA: Multi-hop Reasoning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%AE%9E%E8%B7%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%AE%9E%E8%B7%B5/" class="post-title-link" itemprop="url">机器阅读理解实战：从零构建问答系统</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-11-19 09:03:04" itemprop="dateCreated datePublished" datetime="2019-11-19T09:03:04+08:00">2019-11-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本文从零开始实现一个机器阅读理解系统，涵盖数据处理、模型构建、训练和推理的完整流程。</p>
<h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>给定上下文 $C$ 和问题 $Q$，预测答案 $A$ 在 $C$ 中的位置：</p>
<p>$$<br>(start, end) &#x3D; \arg\max_{i,j} P(start&#x3D;i, end&#x3D;j | C, Q)<br>$$</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="SQuAD-数据格式"><a href="#SQuAD-数据格式" class="headerlink" title="SQuAD 数据格式"></a>SQuAD 数据格式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Example</span>:</span><br><span class="line">    context: <span class="built_in">str</span></span><br><span class="line">    question: <span class="built_in">str</span></span><br><span class="line">    answer_text: <span class="built_in">str</span></span><br><span class="line">    start_position: <span class="built_in">int</span></span><br><span class="line">    end_position: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_squad</span>(<span class="params">file_path: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[Example]:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = json.load(f)</span><br><span class="line">    </span><br><span class="line">    examples = []</span><br><span class="line">    <span class="keyword">for</span> article <span class="keyword">in</span> data[<span class="string">&#x27;data&#x27;</span>]:</span><br><span class="line">        <span class="keyword">for</span> paragraph <span class="keyword">in</span> article[<span class="string">&#x27;paragraphs&#x27;</span>]:</span><br><span class="line">            context = paragraph[<span class="string">&#x27;context&#x27;</span>]</span><br><span class="line">            <span class="keyword">for</span> qa <span class="keyword">in</span> paragraph[<span class="string">&#x27;qas&#x27;</span>]:</span><br><span class="line">                question = qa[<span class="string">&#x27;question&#x27;</span>]</span><br><span class="line">                <span class="keyword">if</span> qa.get(<span class="string">&#x27;is_impossible&#x27;</span>, <span class="literal">False</span>):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                answer = qa[<span class="string">&#x27;answers&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">                examples.append(Example(</span><br><span class="line">                    context=context,</span><br><span class="line">                    question=question,</span><br><span class="line">                    answer_text=answer[<span class="string">&#x27;text&#x27;</span>],</span><br><span class="line">                    start_position=answer[<span class="string">&#x27;answer_start&#x27;</span>],</span><br><span class="line">                    end_position=answer[<span class="string">&#x27;answer_start&#x27;</span>] + <span class="built_in">len</span>(answer[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">                ))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>

<h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MRCTokenizer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name: <span class="built_in">str</span>, max_length: <span class="built_in">int</span> = <span class="number">384</span>, doc_stride: <span class="built_in">int</span> = <span class="number">128</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line">        <span class="variable language_">self</span>.doc_stride = doc_stride</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, example: Example</span>):</span><br><span class="line">        <span class="comment"># Tokenize question and context</span></span><br><span class="line">        encoding = <span class="variable language_">self</span>.tokenizer(</span><br><span class="line">            example.question,</span><br><span class="line">            example.context,</span><br><span class="line">            max_length=<span class="variable language_">self</span>.max_length,</span><br><span class="line">            truncation=<span class="string">&#x27;only_second&#x27;</span>,</span><br><span class="line">            stride=<span class="variable language_">self</span>.doc_stride,</span><br><span class="line">            return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">            return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">            padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 找到答案在 token 序列中的位置</span></span><br><span class="line">        offset_mapping = encoding[<span class="string">&#x27;offset_mapping&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        start_token = <span class="literal">None</span></span><br><span class="line">        end_token = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> idx, (start, end) <span class="keyword">in</span> <span class="built_in">enumerate</span>(offset_mapping):</span><br><span class="line">            <span class="keyword">if</span> start &lt;= example.start_position &lt; end:</span><br><span class="line">                start_token = idx</span><br><span class="line">            <span class="keyword">if</span> start &lt; example.end_position &lt;= end:</span><br><span class="line">                end_token = idx</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>: encoding[<span class="string">&#x27;input_ids&#x27;</span>][<span class="number">0</span>],</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>: encoding[<span class="string">&#x27;attention_mask&#x27;</span>][<span class="number">0</span>],</span><br><span class="line">            <span class="string">&#x27;start_position&#x27;</span>: start_token <span class="keyword">or</span> <span class="number">0</span>,</span><br><span class="line">            <span class="string">&#x27;end_position&#x27;</span>: end_token <span class="keyword">or</span> <span class="number">0</span>,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<h2 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h2><h3 id="基于-BERT-的-MRC-模型"><a href="#基于-BERT-的-MRC-模型" class="headerlink" title="基于 BERT 的 MRC 模型"></a>基于 BERT 的 MRC 模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MRCModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name: <span class="built_in">str</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.bert = AutoModel.from_pretrained(model_name)</span><br><span class="line">        hidden_size = <span class="variable language_">self</span>.bert.config.hidden_size</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.start_classifier = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.end_classifier = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_ids: torch.Tensor,</span></span><br><span class="line"><span class="params">        attention_mask: torch.Tensor,</span></span><br><span class="line"><span class="params">        start_positions: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        end_positions: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        outputs = <span class="variable language_">self</span>.bert(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        sequence_output = <span class="variable language_">self</span>.dropout(outputs.last_hidden_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (batch, seq_len, 1) -&gt; (batch, seq_len)</span></span><br><span class="line">        start_logits = <span class="variable language_">self</span>.start_classifier(sequence_output).squeeze(-<span class="number">1</span>)</span><br><span class="line">        end_logits = <span class="variable language_">self</span>.end_classifier(sequence_output).squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Mask padding tokens</span></span><br><span class="line">        start_logits = start_logits.masked_fill(~attention_mask.<span class="built_in">bool</span>(), -<span class="number">1e9</span>)</span><br><span class="line">        end_logits = end_logits.masked_fill(~attention_mask.<span class="built_in">bool</span>(), -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> start_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> end_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">            start_loss = loss_fct(start_logits, start_positions)</span><br><span class="line">            end_loss = loss_fct(end_logits, end_positions)</span><br><span class="line">            loss = (start_loss + end_loss) / <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span>: loss,</span><br><span class="line">            <span class="string">&#x27;start_logits&#x27;</span>: start_logits,</span><br><span class="line">            <span class="string">&#x27;end_logits&#x27;</span>: end_logits,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<h3 id="改进：联合-Start-End-预测"><a href="#改进：联合-Start-End-预测" class="headerlink" title="改进：联合 Start-End 预测"></a>改进：联合 Start-End 预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">JointMRCModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;联合预测 start 和 end，考虑 start-end 依赖&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name: <span class="built_in">str</span>, max_answer_length: <span class="built_in">int</span> = <span class="number">30</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.bert = AutoModel.from_pretrained(model_name)</span><br><span class="line">        hidden_size = <span class="variable language_">self</span>.bert.config.hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.max_answer_length = max_answer_length</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.start_classifier = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.end_classifier = nn.Linear(hidden_size * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, start_positions=<span class="literal">None</span>, end_positions=<span class="literal">None</span></span>):</span><br><span class="line">        outputs = <span class="variable language_">self</span>.bert(input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        H = outputs.last_hidden_state  <span class="comment"># (batch, seq_len, hidden)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Start prediction</span></span><br><span class="line">        start_logits = <span class="variable language_">self</span>.start_classifier(H).squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> start_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 训练时使用真实的 start 位置</span></span><br><span class="line">            start_indices = start_positions.unsqueeze(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">            start_repr = H.gather(<span class="number">1</span>, start_indices.expand(-<span class="number">1</span>, -<span class="number">1</span>, H.size(-<span class="number">1</span>))).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 推理时使用预测的 start 位置</span></span><br><span class="line">            start_indices = start_logits.argmax(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">            start_repr = H.gather(<span class="number">1</span>, start_indices.expand(-<span class="number">1</span>, -<span class="number">1</span>, H.size(-<span class="number">1</span>))).squeeze(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># End prediction conditioned on start</span></span><br><span class="line">        start_repr_expanded = start_repr.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, H.size(<span class="number">1</span>), -<span class="number">1</span>)</span><br><span class="line">        end_input = torch.cat([H, start_repr_expanded], dim=-<span class="number">1</span>)</span><br><span class="line">        end_logits = <span class="variable language_">self</span>.end_classifier(end_input).squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 只允许 end &gt;= start 且在 max_answer_length 范围内</span></span><br><span class="line">        <span class="comment"># 这里简化处理，完整实现需要更复杂的 mask</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;start_logits&#x27;</span>: start_logits, <span class="string">&#x27;end_logits&#x27;</span>: end_logits&#125;</span><br></pre></td></tr></table></figure>

<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_dataloader, val_dataloader, epochs=<span class="number">3</span>, lr=<span class="number">3e-5</span></span>):</span><br><span class="line">    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=<span class="number">0.01</span>)</span><br><span class="line">    </span><br><span class="line">    total_steps = <span class="built_in">len</span>(train_dataloader) * epochs</span><br><span class="line">    scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">        optimizer, </span><br><span class="line">        num_warmup_steps=<span class="built_in">int</span>(<span class="number">0.1</span> * total_steps),</span><br><span class="line">        num_training_steps=total_steps</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    model.to(device)</span><br><span class="line">    </span><br><span class="line">    best_f1 = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(train_dataloader, desc=<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>&#x27;</span>):</span><br><span class="line">            batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">            </span><br><span class="line">            outputs = model(**batch)</span><br><span class="line">            loss = outputs[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">            </span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">            </span><br><span class="line">            optimizer.step()</span><br><span class="line">            scheduler.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            </span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        avg_loss = total_loss / <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Validation</span></span><br><span class="line">        f1 = evaluate(model, val_dataloader, device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Validation F1: <span class="subst">&#123;f1:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> f1 &gt; best_f1:</span><br><span class="line">            best_f1 = f1</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">&#x27;best_model.pt&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h2 id="评估与推理"><a href="#评估与推理" class="headerlink" title="评估与推理"></a>评估与推理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_answer</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;标准化答案用于评估&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">remove_articles</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> re.sub(<span class="string">r&#x27;\b(a|an|the)\b&#x27;</span>, <span class="string">&#x27; &#x27;</span>, text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">white_space_fix</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(text.split())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">remove_punc</span>(<span class="params">text</span>):</span><br><span class="line">        exclude = <span class="built_in">set</span>(string.punctuation)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(ch <span class="keyword">for</span> ch <span class="keyword">in</span> text <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> exclude)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lower</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> text.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> white_space_fix(remove_articles(remove_punc(lower(s))))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_f1</span>(<span class="params">pred: <span class="built_in">str</span>, gold: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    pred_tokens = normalize_answer(pred).split()</span><br><span class="line">    gold_tokens = normalize_answer(gold).split()</span><br><span class="line">    </span><br><span class="line">    common = Counter(pred_tokens) &amp; Counter(gold_tokens)</span><br><span class="line">    num_same = <span class="built_in">sum</span>(common.values())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> num_same == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    precision = num_same / <span class="built_in">len</span>(pred_tokens)</span><br><span class="line">    recall = num_same / <span class="built_in">len</span>(gold_tokens)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">model, tokenizer, context: <span class="built_in">str</span>, question: <span class="built_in">str</span>, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;单条推理&quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    </span><br><span class="line">    encoding = tokenizer(</span><br><span class="line">        question, context,</span><br><span class="line">        max_length=<span class="number">384</span>,</span><br><span class="line">        truncation=<span class="string">&#x27;only_second&#x27;</span>,</span><br><span class="line">        return_tensors=<span class="string">&#x27;pt&#x27;</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    encoding = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> encoding.items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**encoding)</span><br><span class="line">    </span><br><span class="line">    start_idx = outputs[<span class="string">&#x27;start_logits&#x27;</span>].argmax().item()</span><br><span class="line">    end_idx = outputs[<span class="string">&#x27;end_logits&#x27;</span>].argmax().item()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保 end &gt;= start</span></span><br><span class="line">    <span class="keyword">if</span> end_idx &lt; start_idx:</span><br><span class="line">        end_idx = start_idx</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解码答案</span></span><br><span class="line">    answer_tokens = encoding[<span class="string">&#x27;input_ids&#x27;</span>][<span class="number">0</span>][start_idx:end_idx+<span class="number">1</span>]</span><br><span class="line">    answer = tokenizer.decode(answer_tokens, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure>

<h2 id="现代方法：使用-LLM"><a href="#现代方法：使用-LLM" class="headerlink" title="现代方法：使用 LLM"></a>现代方法：使用 LLM</h2><p>对于更复杂的问答需求，可以使用 LLM：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">llm_qa</span>(<span class="params">context: <span class="built_in">str</span>, question: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    client = OpenAI()</span><br><span class="line">    </span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个问答助手。根据给定的上下文回答问题。如果答案不在上下文中，请说&#x27;无法回答&#x27;。&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;上下文：<span class="subst">&#123;context&#125;</span>\n\n问题：<span class="subst">&#123;question&#125;</span>&quot;</span>&#125;</span><br><span class="line">        ],</span><br><span class="line">        temperature=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> response.choices[<span class="number">0</span>].message.content</span><br></pre></td></tr></table></figure>

<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li><a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD Dataset</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/task_summary#question-answering">Hugging Face QA Pipeline</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/11/19/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/11/19/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">条件随机场：原理与实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-11-19 08:51:23" itemprop="dateCreated datePublished" datetime="2019-11-19T08:51:23+08:00">2019-11-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>条件随机场 (CRF) 是序列标注的经典模型，尽管深度学习时代 BERT 等模型大放异彩，CRF 层仍然在 NER、词性标注等任务中发挥关键作用。</p>
<h2 id="为什么需要-CRF？"><a href="#为什么需要-CRF？" class="headerlink" title="为什么需要 CRF？"></a>为什么需要 CRF？</h2><h3 id="独立分类的问题"><a href="#独立分类的问题" class="headerlink" title="独立分类的问题"></a>独立分类的问题</h3><p>如果对每个位置独立分类：</p>
<p>$$<br>\hat{y}_i &#x3D; \arg\max_y P(y | x_i)<br>$$</p>
<p>会导致<strong>标签不一致</strong>，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;北 京 是 中 国 首 都&quot;</span><br><span class="line">错误: B-LOC I-PER O B-LOC I-LOC I-LOC I-LOC</span><br><span class="line">正确: B-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC</span><br></pre></td></tr></table></figure>

<h3 id="CRF-的解决方案"><a href="#CRF-的解决方案" class="headerlink" title="CRF 的解决方案"></a>CRF 的解决方案</h3><p>CRF 建模整个序列的联合概率，考虑<strong>标签之间的转移约束</strong>。</p>
<h2 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h2><h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>$$<br>P(Y|X) &#x3D; \frac{1}{Z(X)} \exp\left(\sum_{t&#x3D;1}^{T} \left(\phi(y_t, x, t) + \psi(y_{t-1}, y_t)\right)\right)<br>$$</p>
<p>其中：</p>
<ul>
<li>$\phi(y_t, x, t)$：发射分数（emission score）</li>
<li>$\psi(y_{t-1}, y_t)$：转移分数（transition score）</li>
<li>$Z(X)$：配分函数（归一化项）</li>
</ul>
<h3 id="配分函数"><a href="#配分函数" class="headerlink" title="配分函数"></a>配分函数</h3><p>$$<br>Z(X) &#x3D; \sum_{y \in \mathcal{Y}^T} \exp\left(\sum_{t&#x3D;1}^{T} \left(\phi(y_t, x, t) + \psi(y_{t-1}, y_t)\right)\right)<br>$$</p>
<p>直接计算复杂度为 $O(|\mathcal{Y}|^T)$，使用<strong>前向算法</strong>可降至 $O(T \cdot |\mathcal{Y}|^2)$。</p>
<h2 id="PyTorch-实现"><a href="#PyTorch-实现" class="headerlink" title="PyTorch 实现"></a>PyTorch 实现</h2><h3 id="CRF-Layer"><a href="#CRF-Layer" class="headerlink" title="CRF Layer"></a>CRF Layer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_tags, batch_first=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_tags = num_tags</span><br><span class="line">        <span class="variable language_">self</span>.batch_first = batch_first</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 转移矩阵: transitions[i, j] = 从标签 j 转移到标签 i 的分数</span></span><br><span class="line">        <span class="variable language_">self</span>.transitions = nn.Parameter(torch.randn(num_tags, num_tags))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 起始和结束转移</span></span><br><span class="line">        <span class="variable language_">self</span>.start_transitions = nn.Parameter(torch.randn(num_tags))</span><br><span class="line">        <span class="variable language_">self</span>.end_transitions = nn.Parameter(torch.randn(num_tags))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, emissions, tags, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算负对数似然损失&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = torch.ones_like(tags, dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            tags = tags.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算分子（正确路径的分数）</span></span><br><span class="line">        numerator = <span class="variable language_">self</span>._compute_score(emissions, tags, mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算分母（配分函数）</span></span><br><span class="line">        denominator = <span class="variable language_">self</span>._compute_normalizer(emissions, mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 负对数似然</span></span><br><span class="line">        <span class="keyword">return</span> (denominator - numerator).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_compute_score</span>(<span class="params">self, emissions, tags, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算给定标签序列的分数&quot;&quot;&quot;</span></span><br><span class="line">        seq_len, batch_size = tags.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 起始分数</span></span><br><span class="line">        score = <span class="variable language_">self</span>.start_transitions[tags[<span class="number">0</span>]]</span><br><span class="line">        score += emissions[<span class="number">0</span>, torch.arange(batch_size), tags[<span class="number">0</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, seq_len):</span><br><span class="line">            <span class="comment"># 转移分数 + 发射分数</span></span><br><span class="line">            score += <span class="variable language_">self</span>.transitions[tags[i], tags[i-<span class="number">1</span>]] * mask[i]</span><br><span class="line">            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 结束分数</span></span><br><span class="line">        last_tag_idx = mask.<span class="built_in">sum</span>(dim=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">        last_tags = tags.gather(<span class="number">0</span>, last_tag_idx.unsqueeze(<span class="number">0</span>)).squeeze(<span class="number">0</span>)</span><br><span class="line">        score += <span class="variable language_">self</span>.end_transitions[last_tags]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_compute_normalizer</span>(<span class="params">self, emissions, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向算法计算配分函数&quot;&quot;&quot;</span></span><br><span class="line">        seq_len, batch_size, num_tags = emissions.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        score = <span class="variable language_">self</span>.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, seq_len):</span><br><span class="line">            <span class="comment"># broadcast: (batch, num_tags, 1) + (num_tags, num_tags) + (batch, 1, num_tags)</span></span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line">            broadcast_emissions = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            next_score = broadcast_score + <span class="variable language_">self</span>.transitions + broadcast_emissions</span><br><span class="line">            next_score = torch.logsumexp(next_score, dim=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 应用 mask</span></span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加结束分数</span></span><br><span class="line">        score += <span class="variable language_">self</span>.end_transitions</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> torch.logsumexp(score, dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, emissions, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Viterbi 解码&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = torch.ones(emissions.shape[:<span class="number">2</span>], dtype=torch.<span class="built_in">bool</span>, device=emissions.device)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._viterbi_decode(emissions, mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_viterbi_decode</span>(<span class="params">self, emissions, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Viterbi 算法&quot;&quot;&quot;</span></span><br><span class="line">        seq_len, batch_size, num_tags = emissions.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        score = <span class="variable language_">self</span>.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line">        history = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, seq_len):</span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line">            broadcast_emissions = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            next_score = broadcast_score + <span class="variable language_">self</span>.transitions + broadcast_emissions</span><br><span class="line">            next_score, indices = next_score.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line">            history.append(indices)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加结束分数</span></span><br><span class="line">        score += <span class="variable language_">self</span>.end_transitions</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 回溯</span></span><br><span class="line">        best_tags_list = []</span><br><span class="line">        _, best_last_tag = score.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            best_tags = [best_last_tag[idx].item()]</span><br><span class="line">            seq_length = <span class="built_in">int</span>(mask[:, idx].<span class="built_in">sum</span>().item())</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> hist <span class="keyword">in</span> <span class="built_in">reversed</span>(history[:seq_length-<span class="number">1</span>]):</span><br><span class="line">                best_last_tag_idx = best_tags[-<span class="number">1</span>]</span><br><span class="line">                best_tags.append(hist[idx, best_last_tag_idx].item())</span><br><span class="line">            </span><br><span class="line">            best_tags.reverse()</span><br><span class="line">            best_tags_list.append(best_tags)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> best_tags_list</span><br></pre></td></tr></table></figure>

<h3 id="与-BiLSTM-结合"><a href="#与-BiLSTM-结合" class="headerlink" title="与 BiLSTM 结合"></a>与 BiLSTM 结合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTM_CRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_dim, num_tags</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        <span class="variable language_">self</span>.lstm = nn.LSTM(embed_dim, hidden_dim // <span class="number">2</span>, </span><br><span class="line">                           num_layers=<span class="number">2</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_dim, num_tags)</span><br><span class="line">        <span class="variable language_">self</span>.crf = CRF(num_tags)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, tags, mask=<span class="literal">None</span></span>):</span><br><span class="line">        embeddings = <span class="variable language_">self</span>.embedding(x)</span><br><span class="line">        lstm_out, _ = <span class="variable language_">self</span>.lstm(embeddings)</span><br><span class="line">        emissions = <span class="variable language_">self</span>.fc(lstm_out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.crf(emissions, tags, mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        embeddings = <span class="variable language_">self</span>.embedding(x)</span><br><span class="line">        lstm_out, _ = <span class="variable language_">self</span>.lstm(embeddings)</span><br><span class="line">        emissions = <span class="variable language_">self</span>.fc(lstm_out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.crf.decode(emissions, mask)</span><br></pre></td></tr></table></figure>

<h2 id="现代应用：BERT-CRF"><a href="#现代应用：BERT-CRF" class="headerlink" title="现代应用：BERT + CRF"></a>现代应用：BERT + CRF</h2><p>尽管 BERT 已经很强大，但 CRF 层仍能带来一致性提升：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERT_CRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bert_name, num_tags</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.bert = BertModel.from_pretrained(bert_name)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="variable language_">self</span>.bert.config.hidden_size, num_tags)</span><br><span class="line">        <span class="variable language_">self</span>.crf = CRF(num_tags)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, tags=<span class="literal">None</span></span>):</span><br><span class="line">        outputs = <span class="variable language_">self</span>.bert(input_ids, attention_mask=attention_mask)</span><br><span class="line">        sequence_output = <span class="variable language_">self</span>.dropout(outputs.last_hidden_state)</span><br><span class="line">        emissions = <span class="variable language_">self</span>.fc(sequence_output)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> tags <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.crf(emissions, tags, attention_mask.<span class="built_in">bool</span>())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.crf.decode(emissions, attention_mask.<span class="built_in">bool</span>())</span><br></pre></td></tr></table></figure>

<h3 id="性能对比（CoNLL-2003-NER）"><a href="#性能对比（CoNLL-2003-NER）" class="headerlink" title="性能对比（CoNLL-2003 NER）"></a>性能对比（CoNLL-2003 NER）</h3><table>
<thead>
<tr>
<th>模型</th>
<th>F1</th>
</tr>
</thead>
<tbody><tr>
<td>BiLSTM</td>
<td>88.2</td>
</tr>
<tr>
<td>BiLSTM + CRF</td>
<td>90.1</td>
</tr>
<tr>
<td>BERT</td>
<td>92.4</td>
</tr>
<tr>
<td>BERT + CRF</td>
<td>92.8</td>
</tr>
<tr>
<td>RoBERTa + CRF</td>
<td>93.2</td>
</tr>
</tbody></table>
<h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><h3 id="1-标签平滑"><a href="#1-标签平滑" class="headerlink" title="1. 标签平滑"></a>1. 标签平滑</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">label_smoothing_loss</span>(<span class="params">crf, emissions, tags, mask, epsilon=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带标签平滑的 CRF 损失&quot;&quot;&quot;</span></span><br><span class="line">    nll_loss = crf(emissions, tags, mask)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 均匀分布的损失</span></span><br><span class="line">    uniform_loss = -torch.logsumexp(emissions, dim=-<span class="number">1</span>).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> - epsilon) * nll_loss + epsilon * uniform_loss</span><br></pre></td></tr></table></figure>

<h3 id="2-约束解码"><a href="#2-约束解码" class="headerlink" title="2. 约束解码"></a>2. 约束解码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加硬约束：B-X 后面只能接 I-X 或 O</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_constraints</span>(<span class="params">transitions, tag2idx</span>):</span><br><span class="line">    <span class="keyword">for</span> tag_from, idx_from <span class="keyword">in</span> tag2idx.items():</span><br><span class="line">        <span class="keyword">for</span> tag_to, idx_to <span class="keyword">in</span> tag2idx.items():</span><br><span class="line">            <span class="keyword">if</span> tag_from.startswith(<span class="string">&#x27;B-&#x27;</span>) <span class="keyword">or</span> tag_from.startswith(<span class="string">&#x27;I-&#x27;</span>):</span><br><span class="line">                entity = tag_from[<span class="number">2</span>:]</span><br><span class="line">                <span class="keyword">if</span> tag_to.startswith(<span class="string">&#x27;I-&#x27;</span>) <span class="keyword">and</span> tag_to[<span class="number">2</span>:] != entity:</span><br><span class="line">                    transitions.data[idx_to, idx_from] = -<span class="number">1e9</span></span><br></pre></td></tr></table></figure>

<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li>Lafferty et al., <em>Conditional Random Fields</em> (2001)</li>
<li>Huang et al., <em>Bidirectional LSTM-CRF Models for Sequence Tagging</em> (2015)</li>
<li><a target="_blank" rel="noopener" href="https://pytorch-crf.readthedocs.io/">pytorch-crf Documentation</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/11/19/%E8%AE%BA%E6%96%87%E6%A2%97%E6%A6%82%EF%BC%9ABi-Directional-Attention-Flow-for-Machine-Comprehension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/11/19/%E8%AE%BA%E6%96%87%E6%A2%97%E6%A6%82%EF%BC%9ABi-Directional-Attention-Flow-for-Machine-Comprehension/" class="post-title-link" itemprop="url">BiDAF 论文解读：双向注意力流机制</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-11-19 08:47:58" itemprop="dateCreated datePublished" datetime="2019-11-19T08:47:58+08:00">2019-11-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>BiDAF (Bi-Directional Attention Flow) 是机器阅读理解领域的经典模型，其双向注意力机制对后续 Transformer 架构产生了深远影响。</p>
<h2 id="核心创新"><a href="#核心创新" class="headerlink" title="核心创新"></a>核心创新</h2><h3 id="1-Memory-less-Attention"><a href="#1-Memory-less-Attention" class="headerlink" title="1. Memory-less Attention"></a>1. Memory-less Attention</h3><p>传统动态注意力 vs BiDAF 的无记忆注意力：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>Dynamic Attention</th>
<th>Memory-less Attention</th>
</tr>
</thead>
<tbody><tr>
<td>依赖</td>
<td>前一时间步的 attended vector</td>
<td>仅当前 query 和 context</td>
</tr>
<tr>
<td>优势</td>
<td>可建模时序依赖</td>
<td>避免错误累积</td>
</tr>
<tr>
<td>缺点</td>
<td>错误会传播</td>
<td>无法建模长程依赖</td>
</tr>
</tbody></table>
<h3 id="2-双向注意力"><a href="#2-双向注意力" class="headerlink" title="2. 双向注意力"></a>2. 双向注意力</h3><p>同时计算：</p>
<ul>
<li><strong>Context-to-Query (C2Q)</strong>：每个 context 词最相关的 query 词</li>
<li><strong>Query-to-Context (Q2C)</strong>：对回答问题最关键的 context 词</li>
</ul>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input → Embedding → Encoding → Attention → Modeling → Output</span><br><span class="line">  │         │           │          │           │         │</span><br><span class="line"> 词向量    字符CNN     BiLSTM    双向注意力   BiLSTM   Span预测</span><br></pre></td></tr></table></figure>

<h3 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a>数学表达</h3><p><strong>相似度矩阵</strong>：</p>
<p>$$<br>S_{ij} &#x3D; \alpha(H_i, U_j) &#x3D; w^T[H_i; U_j; H_i \odot U_j]<br>$$</p>
<p>其中 $H \in \mathbb{R}^{T \times d}$ 是 context 表示，$U \in \mathbb{R}^{J \times d}$ 是 query 表示。</p>
<p><strong>C2Q Attention</strong>：</p>
<p>$$<br>\tilde{U}<em>i &#x3D; \sum_j a</em>{ij} U_j, \quad a_i &#x3D; \text{softmax}(S_i)<br>$$</p>
<p><strong>Q2C Attention</strong>：</p>
<p>$$<br>\tilde{H} &#x3D; \sum_i b_i H_i, \quad b &#x3D; \text{softmax}(\max_j S_{:j})<br>$$</p>
<p><strong>融合表示</strong>：</p>
<p>$$<br>G_i &#x3D; [H_i; \tilde{U}_i; H_i \odot \tilde{U}_i; H_i \odot \tilde{H}]<br>$$</p>
<h2 id="PyTorch-实现"><a href="#PyTorch-实现" class="headerlink" title="PyTorch 实现"></a>PyTorch 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiDAFAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.W = nn.Linear(hidden_size * <span class="number">3</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, context, query, c_mask, q_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            context: (batch, c_len, hidden)</span></span><br><span class="line"><span class="string">            query: (batch, q_len, hidden)</span></span><br><span class="line"><span class="string">            c_mask: (batch, c_len)</span></span><br><span class="line"><span class="string">            q_mask: (batch, q_len)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch, c_len, hidden = context.size()</span><br><span class="line">        q_len = query.size(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 扩展维度以计算所有 (i, j) 对</span></span><br><span class="line">        c_expand = context.unsqueeze(<span class="number">2</span>).expand(-<span class="number">1</span>, -<span class="number">1</span>, q_len, -<span class="number">1</span>)</span><br><span class="line">        q_expand = query.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, c_len, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算相似度矩阵 S</span></span><br><span class="line">        cq = torch.cat([c_expand, q_expand, c_expand * q_expand], dim=-<span class="number">1</span>)</span><br><span class="line">        S = <span class="variable language_">self</span>.W(cq).squeeze(-<span class="number">1</span>)  <span class="comment"># (batch, c_len, q_len)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Mask</span></span><br><span class="line">        q_mask_expand = q_mask.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, c_len, -<span class="number">1</span>)</span><br><span class="line">        S = S.masked_fill(~q_mask_expand, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># C2Q attention</span></span><br><span class="line">        a = torch.softmax(S, dim=-<span class="number">1</span>)</span><br><span class="line">        c2q = torch.bmm(a, query)  <span class="comment"># (batch, c_len, hidden)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q2C attention</span></span><br><span class="line">        b = torch.softmax(S.<span class="built_in">max</span>(dim=-<span class="number">1</span>)[<span class="number">0</span>], dim=-<span class="number">1</span>)</span><br><span class="line">        q2c = torch.bmm(b.unsqueeze(<span class="number">1</span>), context)  <span class="comment"># (batch, 1, hidden)</span></span><br><span class="line">        q2c = q2c.expand(-<span class="number">1</span>, c_len, -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 融合</span></span><br><span class="line">        G = torch.cat([context, c2q, context * c2q, context * q2c], dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> G</span><br></pre></td></tr></table></figure>

<h2 id="与-Transformer-的对比"><a href="#与-Transformer-的对比" class="headerlink" title="与 Transformer 的对比"></a>与 Transformer 的对比</h2><table>
<thead>
<tr>
<th>特性</th>
<th>BiDAF</th>
<th>Transformer</th>
</tr>
</thead>
<tbody><tr>
<td>注意力方向</td>
<td>双向（C2Q, Q2C）</td>
<td>全方向自注意力</td>
</tr>
<tr>
<td>位置编码</td>
<td>BiLSTM 隐式编码</td>
<td>显式位置编码</td>
</tr>
<tr>
<td>并行化</td>
<td>受限于 RNN</td>
<td>完全并行</td>
</tr>
<tr>
<td>长距离依赖</td>
<td>受限</td>
<td>理论上无限</td>
</tr>
<tr>
<td>参数量</td>
<td>较少</td>
<td>较多</td>
</tr>
</tbody></table>
<h2 id="现代演进"><a href="#现代演进" class="headerlink" title="现代演进"></a>现代演进</h2><p>BiDAF 的思想在现代模型中的体现：</p>
<h3 id="1-Cross-Attention-in-Transformer"><a href="#1-Cross-Attention-in-Transformer" class="headerlink" title="1. Cross-Attention in Transformer"></a>1. Cross-Attention in Transformer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CrossAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, n_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.mha = nn.MultiheadAttention(d_model, n_heads)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key_value</span>):</span><br><span class="line">        <span class="comment"># query 来自一个序列，key/value 来自另一个序列</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.mha(query, key_value, key_value)</span><br></pre></td></tr></table></figure>

<h3 id="2-FiD-Fusion-in-Decoder"><a href="#2-FiD-Fusion-in-Decoder" class="headerlink" title="2. FiD (Fusion-in-Decoder)"></a>2. FiD (Fusion-in-Decoder)</h3><p>用于 RAG 的架构，类似 BiDAF 的融合思想：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FiD</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, question, passages</span>):</span><br><span class="line">        <span class="comment"># 独立编码每个 passage</span></span><br><span class="line">        encoded = []</span><br><span class="line">        <span class="keyword">for</span> passage <span class="keyword">in</span> passages:</span><br><span class="line">            enc = <span class="variable language_">self</span>.encoder(question + passage)</span><br><span class="line">            encoded.append(enc)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 融合解码</span></span><br><span class="line">        fused = torch.cat(encoded, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(fused)</span><br></pre></td></tr></table></figure>

<h2 id="实验结果（原论文）"><a href="#实验结果（原论文）" class="headerlink" title="实验结果（原论文）"></a>实验结果（原论文）</h2><p>在 SQuAD 1.1 上的表现：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>EM</th>
<th>F1</th>
</tr>
</thead>
<tbody><tr>
<td>BiDAF</td>
<td>67.7</td>
<td>77.3</td>
</tr>
<tr>
<td>BiDAF + Self Attention</td>
<td>72.1</td>
<td>81.1</td>
</tr>
<tr>
<td>BERT-base</td>
<td>80.8</td>
<td>88.5</td>
</tr>
<tr>
<td>GPT-4 (few-shot)</td>
<td>~90</td>
<td>~95</td>
</tr>
</tbody></table>
<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01603">BiDAF Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT for QA</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E5%8E%BB%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%87%E6%9C%AC%E5%B9%B6%E4%B8%94%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%EF%BC%88tensorflow%E5%AE%9E%E6%88%98%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/11/19/%E5%A6%82%E4%BD%95%E6%95%99%E4%BC%9A%E6%9C%BA%E5%99%A8%E5%8E%BB%E7%90%86%E8%A7%A3%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%87%E6%9C%AC%E5%B9%B6%E4%B8%94%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%EF%BC%88tensorflow%E5%AE%9E%E6%88%98%EF%BC%89/" class="post-title-link" itemprop="url">MRC 模型实现：从 TensorFlow 到 PyTorch</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-11-19 05:16:27" itemprop="dateCreated datePublished" datetime="2019-11-19T05:16:27+08:00">2019-11-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本文介绍机器阅读理解模型的完整实现，涵盖经典架构和现代最佳实践。</p>
<h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p><strong>输入</strong>：</p>
<ul>
<li>问题 $Q &#x3D; (q_1, q_2, …, q_m)$</li>
<li>文档 $P &#x3D; (p_1, p_2, …, p_n)$</li>
</ul>
<p><strong>输出</strong>：</p>
<ul>
<li>答案起始位置 $start \in [1, n]$</li>
<li>答案结束位置 $end \in [start, n]$</li>
</ul>
<h2 id="经典架构"><a href="#经典架构" class="headerlink" title="经典架构"></a>经典架构</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Input → Embedding → Encoding → Matching → Fusion → Decoding</span><br></pre></td></tr></table></figure>

<h3 id="各层详解"><a href="#各层详解" class="headerlink" title="各层详解"></a>各层详解</h3><table>
<thead>
<tr>
<th>层</th>
<th>功能</th>
<th>现代替代</th>
</tr>
</thead>
<tbody><tr>
<td>Embedding</td>
<td>Token → Vector</td>
<td>Subword Tokenization</td>
</tr>
<tr>
<td>Encoding</td>
<td>序列编码</td>
<td>Transformer Encoder</td>
</tr>
<tr>
<td>Matching</td>
<td>Q-P 交互</td>
<td>Cross-Attention</td>
</tr>
<tr>
<td>Fusion</td>
<td>信息融合</td>
<td>Self-Attention</td>
</tr>
<tr>
<td>Decoding</td>
<td>Span 预测</td>
<td>Linear + Softmax</td>
</tr>
</tbody></table>
<h2 id="PyTorch-实现"><a href="#PyTorch-实现" class="headerlink" title="PyTorch 实现"></a>PyTorch 实现</h2><h3 id="完整模型"><a href="#完整模型" class="headerlink" title="完整模型"></a>完整模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel, AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MRCModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于 Transformer 的 MRC 模型&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, </span></span><br><span class="line"><span class="params">        model_name: <span class="built_in">str</span> = <span class="string">&quot;bert-base-chinese&quot;</span>,</span></span><br><span class="line"><span class="params">        dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        max_answer_length: <span class="built_in">int</span> = <span class="number">30</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = AutoModel.from_pretrained(model_name)</span><br><span class="line">        hidden_size = <span class="variable language_">self</span>.encoder.config.hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.max_answer_length = max_answer_length</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.start_fc = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.end_fc = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_ids: torch.Tensor,</span></span><br><span class="line"><span class="params">        attention_mask: torch.Tensor,</span></span><br><span class="line"><span class="params">        token_type_ids: torch.Tensor = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        start_positions: torch.Tensor = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        end_positions: torch.Tensor = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="comment"># 编码</span></span><br><span class="line">        outputs = <span class="variable language_">self</span>.encoder(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">        )</span><br><span class="line">        sequence_output = <span class="variable language_">self</span>.dropout(outputs.last_hidden_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 预测 start/end</span></span><br><span class="line">        start_logits = <span class="variable language_">self</span>.start_fc(sequence_output).squeeze(-<span class="number">1</span>)</span><br><span class="line">        end_logits = <span class="variable language_">self</span>.end_fc(sequence_output).squeeze(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Mask padding</span></span><br><span class="line">        mask = attention_mask.<span class="built_in">bool</span>()</span><br><span class="line">        start_logits = start_logits.masked_fill(~mask, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        end_logits = end_logits.masked_fill(~mask, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> start_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> end_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss_fct = nn.CrossEntropyLoss(ignore_index=-<span class="number">1</span>)</span><br><span class="line">            start_loss = loss_fct(start_logits, start_positions)</span><br><span class="line">            end_loss = loss_fct(end_logits, end_positions)</span><br><span class="line">            loss = (start_loss + end_loss) / <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span>: loss,</span><br><span class="line">            <span class="string">&#x27;start_logits&#x27;</span>: start_logits,</span><br><span class="line">            <span class="string">&#x27;end_logits&#x27;</span>: end_logits,</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        start_logits: torch.Tensor,</span></span><br><span class="line"><span class="params">        end_logits: torch.Tensor,</span></span><br><span class="line"><span class="params">        attention_mask: torch.Tensor,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;解码最佳答案 span&quot;&quot;&quot;</span></span><br><span class="line">        batch_size, seq_len = start_logits.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算所有有效 (start, end) 对的分数</span></span><br><span class="line">        start_probs = F.softmax(start_logits, dim=-<span class="number">1</span>)</span><br><span class="line">        end_probs = F.softmax(end_logits, dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        results = []</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            best_score = <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br><span class="line">            best_start, best_end = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> start <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> attention_mask[b, start]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">for</span> end <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">min</span>(start + <span class="variable language_">self</span>.max_answer_length, seq_len)):</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> attention_mask[b, end]:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    score = start_probs[b, start] * end_probs[b, end]</span><br><span class="line">                    <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">                        best_score = score</span><br><span class="line">                        best_start, best_end = start, end</span><br><span class="line">            </span><br><span class="line">            results.append((best_start, best_end))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>

<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MRCExample</span>:</span><br><span class="line">    qid: <span class="built_in">str</span></span><br><span class="line">    question: <span class="built_in">str</span></span><br><span class="line">    context: <span class="built_in">str</span></span><br><span class="line">    answer: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br><span class="line">    start_position: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MRCFeature</span>:</span><br><span class="line">    input_ids: <span class="type">List</span>[<span class="built_in">int</span>]</span><br><span class="line">    attention_mask: <span class="type">List</span>[<span class="built_in">int</span>]</span><br><span class="line">    token_type_ids: <span class="type">List</span>[<span class="built_in">int</span>]</span><br><span class="line">    start_position: <span class="built_in">int</span></span><br><span class="line">    end_position: <span class="built_in">int</span></span><br><span class="line">    offset_mapping: <span class="type">List</span>[<span class="built_in">tuple</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MRCProcessor</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name: <span class="built_in">str</span>, max_length: <span class="built_in">int</span> = <span class="number">512</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">self, example: MRCExample</span>) -&gt; MRCFeature:</span><br><span class="line">        encoding = <span class="variable language_">self</span>.tokenizer(</span><br><span class="line">            example.question,</span><br><span class="line">            example.context,</span><br><span class="line">            max_length=<span class="variable language_">self</span>.max_length,</span><br><span class="line">            truncation=<span class="string">&#x27;only_second&#x27;</span>,</span><br><span class="line">            return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">            padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定位答案位置</span></span><br><span class="line">        start_token, end_token = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> example.start_position <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            offset = encoding[<span class="string">&#x27;offset_mapping&#x27;</span>]</span><br><span class="line">            <span class="keyword">for</span> idx, (start, end) <span class="keyword">in</span> <span class="built_in">enumerate</span>(offset):</span><br><span class="line">                <span class="keyword">if</span> start &lt;= example.start_position &lt; end:</span><br><span class="line">                    start_token = idx</span><br><span class="line">                <span class="keyword">if</span> start &lt; example.start_position + <span class="built_in">len</span>(example.answer) &lt;= end:</span><br><span class="line">                    end_token = idx</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> MRCFeature(</span><br><span class="line">            input_ids=encoding[<span class="string">&#x27;input_ids&#x27;</span>],</span><br><span class="line">            attention_mask=encoding[<span class="string">&#x27;attention_mask&#x27;</span>],</span><br><span class="line">            token_type_ids=encoding.get(<span class="string">&#x27;token_type_ids&#x27;</span>, [<span class="number">0</span>] * <span class="built_in">len</span>(encoding[<span class="string">&#x27;input_ids&#x27;</span>])),</span><br><span class="line">            start_position=start_token,</span><br><span class="line">            end_position=end_token,</span><br><span class="line">            offset_mapping=encoding[<span class="string">&#x27;offset_mapping&#x27;</span>],</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<h3 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">model, dataloader, optimizer, scheduler, device</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(dataloader, desc=<span class="string">&quot;Training&quot;</span>):</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        </span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">        </span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(dataloader)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, dataloader, device</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    predictions = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(dataloader, desc=<span class="string">&quot;Evaluating&quot;</span>):</span><br><span class="line">            batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">            </span><br><span class="line">            outputs = model(</span><br><span class="line">                input_ids=batch[<span class="string">&#x27;input_ids&#x27;</span>],</span><br><span class="line">                attention_mask=batch[<span class="string">&#x27;attention_mask&#x27;</span>],</span><br><span class="line">                token_type_ids=batch.get(<span class="string">&#x27;token_type_ids&#x27;</span>),</span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line">            spans = model.decode(</span><br><span class="line">                outputs[<span class="string">&#x27;start_logits&#x27;</span>],</span><br><span class="line">                outputs[<span class="string">&#x27;end_logits&#x27;</span>],</span><br><span class="line">                batch[<span class="string">&#x27;attention_mask&#x27;</span>],</span><br><span class="line">            )</span><br><span class="line">            predictions.extend(spans)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主训练流程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 配置</span></span><br><span class="line">    model_name = <span class="string">&quot;bert-base-chinese&quot;</span></span><br><span class="line">    batch_size = <span class="number">16</span></span><br><span class="line">    learning_rate = <span class="number">3e-5</span></span><br><span class="line">    num_epochs = <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    model = MRCModel(model_name).to(device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化器</span></span><br><span class="line">    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=<span class="number">0.01</span>)</span><br><span class="line">    scheduler = get_scheduler(</span><br><span class="line">        <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        num_warmup_steps=<span class="number">500</span>,</span><br><span class="line">        num_training_steps=num_epochs * <span class="built_in">len</span>(train_dataloader),</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 验证</span></span><br><span class="line">        predictions = evaluate(model, val_dataloader, device)</span><br><span class="line">        f1 = compute_f1(predictions, val_labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Validation F1: <span class="subst">&#123;f1:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_answer</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;标准化答案文本&quot;&quot;&quot;</span></span><br><span class="line">    s = s.lower()</span><br><span class="line">    s = re.sub(<span class="string">r&#x27;\b(a|an|the)\b&#x27;</span>, <span class="string">&#x27; &#x27;</span>, s)</span><br><span class="line">    s = <span class="string">&#x27;&#x27;</span>.join(ch <span class="keyword">for</span> ch <span class="keyword">in</span> s <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation)</span><br><span class="line">    s = <span class="string">&#x27; &#x27;</span>.join(s.split())</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_f1</span>(<span class="params">prediction: <span class="built_in">str</span>, ground_truth: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    pred_tokens = normalize_answer(prediction).split()</span><br><span class="line">    gold_tokens = normalize_answer(ground_truth).split()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pred_tokens <span class="keyword">or</span> <span class="keyword">not</span> gold_tokens:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(pred_tokens == gold_tokens)</span><br><span class="line">    </span><br><span class="line">    common = Counter(pred_tokens) &amp; Counter(gold_tokens)</span><br><span class="line">    num_same = <span class="built_in">sum</span>(common.values())</span><br><span class="line">    </span><br><span class="line">    precision = num_same / <span class="built_in">len</span>(pred_tokens)</span><br><span class="line">    recall = num_same / <span class="built_in">len</span>(gold_tokens)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> precision + recall == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_em</span>(<span class="params">prediction: <span class="built_in">str</span>, ground_truth: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(normalize_answer(prediction) == normalize_answer(ground_truth))</span><br></pre></td></tr></table></figure>

<h2 id="与现代方法对比"><a href="#与现代方法对比" class="headerlink" title="与现代方法对比"></a>与现代方法对比</h2><table>
<thead>
<tr>
<th>方面</th>
<th>经典 MRC (BiDAF)</th>
<th>BERT-based</th>
<th>LLM-based</th>
</tr>
</thead>
<tbody><tr>
<td>参数量</td>
<td>~2M</td>
<td>110M-340M</td>
<td>7B-70B+</td>
</tr>
<tr>
<td>训练数据</td>
<td>Task-specific</td>
<td>预训练+微调</td>
<td>大规模预训练</td>
</tr>
<tr>
<td>推理方式</td>
<td>Span extraction</td>
<td>Span extraction</td>
<td>Generation</td>
</tr>
<tr>
<td>长文档</td>
<td>需要切分</td>
<td>需要切分</td>
<td>更大上下文窗口</td>
</tr>
<tr>
<td>多跳推理</td>
<td>困难</td>
<td>有限</td>
<td>较好</td>
</tr>
</tbody></table>
<h2 id="生产环境优化"><a href="#生产环境优化" class="headerlink" title="生产环境优化"></a>生产环境优化</h2><h3 id="量化推理"><a href="#量化推理" class="headerlink" title="量化推理"></a>量化推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.quantization <span class="keyword">as</span> quant</span><br><span class="line"></span><br><span class="line"><span class="comment"># 动态量化</span></span><br><span class="line">model_int8 = quant.quantize_dynamic(</span><br><span class="line">    model.cpu(),</span><br><span class="line">    &#123;nn.Linear&#125;,</span><br><span class="line">    dtype=torch.qint8</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="ONNX-导出"><a href="#ONNX-导出" class="headerlink" title="ONNX 导出"></a>ONNX 导出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.onnx</span><br><span class="line"></span><br><span class="line">dummy_input = &#123;</span><br><span class="line">    <span class="string">&#x27;input_ids&#x27;</span>: torch.ones(<span class="number">1</span>, <span class="number">512</span>, dtype=torch.long),</span><br><span class="line">    <span class="string">&#x27;attention_mask&#x27;</span>: torch.ones(<span class="number">1</span>, <span class="number">512</span>, dtype=torch.long),</span><br><span class="line">    <span class="string">&#x27;token_type_ids&#x27;</span>: torch.zeros(<span class="number">1</span>, <span class="number">512</span>, dtype=torch.long),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">torch.onnx.export(</span><br><span class="line">    model,</span><br><span class="line">    (dummy_input[<span class="string">&#x27;input_ids&#x27;</span>], dummy_input[<span class="string">&#x27;attention_mask&#x27;</span>], dummy_input[<span class="string">&#x27;token_type_ids&#x27;</span>]),</span><br><span class="line">    <span class="string">&quot;mrc_model.onnx&quot;</span>,</span><br><span class="line">    input_names=[<span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>],</span><br><span class="line">    output_names=[<span class="string">&#x27;start_logits&#x27;</span>, <span class="string">&#x27;end_logits&#x27;</span>],</span><br><span class="line">    dynamic_axes=&#123;</span><br><span class="line">        <span class="string">&#x27;input_ids&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;seq&#x27;</span>&#125;,</span><br><span class="line">        <span class="string">&#x27;attention_mask&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;seq&#x27;</span>&#125;,</span><br><span class="line">        <span class="string">&#x27;token_type_ids&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;seq&#x27;</span>&#125;,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/task_summary#question-answering">Hugging Face Question Answering</a></li>
<li><a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD Leaderboard</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ymcui/cmrc2018">CMRC 2018 中文阅读理解</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/10/31/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E2%80%94%E2%80%94%E8%AF%BB%E9%A6%99%E4%BE%AC%E7%A7%91%E6%8A%80%E6%9D%8E%E7%BA%A7%E4%B8%BA%E3%80%8A%E5%87%BA%E5%85%A5NLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BB%BA%E8%AE%AE%E3%80%8B%E6%96%87%E7%AB%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/31/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E2%80%94%E2%80%94%E8%AF%BB%E9%A6%99%E4%BE%AC%E7%A7%91%E6%8A%80%E6%9D%8E%E7%BA%A7%E4%B8%BA%E3%80%8A%E5%87%BA%E5%85%A5NLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BB%BA%E8%AE%AE%E3%80%8B%E6%96%87%E7%AB%A0/" class="post-title-link" itemprop="url">NLP 学习路线：从基础到大语言模型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-10-31 09:31:53" itemprop="dateCreated datePublished" datetime="2019-10-31T09:31:53+08:00">2019-10-31</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本文整理了 NLP 领域的学习路线，结合经典理论与现代大语言模型技术。</p>
<h2 id="推荐学习资源"><a href="#推荐学习资源" class="headerlink" title="推荐学习资源"></a>推荐学习资源</h2><h3 id="经典教材"><a href="#经典教材" class="headerlink" title="经典教材"></a>经典教材</h3><table>
<thead>
<tr>
<th>书籍</th>
<th>内容</th>
<th>难度</th>
</tr>
</thead>
<tbody><tr>
<td><em>Speech and Language Processing</em> (Jurafsky)</td>
<td>NLP 全面综述</td>
<td>⭐⭐</td>
</tr>
<tr>
<td><em>Introduction to Information Retrieval</em></td>
<td>信息检索基础</td>
<td>⭐⭐</td>
</tr>
<tr>
<td><em>Pattern Recognition and Machine Learning</em></td>
<td>机器学习理论</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><em>Deep Learning</em> (Goodfellow)</td>
<td>深度学习基础</td>
<td>⭐⭐⭐</td>
</tr>
</tbody></table>
<h3 id="现代资源"><a href="#现代资源" class="headerlink" title="现代资源"></a>现代资源</h3><ul>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs224n/">Stanford CS224N: NLP with Deep Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/course">Hugging Face Course</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.cohere.com/docs/llmu">LLM University by Cohere</a></li>
</ul>
<h2 id="阶段一：NLP-基础"><a href="#阶段一：NLP-基础" class="headerlink" title="阶段一：NLP 基础"></a>阶段一：NLP 基础</h2><h3 id="语言模型基础"><a href="#语言模型基础" class="headerlink" title="语言模型基础"></a>语言模型基础</h3><p><strong>N-gram 模型</strong>：N-1 阶马尔可夫假设</p>
<p>$$<br>P(w_k | w_1, …, w_{k-1}) \approx P(w_k | w_{k-n+1}, …, w_{k-1})<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NGramLM</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n=<span class="number">3</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.n = n</span><br><span class="line">        <span class="variable language_">self</span>.counts = defaultdict(<span class="keyword">lambda</span>: defaultdict(<span class="built_in">int</span>))</span><br><span class="line">        <span class="variable language_">self</span>.totals = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, corpus</span>):</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">            tokens = [<span class="string">&#x27;&lt;s&gt;&#x27;</span>] * (<span class="variable language_">self</span>.n - <span class="number">1</span>) + sentence + [<span class="string">&#x27;&lt;/s&gt;&#x27;</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tokens) - <span class="variable language_">self</span>.n + <span class="number">1</span>):</span><br><span class="line">                context = <span class="built_in">tuple</span>(tokens[i:i+<span class="variable language_">self</span>.n-<span class="number">1</span>])</span><br><span class="line">                word = tokens[i+<span class="variable language_">self</span>.n-<span class="number">1</span>]</span><br><span class="line">                <span class="variable language_">self</span>.counts[context][word] += <span class="number">1</span></span><br><span class="line">                <span class="variable language_">self</span>.totals[context] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">probability</span>(<span class="params">self, word, context</span>):</span><br><span class="line">        context = <span class="built_in">tuple</span>(context[-(<span class="variable language_">self</span>.n-<span class="number">1</span>):])</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.counts[context][word] / <span class="built_in">max</span>(<span class="variable language_">self</span>.totals[context], <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>从 One-hot 到 Dense Embedding 的演进：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>年份</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>One-hot</td>
<td>-</td>
<td>稀疏，无语义</td>
</tr>
<tr>
<td>Word2Vec</td>
<td>2013</td>
<td>分布式表示</td>
</tr>
<tr>
<td>GloVe</td>
<td>2014</td>
<td>全局统计</td>
</tr>
<tr>
<td>FastText</td>
<td>2016</td>
<td>子词信息</td>
</tr>
<tr>
<td>ELMo</td>
<td>2018</td>
<td>上下文相关</td>
</tr>
<tr>
<td>BERT</td>
<td>2018</td>
<td>双向上下文</td>
</tr>
</tbody></table>
<h2 id="阶段二：深度学习-NLP"><a href="#阶段二：深度学习-NLP" class="headerlink" title="阶段二：深度学习 NLP"></a>阶段二：深度学习 NLP</h2><h3 id="Transformer-架构"><a href="#Transformer-架构" class="headerlink" title="Transformer 架构"></a>Transformer 架构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, n_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // n_heads</span><br><span class="line">        <span class="variable language_">self</span>.n_heads = n_heads</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size = Q.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Linear projections</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.W_q(Q).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = <span class="variable language_">self</span>.W_k(K).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = <span class="variable language_">self</span>.W_v(V).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Attention scores</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.d_k)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        attn = torch.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn, V)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Concatenate and project</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.n_heads * <span class="variable language_">self</span>.d_k)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W_o(output)</span><br></pre></td></tr></table></figure>

<h3 id="注意力机制的数学表达"><a href="#注意力机制的数学表达" class="headerlink" title="注意力机制的数学表达"></a>注意力机制的数学表达</h3><p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p>
<h2 id="阶段三：大语言模型"><a href="#阶段三：大语言模型" class="headerlink" title="阶段三：大语言模型"></a>阶段三：大语言模型</h2><h3 id="LLM-架构演进"><a href="#LLM-架构演进" class="headerlink" title="LLM 架构演进"></a>LLM 架构演进</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GPT-1 (2018) → GPT-2 → GPT-3 → ChatGPT → GPT-4</span><br><span class="line">     ↓</span><br><span class="line">BERT → RoBERTa → DeBERTa</span><br><span class="line">     ↓</span><br><span class="line">T5 → Flan-T5 → UL2</span><br><span class="line">     ↓</span><br><span class="line">LLaMA → LLaMA 2 → Mistral → Mixtral</span><br></pre></td></tr></table></figure>

<h3 id="Prompt-Engineering"><a href="#Prompt-Engineering" class="headerlink" title="Prompt Engineering"></a>Prompt Engineering</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Zero-shot</span></span><br><span class="line">prompt = <span class="string">&quot;Translate to French: Hello, how are you?&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Few-shot</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;Translate to French:</span></span><br><span class="line"><span class="string">Hello -&gt; Bonjour</span></span><br><span class="line"><span class="string">Goodbye -&gt; Au revoir</span></span><br><span class="line"><span class="string">How are you? -&gt;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Chain-of-Thought</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;Q: If I have 3 apples and buy 5 more, how many do I have?</span></span><br><span class="line"><span class="string">A: Let&#x27;s think step by step.</span></span><br><span class="line"><span class="string">1. I start with 3 apples.</span></span><br><span class="line"><span class="string">2. I buy 5 more apples.</span></span><br><span class="line"><span class="string">3. Total = 3 + 5 = 8 apples.</span></span><br><span class="line"><span class="string">The answer is 8.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: If I have 7 oranges and eat 2, how many remain?</span></span><br><span class="line"><span class="string">A: Let&#x27;s think step by step.&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="Fine-tuning-技术"><a href="#Fine-tuning-技术" class="headerlink" title="Fine-tuning 技术"></a>Fine-tuning 技术</h3><table>
<thead>
<tr>
<th>方法</th>
<th>可训练参数</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Full Fine-tuning</strong></td>
<td>100%</td>
<td>大量数据，充足算力</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>0.1-1%</td>
<td>资源受限</td>
</tr>
<tr>
<td><strong>QLoRA</strong></td>
<td>0.1%</td>
<td>消费级 GPU</td>
</tr>
<tr>
<td><strong>Prefix Tuning</strong></td>
<td>0.1%</td>
<td>多任务</td>
</tr>
<tr>
<td><strong>Prompt Tuning</strong></td>
<td>&lt;0.01%</td>
<td>极端资源受限</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,</span><br><span class="line">    lora_alpha=<span class="number">32</span>,</span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>],</span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">    bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = get_peft_model(base_model, lora_config)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Trainable params: <span class="subst">&#123;model.print_trainable_parameters()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="阶段四：高级主题"><a href="#阶段四：高级主题" class="headerlink" title="阶段四：高级主题"></a>阶段四：高级主题</h2><h3 id="检索增强生成-RAG"><a href="#检索增强生成-RAG" class="headerlink" title="检索增强生成 (RAG)"></a>检索增强生成 (RAG)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建向量库</span></span><br><span class="line">embeddings = HuggingFaceEmbeddings(model_name=<span class="string">&quot;BAAI/bge-small-zh&quot;</span>)</span><br><span class="line">vectorstore = Chroma.from_documents(documents, embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 RAG 链</span></span><br><span class="line">qa = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    retriever=vectorstore.as_retriever(search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">3</span>&#125;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 困惑度 (Perplexity)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">perplexity</span>(<span class="params">model, tokenizer, text</span>):</span><br><span class="line">    encodings = tokenizer(text, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">    max_length = model.config.n_positions</span><br><span class="line">    </span><br><span class="line">    nlls = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, encodings.input_ids.size(<span class="number">1</span>), max_length):</span><br><span class="line">        begin_loc = <span class="built_in">max</span>(i - max_length, <span class="number">0</span>)</span><br><span class="line">        end_loc = i + max_length</span><br><span class="line">        input_ids = encodings.input_ids[:, begin_loc:end_loc]</span><br><span class="line">        target_ids = input_ids.clone()</span><br><span class="line">        target_ids[:, :-<span class="number">1</span>] = -<span class="number">100</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model(input_ids, labels=target_ids)</span><br><span class="line">            nlls.append(outputs.loss)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.exp(torch.stack(nlls).mean())</span><br></pre></td></tr></table></figure>

<h2 id="实践项目建议"><a href="#实践项目建议" class="headerlink" title="实践项目建议"></a>实践项目建议</h2><ol>
<li><strong>入门</strong>：情感分析、文本分类</li>
<li><strong>进阶</strong>：命名实体识别、机器翻译</li>
<li><strong>高级</strong>：问答系统、RAG 应用</li>
<li><strong>专家</strong>：LLM 预训练、RLHF</li>
</ol>
<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13971">LLaMA Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">LoRA Paper</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/10/03/%E5%BD%93%E6%88%91%E4%BB%AC%E6%8A%8A%E7%9B%AE%E5%85%89%E6%94%BE%E5%9C%A8%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/03/%E5%BD%93%E6%88%91%E4%BB%AC%E6%8A%8A%E7%9B%AE%E5%85%89%E6%94%BE%E5%9C%A8%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/" class="post-title-link" itemprop="url">机器阅读理解：从传统方法到大语言模型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-10-03 23:41:50" itemprop="dateCreated datePublished" datetime="2019-10-03T23:41:50+08:00">2019-10-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p><strong>核心问题</strong>：当我们期望机器”理解”文本时，我们的期望到底是什么？</p>
</blockquote>
<h2 id="机器阅读理解的演进"><a href="#机器阅读理解的演进" class="headerlink" title="机器阅读理解的演进"></a>机器阅读理解的演进</h2><h3 id="传统-MRC-2015-2019"><a href="#传统-MRC-2015-2019" class="headerlink" title="传统 MRC (2015-2019)"></a>传统 MRC (2015-2019)</h3><p>基于 span extraction 的方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: Context + Question</span><br><span class="line">输出: (start_idx, end_idx)</span><br></pre></td></tr></table></figure>

<p>代表模型：BiDAF, R-Net, QANet, BERT</p>
<h3 id="LLM-时代的-MRC-2020-至今"><a href="#LLM-时代的-MRC-2020-至今" class="headerlink" title="LLM 时代的 MRC (2020-至今)"></a>LLM 时代的 MRC (2020-至今)</h3><p>从”抽取”到”生成”的范式转变：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: Context + Question + Instruction</span><br><span class="line">输出: 自由形式的答案</span><br></pre></td></tr></table></figure>

<h2 id="任务分类与难度"><a href="#任务分类与难度" class="headerlink" title="任务分类与难度"></a>任务分类与难度</h2><table>
<thead>
<tr>
<th>类型</th>
<th>传统方法</th>
<th>LLM 方法</th>
<th>难度</th>
</tr>
</thead>
<tbody><tr>
<td><strong>抽取式</strong></td>
<td>✅ 擅长</td>
<td>✅ 擅长</td>
<td>⭐</td>
</tr>
<tr>
<td><strong>多跳推理</strong></td>
<td>❌ 困难</td>
<td>⚠️ 有限</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong>数值推理</strong></td>
<td>❌ 几乎不能</td>
<td>⚠️ 需要 CoT</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong>常识推理</strong></td>
<td>❌ 不能</td>
<td>✅ 较好</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong>开放生成</strong></td>
<td>❌ 不能</td>
<td>✅ 擅长</td>
<td>⭐⭐</td>
</tr>
</tbody></table>
<h2 id="现代方法：RAG"><a href="#现代方法：RAG" class="headerlink" title="现代方法：RAG"></a>现代方法：RAG</h2><p>检索增强生成 (Retrieval-Augmented Generation) 结合了检索和生成的优势：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RAGSystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, retriever, generator</span>):</span><br><span class="line">        <span class="variable language_">self</span>.retriever = retriever  <span class="comment"># e.g., Dense Retriever</span></span><br><span class="line">        <span class="variable language_">self</span>.generator = generator  <span class="comment"># e.g., LLM</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">answer</span>(<span class="params">self, question: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># 1. 检索相关文档</span></span><br><span class="line">        docs = <span class="variable language_">self</span>.retriever.retrieve(question, top_k=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 构建上下文</span></span><br><span class="line">        context = <span class="string">&quot;\n\n&quot;</span>.join([d.text <span class="keyword">for</span> d <span class="keyword">in</span> docs])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 生成答案</span></span><br><span class="line">        prompt = <span class="string">f&quot;&quot;&quot;基于以下文档回答问题：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;context&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">问题：<span class="subst">&#123;question&#125;</span></span></span><br><span class="line"><span class="string">答案：&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.generator.generate(prompt)</span><br></pre></td></tr></table></figure>

<h3 id="检索器选择"><a href="#检索器选择" class="headerlink" title="检索器选择"></a>检索器选择</h3><table>
<thead>
<tr>
<th>检索器</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>BM25</strong></td>
<td>关键词匹配，快速</td>
<td>短查询，精确匹配</td>
</tr>
<tr>
<td><strong>Dense Retriever</strong></td>
<td>语义匹配</td>
<td>语义相似查询</td>
</tr>
<tr>
<td><strong>ColBERT</strong></td>
<td>延迟交互</td>
<td>平衡效率与效果</td>
</tr>
<tr>
<td><strong>Hybrid</strong></td>
<td>结合稀疏+稠密</td>
<td>生产环境</td>
</tr>
</tbody></table>
<h2 id="Chain-of-Thought-推理"><a href="#Chain-of-Thought-推理" class="headerlink" title="Chain-of-Thought 推理"></a>Chain-of-Thought 推理</h2><p>对于需要推理的问题，CoT prompting 显著提升效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准 Prompting</span></span><br><span class="line">prompt_standard = <span class="string">&quot;Q: 小明有5个苹果，给了小红2个，还剩几个？\nA:&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Chain-of-Thought Prompting  </span></span><br><span class="line">prompt_cot = <span class="string">&quot;&quot;&quot;Q: 小明有5个苹果，给了小红2个，还剩几个？</span></span><br><span class="line"><span class="string">A: 让我们一步步思考：</span></span><br><span class="line"><span class="string">1. 小明最初有 5 个苹果</span></span><br><span class="line"><span class="string">2. 他给了小红 2 个苹果</span></span><br><span class="line"><span class="string">3. 剩余苹果数 = 5 - 2 = 3</span></span><br><span class="line"><span class="string">答案是 3 个苹果。&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><h3 id="传统指标"><a href="#传统指标" class="headerlink" title="传统指标"></a>传统指标</h3><p>$$<br>\text{F1} &#x3D; 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}<br>$$</p>
<p>$$<br>\text{EM (Exact Match)} &#x3D; \mathbb{1}[\text{pred} &#x3D; \text{gold}]<br>$$</p>
<h3 id="LLM-时代的指标"><a href="#LLM-时代的指标" class="headerlink" title="LLM 时代的指标"></a>LLM 时代的指标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 LLM 作为评估器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">llm_evaluate</span>(<span class="params">question, gold_answer, pred_answer</span>):</span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;评估预测答案的质量（1-5分）：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">问题：<span class="subst">&#123;question&#125;</span></span></span><br><span class="line"><span class="string">标准答案：<span class="subst">&#123;gold_answer&#125;</span></span></span><br><span class="line"><span class="string">预测答案：<span class="subst">&#123;pred_answer&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">评分标准：</span></span><br><span class="line"><span class="string">5分 - 完全正确且信息完整</span></span><br><span class="line"><span class="string">4分 - 基本正确，略有遗漏</span></span><br><span class="line"><span class="string">3分 - 部分正确</span></span><br><span class="line"><span class="string">2分 - 有相关信息但不正确</span></span><br><span class="line"><span class="string">1分 - 完全错误</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">分数：&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> llm.generate(prompt)</span><br></pre></td></tr></table></figure>

<h2 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h2><h3 id="何时用传统-MRC"><a href="#何时用传统-MRC" class="headerlink" title="何时用传统 MRC"></a>何时用传统 MRC</h3><ul>
<li>答案明确在文档中</li>
<li>需要精确的位置标注</li>
<li>低延迟要求</li>
<li>资源受限</li>
</ul>
<h3 id="何时用-RAG-LLM"><a href="#何时用-RAG-LLM" class="headerlink" title="何时用 RAG + LLM"></a>何时用 RAG + LLM</h3><ul>
<li>需要整合多个文档</li>
<li>答案需要推理或总结</li>
<li>开放域问答</li>
<li>用户期望自然语言回答</li>
</ul>
<h2 id="代码示例：现代-RAG-系统"><a href="#代码示例：现代-RAG-系统" class="headerlink" title="代码示例：现代 RAG 系统"></a>代码示例：现代 RAG 系统</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化组件</span></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">vectorstore = FAISS.load_local(<span class="string">&quot;my_index&quot;</span>, embeddings)</span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-4&quot;</span>, temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 RAG 链</span></span><br><span class="line">qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">&quot;stuff&quot;</span>,  <span class="comment"># 或 &quot;map_reduce&quot;, &quot;refine&quot;</span></span><br><span class="line">    retriever=vectorstore.as_retriever(search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">5</span>&#125;),</span><br><span class="line">    return_source_documents=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">result = qa_chain(&#123;<span class="string">&quot;query&quot;</span>: <span class="string">&quot;什么是机器阅读理解？&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(result[<span class="string">&quot;result&quot;</span>])</span><br></pre></td></tr></table></figure>

<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li><a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD 2.0</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11401">RAG Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.langchain.com/">LangChain Documentation</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/10/03/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/03/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">因果关系推断介绍</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-10-03 23:34:37" itemprop="dateCreated datePublished" datetime="2019-10-03T23:34:37+08:00">2019-10-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>因果推断是机器学习领域的重要研究方向，特别是在大语言模型时代，理解因果关系对于构建可解释、可信赖的 AI 系统至关重要。</p>
<h2 id="为什么需要因果推断？"><a href="#为什么需要因果推断？" class="headerlink" title="为什么需要因果推断？"></a>为什么需要因果推断？</h2><p>传统机器学习依赖相关性，但<strong>相关性不等于因果性</strong>。例如：</p>
<ul>
<li>冰淇淋销量与溺水事件正相关（共同原因：夏天）</li>
<li>LLM 可能学到虚假相关性，导致 hallucination</li>
</ul>
<p>因果推断帮助我们：</p>
<ol>
<li>理解干预效果（如果我做 X，会发生什么？）</li>
<li>进行反事实推理（如果当时做了 Y，结果会怎样？）</li>
<li>构建更鲁棒的模型</li>
</ol>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="因果图-Causal-Graph"><a href="#因果图-Causal-Graph" class="headerlink" title="因果图 (Causal Graph)"></a>因果图 (Causal Graph)</h3><p>使用有向无环图 (DAG) 表示变量之间的因果关系：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X → Y → Z    (链式结构)</span><br><span class="line">X ← W → Y    (混杂结构)  </span><br><span class="line">X → W ← Y    (对撞结构)</span><br></pre></td></tr></table></figure>

<h3 id="结构因果模型-SCM"><a href="#结构因果模型-SCM" class="headerlink" title="结构因果模型 (SCM)"></a>结构因果模型 (SCM)</h3><p>$$<br>Y &#x3D; f(X, U_Y)<br>$$</p>
<p>其中 $X$ 是原因，$Y$ 是结果，$U_Y$ 是噪声项。</p>
<h3 id="do-算子与干预"><a href="#do-算子与干预" class="headerlink" title="do 算子与干预"></a>do 算子与干预</h3><p>区分观测和干预：</p>
<ul>
<li><strong>观测</strong>：$P(Y|X&#x3D;x)$ — 看到 X&#x3D;x 时 Y 的分布</li>
<li><strong>干预</strong>：$P(Y|do(X&#x3D;x))$ — 强制设置 X&#x3D;x 时 Y 的分布</li>
</ul>
<h2 id="因果发现算法"><a href="#因果发现算法" class="headerlink" title="因果发现算法"></a>因果发现算法</h2><h3 id="PC-算法"><a href="#PC-算法" class="headerlink" title="PC 算法"></a>PC 算法</h3><p>基于条件独立性检验的经典算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PC 算法伪代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pc_algorithm</span>(<span class="params">data, alpha=<span class="number">0.05</span></span>):</span><br><span class="line">    <span class="comment"># 1. 初始化完全图</span></span><br><span class="line">    G = complete_graph(variables)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 骨架学习：移除条件独立的边</span></span><br><span class="line">    <span class="keyword">for</span> (X, Y) <span class="keyword">in</span> edges(G):</span><br><span class="line">        <span class="keyword">for</span> S <span class="keyword">in</span> subsets(neighbors):</span><br><span class="line">            <span class="keyword">if</span> conditional_independent(X, Y, S, alpha):</span><br><span class="line">                remove_edge(G, X, Y)</span><br><span class="line">                sep_set[X, Y] = S</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 方向确定：识别 v-structure</span></span><br><span class="line">    orient_v_structures(G, sep_set)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> G</span><br></pre></td></tr></table></figure>

<p>Python 实现参考：<a target="_blank" rel="noopener" href="https://github.com/fooSynaptic/py_pcalg">fooSynaptic&#x2F;py_pcalg</a></p>
<h3 id="现代方法"><a href="#现代方法" class="headerlink" title="现代方法"></a>现代方法</h3><table>
<thead>
<tr>
<th>方法</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>NOTEARS</strong></td>
<td>连续优化，可微分</td>
<td>线性&#x2F;非线性因果发现</td>
</tr>
<tr>
<td><strong>DAG-GNN</strong></td>
<td>基于图神经网络</td>
<td>大规模因果图学习</td>
</tr>
<tr>
<td><strong>Causal Transformer</strong></td>
<td>结合注意力机制</td>
<td>时序因果推断</td>
</tr>
</tbody></table>
<h2 id="因果推断与大语言模型"><a href="#因果推断与大语言模型" class="headerlink" title="因果推断与大语言模型"></a>因果推断与大语言模型</h2><h3 id="LLM-中的因果问题"><a href="#LLM-中的因果问题" class="headerlink" title="LLM 中的因果问题"></a>LLM 中的因果问题</h3><ol>
<li><strong>Hallucination</strong>：模型学到虚假相关性</li>
<li><strong>Bias</strong>：训练数据中的混杂因素</li>
<li><strong>Robustness</strong>：分布外泛化能力差</li>
</ol>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因果提示 (Causal Prompting) 示例</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">请分析以下事件的因果关系，而非相关性：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">事件A: 公司增加广告投入</span></span><br><span class="line"><span class="string">事件B: 销售额上升</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">问：A 是否导致了 B？请考虑可能的混杂因素。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="因果推理增强-RAG"><a href="#因果推理增强-RAG" class="headerlink" title="因果推理增强 RAG"></a>因果推理增强 RAG</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CausalRAG</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, retriever, causal_graph</span>):</span><br><span class="line">        <span class="variable language_">self</span>.retriever = retriever</span><br><span class="line">        <span class="variable language_">self</span>.causal_graph = causal_graph</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">retrieve</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="comment"># 1. 识别查询中的因果关系</span></span><br><span class="line">        cause, effect = extract_causal_pair(query)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 基于因果图过滤无关文档</span></span><br><span class="line">        relevant_vars = <span class="variable language_">self</span>.causal_graph.ancestors(effect)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 检索因果相关的文档</span></span><br><span class="line">        docs = <span class="variable language_">self</span>.retriever.search(query)</span><br><span class="line">        <span class="keyword">return</span> filter_by_causal_relevance(docs, relevant_vars)</span><br></pre></td></tr></table></figure>

<h2 id="工具与资源"><a href="#工具与资源" class="headerlink" title="工具与资源"></a>工具与资源</h2><table>
<thead>
<tr>
<th>工具</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td><strong>DoWhy</strong></td>
<td>Python</td>
<td>因果推断框架</td>
</tr>
<tr>
<td><strong>CausalNex</strong></td>
<td>Python</td>
<td>贝叶斯网络 + 因果发现</td>
</tr>
<tr>
<td><strong>pgmpy</strong></td>
<td>Python</td>
<td>概率图模型</td>
</tr>
<tr>
<td><strong>Tetrad</strong></td>
<td>Java</td>
<td>因果搜索算法</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DoWhy 示例</span></span><br><span class="line"><span class="keyword">import</span> dowhy</span><br><span class="line"><span class="keyword">from</span> dowhy <span class="keyword">import</span> CausalModel</span><br><span class="line"></span><br><span class="line">model = CausalModel(</span><br><span class="line">    data=df,</span><br><span class="line">    treatment=<span class="string">&#x27;treatment&#x27;</span>,</span><br><span class="line">    outcome=<span class="string">&#x27;outcome&#x27;</span>,</span><br><span class="line">    graph=<span class="string">&#x27;digraph &#123;treatment -&gt; outcome; confounder -&gt; treatment; confounder -&gt; outcome&#125;&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 识别因果效应</span></span><br><span class="line">identified = model.identify_effect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 估计因果效应</span></span><br><span class="line">estimate = model.estimate_effect(identified, method_name=<span class="string">&quot;backdoor.propensity_score_matching&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li>Judea Pearl, <em>The Book of Why</em> (2018)</li>
<li>Peters et al., <em>Elements of Causal Inference</em> (2017)</li>
<li><a target="_blank" rel="noopener" href="https://cs.stanford.edu/~ermon/cs228/">Stanford CS 228: Probabilistic Graphical Models</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/10/03/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B9%8B%E4%B8%80%EF%BC%9ATruncate-SVD-%E5%92%8Crandom-SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/03/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E4%B9%8B%E4%B8%80%EF%BC%9ATruncate-SVD-%E5%92%8Crandom-SVD/" class="post-title-link" itemprop="url">矩阵分解：从 SVD 到现代 AI 应用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-10-03 23:32:18" itemprop="dateCreated datePublished" datetime="2019-10-03T23:32:18+08:00">2019-10-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>矩阵分解是机器学习的基石技术，从传统的推荐系统到现代大语言模型的参数高效微调（LoRA），都离不开矩阵分解的思想。</p>
<h2 id="奇异值分解-SVD"><a href="#奇异值分解-SVD" class="headerlink" title="奇异值分解 (SVD)"></a>奇异值分解 (SVD)</h2><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><p>任意矩阵 $A \in \mathbb{R}^{m \times n}$ 可以分解为：</p>
<p>$$<br>A &#x3D; U \Sigma V^T<br>$$</p>
<p>其中：</p>
<ul>
<li>$U \in \mathbb{R}^{m \times m}$：左奇异向量（正交矩阵）</li>
<li>$\Sigma \in \mathbb{R}^{m \times n}$：奇异值对角矩阵</li>
<li>$V \in \mathbb{R}^{n \times n}$：右奇异向量（正交矩阵）</li>
</ul>
<h3 id="Truncated-SVD"><a href="#Truncated-SVD" class="headerlink" title="Truncated SVD"></a>Truncated SVD</h3><p>保留前 $r$ 个最大奇异值：</p>
<p>$$<br>A \approx A_r &#x3D; U_r \Sigma_r V_r^T<br>$$</p>
<p>这是<strong>最优</strong>的秩 $r$ 近似（Eckart-Young 定理）：</p>
<p>$$<br>A_r &#x3D; \arg\min_{\text{rank}(B) &#x3D; r} |A - B|_F<br>$$</p>
<h2 id="Randomized-SVD"><a href="#Randomized-SVD" class="headerlink" title="Randomized SVD"></a>Randomized SVD</h2><p>当矩阵规模巨大时，精确 SVD 计算代价过高。Randomized SVD 提供了高效的近似方法。</p>
<h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> linalg</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">randomized_svd</span>(<span class="params">A, n_components, n_oversamples=<span class="number">10</span>, n_iter=<span class="number">4</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Randomized SVD for large matrices.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        A: Input matrix (m x n)</span></span><br><span class="line"><span class="string">        n_components: Number of singular values to compute</span></span><br><span class="line"><span class="string">        n_oversamples: Additional random vectors for accuracy</span></span><br><span class="line"><span class="string">        n_iter: Number of power iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        U, s, Vt: Truncated SVD components</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m, n = A.shape</span><br><span class="line">    n_random = n_components + n_oversamples</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Random projection</span></span><br><span class="line">    Q = np.random.randn(n, n_random)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Power iteration for accuracy</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_iter):</span><br><span class="line">        Q, _ = linalg.lu(A @ Q, permute_l=<span class="literal">True</span>)</span><br><span class="line">        Q, _ = linalg.lu(A.T @ Q, permute_l=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    Q, _ = linalg.qr(A @ Q, mode=<span class="string">&#x27;economic&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Project and compute SVD</span></span><br><span class="line">    B = Q.T @ A</span><br><span class="line">    Uhat, s, Vt = linalg.svd(B, full_matrices=<span class="literal">False</span>)</span><br><span class="line">    U = Q @ Uhat</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> U[:, :n_components], s[:n_components], Vt[:n_components, :]</span><br></pre></td></tr></table></figure>

<h3 id="复杂度对比"><a href="#复杂度对比" class="headerlink" title="复杂度对比"></a>复杂度对比</h3><table>
<thead>
<tr>
<th>方法</th>
<th>时间复杂度</th>
<th>空间复杂度</th>
</tr>
</thead>
<tbody><tr>
<td>精确 SVD</td>
<td>$O(mn \cdot \min(m,n))$</td>
<td>$O(mn)$</td>
</tr>
<tr>
<td>Randomized SVD</td>
<td>$O(mn \cdot r)$</td>
<td>$O((m+n) \cdot r)$</td>
</tr>
<tr>
<td>Truncated SVD (Lanczos)</td>
<td>$O(mn \cdot r)$</td>
<td>$O((m+n) \cdot r)$</td>
</tr>
</tbody></table>
<h2 id="现代应用：LoRA"><a href="#现代应用：LoRA" class="headerlink" title="现代应用：LoRA"></a>现代应用：LoRA</h2><p>LoRA (Low-Rank Adaptation) 是大语言模型参数高效微调的核心技术，直接利用了低秩分解的思想。</p>
<h3 id="LoRA-原理"><a href="#LoRA-原理" class="headerlink" title="LoRA 原理"></a>LoRA 原理</h3><p>预训练权重 $W_0$ 固定，只训练低秩增量：</p>
<p>$$<br>W &#x3D; W_0 + \Delta W &#x3D; W_0 + BA<br>$$</p>
<p>其中 $B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times k}$，$r \ll \min(d, k)$。</p>
<h3 id="实现示例"><a href="#实现示例" class="headerlink" title="实现示例"></a>实现示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoRALayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features, rank=<span class="number">4</span>, alpha=<span class="number">1.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.rank = rank</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 原始权重（冻结）</span></span><br><span class="line">        <span class="variable language_">self</span>.W = nn.Linear(in_features, out_features, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 低秩分解</span></span><br><span class="line">        <span class="variable language_">self</span>.A = nn.Linear(in_features, rank, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.B = nn.Linear(rank, out_features, bias=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        nn.init.kaiming_uniform_(<span class="variable language_">self</span>.A.weight)</span><br><span class="line">        nn.init.zeros_(<span class="variable language_">self</span>.B.weight)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.scaling = alpha / rank</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># W(x) + scaling * B(A(x))</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W(x) + <span class="variable language_">self</span>.scaling * <span class="variable language_">self</span>.B(<span class="variable language_">self</span>.A(x))</span><br></pre></td></tr></table></figure>

<h3 id="参数效率"><a href="#参数效率" class="headerlink" title="参数效率"></a>参数效率</h3><p>对于 LLaMA-7B：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>可训练参数</th>
<th>显存占用</th>
</tr>
</thead>
<tbody><tr>
<td>全量微调</td>
<td>7B (100%)</td>
<td>~140GB</td>
</tr>
<tr>
<td>LoRA (r&#x3D;8)</td>
<td>4.7M (0.07%)</td>
<td>~14GB</td>
</tr>
<tr>
<td>LoRA (r&#x3D;16)</td>
<td>9.4M (0.13%)</td>
<td>~16GB</td>
</tr>
</tbody></table>
<h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><h3 id="1-推荐系统"><a href="#1-推荐系统" class="headerlink" title="1. 推荐系统"></a>1. 推荐系统</h3><p>矩阵分解用于协同过滤：</p>
<p>$$<br>R \approx U V^T<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 surprise 库</span></span><br><span class="line"><span class="keyword">from</span> surprise <span class="keyword">import</span> SVD, Dataset, Reader</span><br><span class="line"></span><br><span class="line">reader = Reader(rating_scale=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">data = Dataset.load_from_df(df[[<span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;item&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>]], reader)</span><br><span class="line">model = SVD(n_factors=<span class="number">100</span>)</span><br><span class="line">model.fit(trainset)</span><br></pre></td></tr></table></figure>

<h3 id="2-文本表示-LSA"><a href="#2-文本表示-LSA" class="headerlink" title="2. 文本表示 (LSA)"></a>2. 文本表示 (LSA)</h3><p>潜在语义分析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> TruncatedSVD</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(max_features=<span class="number">10000</span>)</span><br><span class="line">X = vectorizer.fit_transform(documents)</span><br><span class="line"></span><br><span class="line">svd = TruncatedSVD(n_components=<span class="number">100</span>)</span><br><span class="line">X_reduced = svd.fit_transform(X)</span><br></pre></td></tr></table></figure>

<h3 id="3-图像压缩"><a href="#3-图像压缩" class="headerlink" title="3. 图像压缩"></a>3. 图像压缩</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compress_image</span>(<span class="params">image_path, n_components=<span class="number">50</span></span>):</span><br><span class="line">    img = np.array(Image.<span class="built_in">open</span>(image_path).convert(<span class="string">&#x27;L&#x27;</span>))</span><br><span class="line">    U, s, Vt = np.linalg.svd(img, full_matrices=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保留前 n_components 个奇异值</span></span><br><span class="line">    compressed = U[:, :n_components] @ np.diag(s[:n_components]) @ Vt[:n_components, :]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compressed.astype(np.uint8)</span><br></pre></td></tr></table></figure>

<h2 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h2><h3 id="条件数"><a href="#条件数" class="headerlink" title="条件数"></a>条件数</h3><p>$$<br>\kappa(A) &#x3D; \frac{\sigma_{\max}}{\sigma_{\min}}<br>$$</p>
<p>条件数过大会导致数值不稳定。</p>
<h3 id="正则化-SVD"><a href="#正则化-SVD" class="headerlink" title="正则化 SVD"></a>正则化 SVD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">regularized_svd</span>(<span class="params">A, lambda_reg=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Add regularization for numerical stability.&quot;&quot;&quot;</span></span><br><span class="line">    U, s, Vt = np.linalg.svd(A, full_matrices=<span class="literal">False</span>)</span><br><span class="line">    s_reg = s / (s**<span class="number">2</span> + lambda_reg)</span><br><span class="line">    <span class="keyword">return</span> U, s_reg, Vt</span><br></pre></td></tr></table></figure>

<h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ul>
<li>Halko et al., <em>Finding Structure with Randomness</em> (2011)</li>
<li>Hu et al., <em>LoRA: Low-Rank Adaptation of Large Language Models</em> (2021)</li>
<li><a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">NumPy SVD Documentation</a></li>
</ul>
<hr>
<blockquote>
<p>转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://foosynaptic.github.io/2019/10/03/%E5%BC%80%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fooSynaptic">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fooSynaptic">
      <meta itemprop="description" content="Head first to the Truth as Synaptic.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | fooSynaptic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/10/03/%E5%BC%80%E7%AF%87/" class="post-title-link" itemprop="url">开篇</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-10-03 23:18:17" itemprop="dateCreated datePublished" datetime="2019-10-03T23:18:17+08:00">2019-10-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>各位读者朋友们大家好，我是 fooSynaptic。</p>
<p>欢迎来到我的技术博客！这里记录我在 AI 和 NLP 领域的学习与思考。</p>
<h2 id="关于这个博客"><a href="#关于这个博客" class="headerlink" title="关于这个博客"></a>关于这个博客</h2><p>这个博客主要记录以下内容：</p>
<ul>
<li><strong>自然语言处理 (NLP)</strong>：从传统方法到大语言模型</li>
<li><strong>机器学习</strong>：算法原理与实现细节</li>
<li><strong>深度学习</strong>：模型架构与训练技巧</li>
<li><strong>数学基础</strong>：线性代数、概率论、优化理论</li>
<li><strong>工程实践</strong>：Python、PyTorch、分布式训练</li>
</ul>
<h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NLP: Transformers, LLMs, RAG, Prompt Engineering</span><br><span class="line">ML: PyTorch, JAX, scikit-learn</span><br><span class="line">Infra: CUDA, Triton, vLLM, DeepSpeed</span><br></pre></td></tr></table></figure>

<h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><p>NLP Researcher，专注于：</p>
<ul>
<li>大语言模型 (LLM) 训练与推理优化</li>
<li>检索增强生成 (RAG)</li>
<li>机器阅读理解与问答系统</li>
</ul>
<p><strong>GitHub</strong>: <a target="_blank" rel="noopener" href="https://github.com/fooSynaptic">fooSynaptic</a></p>
<hr>
<blockquote>
<p>欢迎交流讨论，转载请注明出处</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">fooSynaptic's Blog</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
